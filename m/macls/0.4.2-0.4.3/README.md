# Comparing `tmp/macls-0.4.2-py3-none-any.whl.zip` & `tmp/macls-0.4.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,33 +1,33 @@
-Zip file size: 51797 bytes, number of entries: 31
--rw-rw-rw-  2.0 fat      196 b- defN 23-Aug-30 06:18 macls/__init__.py
--rw-rw-rw-  2.0 fat     9557 b- defN 23-Aug-30 01:49 macls/predict.py
--rw-rw-rw-  2.0 fat    27397 b- defN 23-Aug-30 08:26 macls/trainer.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-07 07:16 macls/data_utils/__init__.py
--rw-rw-rw-  2.0 fat    22287 b- defN 23-Aug-07 07:16 macls/data_utils/audio.py
--rw-rw-rw-  2.0 fat      965 b- defN 23-Aug-07 07:16 macls/data_utils/collate_fn.py
--rw-rw-rw-  2.0 fat     3587 b- defN 23-Aug-07 07:16 macls/data_utils/featurizer.py
--rw-rw-rw-  2.0 fat     5640 b- defN 23-Aug-07 07:16 macls/data_utils/reader.py
--rw-rw-rw-  2.0 fat     1572 b- defN 23-Aug-07 07:15 macls/data_utils/spec_aug.py
--rw-rw-rw-  2.0 fat     5652 b- defN 23-Aug-07 07:16 macls/data_utils/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-08 00:53 macls/metric/__init__.py
--rw-rw-rw-  2.0 fat      329 b- defN 23-Aug-08 00:53 macls/metric/metrics.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-07 07:16 macls/models/__init__.py
--rw-rw-rw-  2.0 fat    13608 b- defN 23-Aug-30 01:41 macls/models/campplus.py
--rw-rw-rw-  2.0 fat     5375 b- defN 23-Aug-10 01:36 macls/models/ecapa_tdnn.py
--rw-rw-rw-  2.0 fat    10504 b- defN 23-Aug-10 02:19 macls/models/eres2net.py
--rw-rw-rw-  2.0 fat     9707 b- defN 23-Aug-10 01:36 macls/models/panns.py
--rw-rw-rw-  2.0 fat     3789 b- defN 23-Aug-10 01:56 macls/models/pooling.py
--rw-rw-rw-  2.0 fat     6776 b- defN 23-Aug-09 03:00 macls/models/res2net.py
--rw-rw-rw-  2.0 fat     5609 b- defN 23-Aug-09 03:01 macls/models/resnet_se.py
--rw-rw-rw-  2.0 fat     3050 b- defN 23-Aug-07 07:16 macls/models/tdnn.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-07 07:16 macls/utils/__init__.py
--rw-rw-rw-  2.0 fat     2839 b- defN 23-Aug-07 07:16 macls/utils/logger.py
--rw-rw-rw-  2.0 fat     1067 b- defN 23-Aug-07 07:16 macls/utils/record.py
--rw-rw-rw-  2.0 fat     1496 b- defN 23-Aug-07 07:16 macls/utils/scheduler.py
--rw-rw-rw-  2.0 fat     3364 b- defN 23-Aug-07 07:16 macls/utils/utils.py
--rw-rw-rw-  2.0 fat    11558 b- defN 23-Aug-30 08:27 macls-0.4.2.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    20301 b- defN 23-Aug-30 08:27 macls-0.4.2.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Aug-30 08:27 macls-0.4.2.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        6 b- defN 23-Aug-30 08:27 macls-0.4.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2455 b- defN 23-Aug-30 08:27 macls-0.4.2.dist-info/RECORD
-31 files, 178778 bytes uncompressed, 47907 bytes compressed:  73.2%
+Zip file size: 53056 bytes, number of entries: 31
+-rw-rw-rw-  2.0 fat      196 b- defN 24-Apr-27 06:46 macls/__init__.py
+-rw-rw-rw-  2.0 fat     9557 b- defN 23-Oct-24 13:59 macls/predict.py
+-rw-rw-rw-  2.0 fat    28617 b- defN 24-Apr-21 10:35 macls/trainer.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-30 13:17 macls/data_utils/__init__.py
+-rw-rw-rw-  2.0 fat    22218 b- defN 24-Apr-27 05:31 macls/data_utils/audio.py
+-rw-rw-rw-  2.0 fat      965 b- defN 23-Feb-15 12:59 macls/data_utils/collate_fn.py
+-rw-rw-rw-  2.0 fat     3587 b- defN 24-Jan-23 10:26 macls/data_utils/featurizer.py
+-rw-rw-rw-  2.0 fat     5630 b- defN 23-Sep-13 12:03 macls/data_utils/reader.py
+-rw-rw-rw-  2.0 fat     1572 b- defN 23-Aug-07 14:52 macls/data_utils/spec_aug.py
+-rw-rw-rw-  2.0 fat     5712 b- defN 24-Jan-28 03:59 macls/data_utils/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Mar-16 05:03 macls/metric/__init__.py
+-rw-rw-rw-  2.0 fat      329 b- defN 23-Aug-07 15:15 macls/metric/metrics.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-30 13:17 macls/models/__init__.py
+-rw-rw-rw-  2.0 fat    13608 b- defN 23-Aug-30 14:35 macls/models/campplus.py
+-rw-rw-rw-  2.0 fat     5375 b- defN 23-Oct-24 13:59 macls/models/ecapa_tdnn.py
+-rw-rw-rw-  2.0 fat    10504 b- defN 23-Aug-10 10:25 macls/models/eres2net.py
+-rw-rw-rw-  2.0 fat     9707 b- defN 23-Aug-30 14:43 macls/models/panns.py
+-rw-rw-rw-  2.0 fat     3789 b- defN 23-Aug-10 10:25 macls/models/pooling.py
+-rw-rw-rw-  2.0 fat     6776 b- defN 23-Aug-09 09:51 macls/models/res2net.py
+-rw-rw-rw-  2.0 fat     5609 b- defN 23-Aug-09 09:51 macls/models/resnet_se.py
+-rw-rw-rw-  2.0 fat     3050 b- defN 23-Mar-21 11:25 macls/models/tdnn.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-30 13:17 macls/utils/__init__.py
+-rw-rw-rw-  2.0 fat     2839 b- defN 23-Jan-30 13:17 macls/utils/logger.py
+-rw-rw-rw-  2.0 fat     1067 b- defN 23-Mar-23 11:45 macls/utils/record.py
+-rw-rw-rw-  2.0 fat     1496 b- defN 23-Aug-06 15:19 macls/utils/scheduler.py
+-rw-rw-rw-  2.0 fat     4300 b- defN 24-Apr-27 04:48 macls/utils/utils.py
+-rw-rw-rw-  2.0 fat    11558 b- defN 24-Apr-27 06:46 macls-0.4.3.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    21616 b- defN 24-Apr-27 06:46 macls-0.4.3.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-27 06:46 macls-0.4.3.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        6 b- defN 24-Apr-27 06:46 macls-0.4.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2455 b- defN 24-Apr-27 06:46 macls-0.4.3.dist-info/RECORD
+31 files, 182230 bytes uncompressed, 49166 bytes compressed:  73.0%
```

## zipnote {}

```diff
@@ -72,23 +72,23 @@
 
 Filename: macls/utils/scheduler.py
 Comment: 
 
 Filename: macls/utils/utils.py
 Comment: 
 
-Filename: macls-0.4.2.dist-info/LICENSE
+Filename: macls-0.4.3.dist-info/LICENSE
 Comment: 
 
-Filename: macls-0.4.2.dist-info/METADATA
+Filename: macls-0.4.3.dist-info/METADATA
 Comment: 
 
-Filename: macls-0.4.2.dist-info/WHEEL
+Filename: macls-0.4.3.dist-info/WHEEL
 Comment: 
 
-Filename: macls-0.4.2.dist-info/top_level.txt
+Filename: macls-0.4.3.dist-info/top_level.txt
 Comment: 
 
-Filename: macls-0.4.2.dist-info/RECORD
+Filename: macls-0.4.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## macls/__init__.py

```diff
@@ -1,4 +1,4 @@
-__version__ = "0.4.2"
+__version__ = "0.4.3"
 # 项目支持的模型
 SUPPORT_MODEL = ['EcapaTdnn', 'PANNS_CNN6', 'PANNS_CNN10', 'PANNS_CNN14', 'Res2Net', 'ResNetSE', 'TDNN', 'ERes2Net',
                  'CAMPPlus']
```

## macls/trainer.py

```diff
@@ -72,18 +72,20 @@
         # 获取特征器
         self.audio_featurizer = AudioFeaturizer(feature_method=self.configs.preprocess_conf.feature_method,
                                                 method_args=self.configs.preprocess_conf.get('method_args', {}))
         self.audio_featurizer.to(self.device)
         # 特征增强
         self.spec_aug = SpecAug(**self.configs.dataset_conf.get('spec_aug_args', {}))
         self.spec_aug.to(self.device)
-        # 获取分类标签
-        with open(self.configs.dataset_conf.label_list_path, 'r', encoding='utf-8') as f:
-            lines = f.readlines()
-        self.class_labels = [l.replace('\n', '') for l in lines]
+        self.max_step, self.train_step = None, None
+        self.train_loss, self.train_acc = None, None
+        self.train_eta_sec = None
+        self.eval_loss, self.eval_acc = None, None
+        self.test_log_step, self.train_log_step = 0, 0
+        self.stop_train, self.stop_eval = False, False
 
     def __setup_dataloader(self, is_train=False):
         if is_train:
             self.train_dataset = CustomDataset(data_list_path=self.configs.dataset_conf.train_list,
                                                do_vad=self.configs.dataset_conf.do_vad,
                                                max_duration=self.configs.dataset_conf.max_duration,
                                                min_duration=self.configs.dataset_conf.min_duration,
@@ -146,15 +148,17 @@
         self.audio_featurizer.to(self.device)
         summary(self.model, input_size=(1, 98, self.audio_featurizer.feature_dim))
         # 使用Pytorch2.0的编译器
         if self.configs.train_conf.use_compile and torch.__version__ >= "2" and platform.system().lower() == 'windows':
             self.model = torch.compile(self.model, mode="reduce-overhead")
         # print(self.model)
         # 获取损失函数
-        self.loss = torch.nn.CrossEntropyLoss()
+        weight = torch.tensor(self.configs.train_conf.loss_weight, dtype=torch.float, device=self.device)\
+            if self.configs.train_conf.loss_weight is not None else None
+        self.loss = torch.nn.CrossEntropyLoss(weight=weight)
         if is_train:
             if self.configs.train_conf.enable_amp:
                 self.amp_scaler = torch.cuda.amp.GradScaler(init_scale=1024)
             # 获取优化方法
             optimizer = self.configs.optimizer_conf.optimizer
             if optimizer == 'Adam':
                 self.optimizer = torch.optim.Adam(params=self.model.parameters(),
@@ -182,14 +186,16 @@
             elif self.configs.optimizer_conf.scheduler == 'WarmupCosineSchedulerLR':
                 self.scheduler = WarmupCosineSchedulerLR(optimizer=self.optimizer,
                                                          fix_epoch=self.configs.train_conf.max_epoch,
                                                          step_per_epoch=len(self.train_loader),
                                                          **scheduler_args)
             else:
                 raise Exception(f'不支持学习率衰减函数：{self.configs.optimizer_conf.scheduler}')
+        if self.configs.train_conf.use_compile and torch.__version__ >= "2" and platform.system().lower() != 'windows':
+            self.model = torch.compile(self.model, mode="reduce-overhead")
 
     def __load_pretrained(self, pretrained_model):
         # 加载预训练模型
         if pretrained_model is not None:
             if os.path.isdir(pretrained_model):
                 pretrained_model = os.path.join(pretrained_model, 'model.pth')
             assert os.path.exists(pretrained_model), f"{pretrained_model} 模型不存在！"
@@ -279,16 +285,16 @@
             if os.path.exists(old_model_path):
                 shutil.rmtree(old_model_path)
         logger.info('已保存模型：{}'.format(model_path))
 
     def __train_epoch(self, epoch_id, local_rank, writer, nranks=0):
         train_times, accuracies, loss_sum = [], [], []
         start = time.time()
-        sum_batch = len(self.train_loader) * self.configs.train_conf.max_epoch
         for batch_id, (audio, label, input_lens_ratio) in enumerate(self.train_loader):
+            if self.stop_train: break
             if nranks > 1:
                 audio = audio.to(local_rank)
                 input_lens_ratio = input_lens_ratio.to(local_rank)
                 label = label.to(local_rank).long()
             else:
                 audio = audio.to(self.device)
                 input_lens_ratio = input_lens_ratio.to(self.device)
@@ -319,36 +325,37 @@
             self.optimizer.zero_grad()
 
             # 计算准确率
             acc = accuracy(output, label)
             accuracies.append(acc)
             loss_sum.append(los.data.cpu().numpy())
             train_times.append((time.time() - start) * 1000)
+            self.train_step += 1
 
             # 多卡训练只使用一个进程打印
             if batch_id % self.configs.train_conf.log_interval == 0 and local_rank == 0:
                 batch_id = batch_id + 1
                 # 计算每秒训练数据量
                 train_speed = self.configs.dataset_conf.dataLoader.batch_size / (sum(train_times) / len(train_times) / 1000)
                 # 计算剩余时间
-                eta_sec = (sum(train_times) / len(train_times)) * (
-                        sum_batch - (epoch_id - 1) * len(self.train_loader) - batch_id)
-                eta_str = str(timedelta(seconds=int(eta_sec / 1000)))
+                self.train_eta_sec = (sum(train_times) / len(train_times)) * (self.max_step - self.train_step) / 1000
+                eta_str = str(timedelta(seconds=int(self.train_eta_sec)))
+                self.train_loss = sum(loss_sum) / len(loss_sum)
+                self.train_acc = sum(accuracies) / len(accuracies)
                 logger.info(f'Train epoch: [{epoch_id}/{self.configs.train_conf.max_epoch}], '
                             f'batch: [{batch_id}/{len(self.train_loader)}], '
-                            f'loss: {sum(loss_sum) / len(loss_sum):.5f}, '
-                            f'accuracy: {sum(accuracies) / len(accuracies):.5f}, '
+                            f'loss: {self.train_loss:.5f}, accuracy: {self.train_acc:.5f}, '
                             f'learning rate: {self.scheduler.get_last_lr()[0]:>.8f}, '
                             f'speed: {train_speed:.2f} data/sec, eta: {eta_str}')
-                writer.add_scalar('Train/Loss', sum(loss_sum) / len(loss_sum), self.train_step)
-                writer.add_scalar('Train/Accuracy', (sum(accuracies) / len(accuracies)), self.train_step)
+                writer.add_scalar('Train/Loss', self.train_loss, self.train_log_step)
+                writer.add_scalar('Train/Accuracy', self.train_acc, self.train_log_step)
                 # 记录学习率
-                writer.add_scalar('Train/lr', self.scheduler.get_last_lr()[0], self.train_step)
+                writer.add_scalar('Train/lr', self.scheduler.get_last_lr()[0], self.train_log_step)
                 train_times, accuracies, loss_sum = [], [], []
-                self.train_step += 1
+                self.train_log_step += 1
             start = time.time()
             self.scheduler.step()
 
     def train(self,
               save_model_path='models/',
               resume_model=None,
               pretrained_model=None):
@@ -383,42 +390,49 @@
             self.model = torch.nn.parallel.DistributedDataParallel(self.model, device_ids=[local_rank])
         logger.info('训练数据：{}'.format(len(self.train_dataset)))
 
         self.__load_pretrained(pretrained_model=pretrained_model)
         # 加载恢复模型
         last_epoch, best_acc = self.__load_checkpoint(save_model_path=save_model_path, resume_model=resume_model)
 
-        test_step, self.train_step = 0, 0
+        self.train_loss, self.train_acc = None, None
+        self.eval_loss, self.eval_acc = None, None
+        self.test_log_step, self.train_log_step = 0, 0
         last_epoch += 1
         if local_rank == 0:
             writer.add_scalar('Train/lr', self.scheduler.get_last_lr()[0], last_epoch)
+        # 最大步数
+        self.max_step = len(self.train_loader) * self.configs.train_conf.max_epoch
+        self.train_step = max(last_epoch, 0) * len(self.train_loader)
         # 开始训练
         for epoch_id in range(last_epoch, self.configs.train_conf.max_epoch):
+            if self.stop_train: break
             epoch_id += 1
             start_epoch = time.time()
             # 训练一个epoch
             self.__train_epoch(epoch_id=epoch_id, local_rank=local_rank, writer=writer, nranks=nranks)
             # 多卡训练只使用一个进程执行评估和保存模型
             if local_rank == 0:
+                if self.stop_eval: continue
                 logger.info('=' * 70)
-                loss, acc = self.evaluate()
+                self.eval_loss, self.eval_acc = self.evaluate()
                 logger.info('Test epoch: {}, time/epoch: {}, loss: {:.5f}, accuracy: {:.5f}'.format(
-                    epoch_id, str(timedelta(seconds=(time.time() - start_epoch))), loss, acc))
+                    epoch_id, str(timedelta(seconds=(time.time() - start_epoch))), self.eval_loss, self.eval_acc))
                 logger.info('=' * 70)
-                writer.add_scalar('Test/Accuracy', acc, test_step)
-                writer.add_scalar('Test/Loss', loss, test_step)
-                test_step += 1
+                writer.add_scalar('Test/Accuracy', self.eval_acc, self.test_log_step)
+                writer.add_scalar('Test/Loss', self.eval_loss, self.test_log_step)
+                self.test_log_step += 1
                 self.model.train()
                 # # 保存最优模型
-                if acc >= best_acc:
-                    best_acc = acc
-                    self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id, best_acc=acc,
+                if self.eval_acc >= best_acc:
+                    best_acc = self.eval_acc
+                    self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id, best_acc=self.eval_acc,
                                            best_model=True)
                 # 保存模型
-                self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id, best_acc=acc)
+                self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id, best_acc=self.eval_acc)
 
     def evaluate(self, resume_model=None, save_matrix_path=None):
         """
         评估模型
         :param resume_model: 所使用的模型
         :param save_matrix_path: 保存混合矩阵的路径
         :return: 评估结果
@@ -439,14 +453,15 @@
             eval_model = self.model.module
         else:
             eval_model = self.model
 
         accuracies, losses, preds, labels = [], [], [], []
         with torch.no_grad():
             for batch_id, (audio, label, input_lens_ratio) in enumerate(tqdm(self.test_loader)):
+                if self.stop_eval: break
                 audio = audio.to(self.device)
                 input_lens_ratio = input_lens_ratio.to(self.device)
                 label = label.to(self.device).long()
                 features, _ = self.audio_featurizer(audio, input_lens_ratio)
                 output = eval_model(features)
                 los = self.loss(output, label)
                 # 计算准确率
@@ -456,22 +471,24 @@
                 label = label.data.cpu().numpy()
                 output = output.data.cpu().numpy()
                 pred = np.argmax(output, axis=1)
                 preds.extend(pred.tolist())
                 # 真实标签
                 labels.extend(label.tolist())
                 losses.append(los.data.cpu().numpy())
-        loss = float(sum(losses) / len(losses))
-        acc = float(sum(accuracies) / len(accuracies))
+        loss = float(sum(losses) / len(losses)) if len(losses) > 0 else -1
+        acc = float(sum(accuracies) / len(accuracies)) if len(accuracies) > 0 else -1
         # 保存混合矩阵
         if save_matrix_path is not None:
-            cm = confusion_matrix(labels, preds)
-            plot_confusion_matrix(cm=cm, save_path=os.path.join(save_matrix_path, f'{int(time.time())}.png'),
-                                  class_labels=self.class_labels)
-
+            try:
+                cm = confusion_matrix(labels, preds)
+                plot_confusion_matrix(cm=cm, save_path=os.path.join(save_matrix_path, f'{int(time.time())}.png'),
+                                      class_labels=self.class_labels)
+            except Exception as e:
+                logger.error(f'保存混淆矩阵失败：{e}')
         self.model.train()
         return loss, acc
 
     def export(self, save_model_path='models/', resume_model='models/EcapaTdnn_Fbank/best_model/'):
         """
         导出预测模型
         :param save_model_path: 模型保存的路径
```

## macls/data_utils/audio.py

```diff
@@ -44,21 +44,21 @@
 
     def __ne__(self, other):
         """返回两个对象是否不相等"""
         return not self.__eq__(other)
 
     def __str__(self):
         """返回该音频的信息"""
-        return ("%s: num_samples=%d, sample_rate=%d, duration=%.2fsec, "
-                "rms=%.2fdB" % (type(self), self.num_samples, self.sample_rate, self.duration, self.rms_db))
+        return (f"{type(self)}: num_samples={self.num_samples}, sample_rate={self.sample_rate}, "
+                f"duration={self.duration:.2f}sec, rms={self.rms_db:.2f}dB")
 
     @classmethod
     def from_file(cls, file):
         """从音频文件创建音频段
-        
+
         :param file: 文件路径，或者文件对象
         :type file: str, BufferedReader
         :return: 音频片段实例
         :rtype: AudioSegment
         """
         assert os.path.exists(file), f'文件不存在，请检查路径：{file}'
         try:
@@ -91,17 +91,17 @@
         # 从末尾开始计
         if start < 0.0: start += duration
         if end < 0.0: end += duration
         # 保证数据不越界
         if start < 0.0: start = 0.0
         if end > duration: end = duration
         if end < 0.0:
-            raise ValueError("切片结束位置(%f s)越界" % end)
+            raise ValueError(f"切片结束位置({end} s)越界")
         if start > end:
-            raise ValueError("切片开始位置(%f s)晚于切片结束位置(%f s)" % (start, end))
+            raise ValueError(f"切片开始位置({start} s)晚于切片结束位置({end} s)")
         start_frame = int(start * sample_rate)
         end_frame = int(end * sample_rate)
         sndfile.seek(start_frame)
         data = sndfile.read(frames=end_frame - start_frame, dtype='float32')
         return cls(data, sample_rate)
 
     @classmethod
@@ -153,15 +153,15 @@
     def concatenate(cls, *segments):
         """将任意数量的音频片段连接在一起
 
         :param *segments: 输入音频片段被连接
         :type *segments: tuple of AudioSegment
         :return: Audio segment instance as concatenating results.
         :rtype: AudioSegment
-        :raises ValueError: If the number of segments is zero, or if the 
+        :raises ValueError: If the number of segments is zero, or if the
                             sample_rate of any segments does not match.
         :raises TypeError: If any segment is not AudioSegment instance.
         """
         # Perform basic sanity-checks.
         if len(segments) == 0:
             raise ValueError("没有音频片段被给予连接")
         sample_rate = segments[0]._sample_rate
@@ -185,15 +185,15 @@
         :rtype: AudioSegment
         """
         samples = np.zeros(int(duration * sample_rate))
         return cls(samples, sample_rate)
 
     def to_wav_file(self, filepath, dtype='float32'):
         """保存音频段到磁盘为wav文件
-        
+
         :param filepath: WAV文件路径或文件对象，以保存音频段
         :type filepath: str|file
         :param dtype: Subtype for audio file. Options: 'int16', 'int32',
                       'float32', 'float64'. Default is 'float32'.
         :type dtype: str
         :raises TypeError: If dtype is not supported.
         """
@@ -216,24 +216,24 @@
 
         :param other: 包含样品的片段被添加进去
         :type other: AudioSegments
         :raise TypeError: 如果两个片段的类型不匹配
         :raise ValueError: 不能添加不同类型的段
         """
         if not isinstance(other, type(self)):
-            raise TypeError("不能添加不同类型的段: %s 和 %s" % (type(self), type(other)))
+            raise TypeError(f"不能添加不同类型的段: {type(self)} 和 {type(other)}")
         if self._sample_rate != other._sample_rate:
             raise ValueError("采样率必须匹配才能添加片段")
         if len(self._samples) != len(other._samples):
             raise ValueError("段长度必须匹配才能添加段")
         self._samples += other._samples
 
     def to_bytes(self, dtype='float32'):
         """创建包含音频内容的字节字符串
-        
+
         :param dtype: Data type for export samples. Options: 'int16', 'int32',
                       'float32', 'float64'. Default is 'float32'.
         :type dtype: str
         :return: Byte string containing audio content.
         :rtype: str
         """
         samples = self._convert_samples_from_float32(self._samples, dtype)
@@ -251,19 +251,19 @@
         samples = self._convert_samples_from_float32(self._samples, dtype)
         return samples
 
     def gain_db(self, gain):
         """对音频施加分贝增益。
 
         Note that this is an in-place transformation.
-        
-        :param gain: Gain in decibels to apply to samples. 
+
+        :param gain: Gain in decibels to apply to samples.
         :type gain: float|1darray
         """
-        self._samples *= 10.**(gain / 20.)
+        self._samples *= 10. ** (gain / 20.)
 
     def change_speed(self, speed_rate):
         """通过线性插值改变音频速度
 
         :param speed_rate: Rate of speed change:
                            speed_rate > 1.0, speed up the audio;
                            speed_rate = 1.0, unchanged;
@@ -292,19 +292,17 @@
                             normalization. This is to prevent nans when
                             attempting to normalize a signal consisting of
                             all zeros.
         :type max_gain_db: float
         :raises ValueError: If the required gain to normalize the segment to
                             the target_db value exceeds max_gain_db.
         """
-        if -np.inf == self.rms_db: return
         gain = target_db - self.rms_db
         if gain > max_gain_db:
-            raise ValueError(
-                "无法将段规范化到 %f dB，因为可能的增益已经超过max_gain_db (%f dB)" % (target_db, max_gain_db))
+            raise ValueError(f"无法将段规范化到{target_db}dB，音频增益{gain}增益已经超过max_gain_db ({max_gain_db}dB)")
         self.gain_db(min(max_gain_db, target_db - self.rms_db))
 
     def resample(self, target_sample_rate, filter='kaiser_best'):
         """按目标采样率重新采样音频
 
         Note that this is an in-place transformation.
 
@@ -337,15 +335,15 @@
         if sides == "beginning":
             padded = cls.concatenate(silence, self)
         elif sides == "end":
             padded = cls.concatenate(self, silence)
         elif sides == "both":
             padded = cls.concatenate(silence, self, silence)
         else:
-            raise ValueError("Unknown value for the sides %s" % sides)
+            raise ValueError(f"Unknown value for the sides {sides}")
         self._samples = padded._samples
 
     def shift(self, shift_ms):
         """音频偏移。如果shift_ms为正，则随时间提前移位;如果为负，则随时间延迟移位。填补静音以保持持续时间不变。
 
         Note that this is an in-place transformation.
 
@@ -381,21 +379,21 @@
         start_sec = 0.0 if start_sec is None else start_sec
         end_sec = self.duration if end_sec is None else end_sec
         if start_sec < 0.0:
             start_sec = self.duration + start_sec
         if end_sec < 0.0:
             end_sec = self.duration + end_sec
         if start_sec < 0.0:
-            raise ValueError("切片起始位置(%f s)越界" % start_sec)
+            raise ValueError(f"切片起始位置({start_sec} s)越界")
         if end_sec < 0.0:
-            raise ValueError("切片结束位置(%f s)越界" % end_sec)
+            raise ValueError(f"切片结束位置({end_sec} s)越界")
         if start_sec > end_sec:
-            raise ValueError("切片的起始位置(%f s)晚于结束位置(%f s)" % (start_sec, end_sec))
+            raise ValueError(f"切片的起始位置({start_sec} s)晚于结束位置({end_sec} s)")
         if end_sec > self.duration:
-            raise ValueError("切片结束位置(%f s)越界(> %f s)" % (end_sec, self.duration))
+            raise ValueError(f"切片结束位置({end_sec} s)越界(> {self.duration} s)")
         start_sample = int(round(start_sec * self._sample_rate))
         end_sample = int(round(end_sec * self._sample_rate))
         self._samples = self._samples[start_sample:end_sample]
 
     def random_subsegment(self, subsegment_length):
         """随机剪切指定长度的音频片段
 
@@ -429,17 +427,17 @@
                             to apply infinite gain to a zero signal.
         :type max_gain_db: float
         :raises ValueError: If the sample rate does not match between the two
                             audio segments, or if the duration of noise segments
                             is shorter than original audio segments.
         """
         if noise.sample_rate != self.sample_rate:
-            raise ValueError("噪声采样率(%d Hz)不等于基信号采样率(%d Hz)" % (noise.sample_rate, self.sample_rate))
+            raise ValueError(f"噪声采样率({noise.sample_rate} Hz)不等于基信号采样率({self.sample_rate} Hz)")
         if noise.duration < self.duration:
-            raise ValueError("噪声信号(%f秒)必须至少与基信号(%f秒)一样长" % (noise.duration, self.duration))
+            raise ValueError(f"噪声信号({noise.duration}秒)必须至少与基信号({self.duration}秒)一样长")
         noise_gain_db = min(self.rms_db - noise.rms_db - snr_dB, max_gain_db)
         noise_new = copy.deepcopy(noise)
         noise_new.random_subsegment(self.duration)
         noise_new.gain_db(noise_gain_db)
         self.superimpose(noise_new)
 
     def vad(self, top_db=20, overlap=200):
@@ -494,33 +492,37 @@
         """返回以分贝为单位的音频均方根能量
 
         :return: Root mean square energy in decibels.
         :rtype: float
         """
         # square root => multiply by 10 instead of 20 for dBs
         mean_square = np.mean(self._samples ** 2)
+        if mean_square == 0:
+            mean_square = 1
         return 10 * np.log10(mean_square)
 
-    def _convert_samples_to_float32(self, samples):
+    @staticmethod
+    def _convert_samples_to_float32(samples):
         """Convert sample type to float32.
 
         Audio sample type is usually integer or float-point.
         Integers will be scaled to [-1, 1] in float32.
         """
         float32_samples = samples.astype('float32')
         if samples.dtype in np.sctypes['int']:
             bits = np.iinfo(samples.dtype).bits
             float32_samples *= (1. / 2 ** (bits - 1))
         elif samples.dtype in np.sctypes['float']:
             pass
         else:
-            raise TypeError("Unsupported sample type: %s." % samples.dtype)
+            raise TypeError(f"Unsupported sample type: {samples.dtype}.")
         return float32_samples
 
-    def _convert_samples_from_float32(self, samples, dtype):
+    @staticmethod
+    def _convert_samples_from_float32(samples, dtype):
         """Convert sample type from float32 to dtype.
 
         Audio sample type is usually integer or float-point. For integer
         type, float32 will be rescaled from [-1, 1] to the maximum range
         supported by the integer type.
 
         This is for writing a audio file.
@@ -536,9 +538,9 @@
             output_samples[output_samples < min_val] = min_val
         elif samples.dtype in np.sctypes['float']:
             min_val = np.finfo(dtype).min
             max_val = np.finfo(dtype).max
             output_samples[output_samples > max_val] = max_val
             output_samples[output_samples < min_val] = min_val
         else:
-            raise TypeError("Unsupported sample type: %s." % samples.dtype)
+            raise TypeError(f"Unsupported sample type: {samples.dtype}.")
         return output_samples.astype(dtype)
```

## macls/data_utils/reader.py

```diff
@@ -49,15 +49,15 @@
         self.noises_path = None
         # 获取数据列表
         with open(data_list_path, 'r', encoding='utf-8') as f:
             self.lines = f.readlines()
 
     def __getitem__(self, idx):
         # 分割音频路径和标签
-        audio_path, label = self.lines[idx].replace('\n', '').split('\t')
+        audio_path, label = self.lines[idx].strip().split('\t')
         # 读取音频
         audio_segment = AudioSegment.from_file(audio_path)
         # 裁剪静音
         if self.do_vad:
             audio_segment.vad()
         # 数据太短不利于训练
         if self.mode == 'train':
```

## macls/data_utils/utils.py

```diff
@@ -1,10 +1,11 @@
 import io
 import itertools
 
+import gc
 import av
 import librosa
 import numpy as np
 import torch
 
 
 def vad(wav, top_db=20, overlap=200):
@@ -77,25 +78,28 @@
       A float32 Numpy array.
     """
     resampler = av.audio.resampler.AudioResampler(format="s16", layout="mono", rate=sample_rate)
 
     raw_buffer = io.BytesIO()
     dtype = None
 
-    with av.open(file, metadata_errors="ignore") as container:
+    with av.open(file, mode="r", metadata_errors="ignore") as container:
         frames = container.decode(audio=0)
         frames = _ignore_invalid_frames(frames)
         frames = _group_frames(frames, 500000)
         frames = _resample_frames(frames, resampler)
 
         for frame in frames:
             array = frame.to_ndarray()
             dtype = array.dtype
             raw_buffer.write(array)
 
+    del resampler
+    gc.collect()
+
     audio = np.frombuffer(raw_buffer.getbuffer(), dtype=dtype)
 
     # Convert s16 back to f32.
     return audio.astype(np.float32) / 32768.0
 
 
 def _ignore_invalid_frames(frames):
```

## macls/utils/utils.py

```diff
@@ -51,36 +51,52 @@
         return dict_obj
     inst = Dict()
     for k, v in dict_obj.items():
         inst[k] = dict_to_object(v)
     return inst
 
 
-def plot_confusion_matrix(cm, save_path, class_labels, title='Confusion Matrix', show=False):
+def plot_confusion_matrix(cm, save_path, class_labels, show=False):
+    """
+    绘制混淆矩阵
+    @param cm: 混淆矩阵, 一个二维数组，表示预测结果与真实结果的混淆情况。
+    @param save_path: 保存路径, 字符串，指定混淆矩阵图像的保存位置。
+    @param class_labels: 类别名称, 一个列表，包含各个类别的名称。
+    @param show: 是否显示图像, 布尔值，控制是否在绘图窗口显示混淆矩阵图像。
+    """
+    # 检测类别名称是否包含中文，是则设置相应字体
+    s = ''.join(class_labels)
+    is_ascii = all(ord(c) < 128 for c in s)
+    if not is_ascii:
+        plt.rcParams['font.sans-serif'] = ['SimHei']
+        plt.rcParams['axes.unicode_minus'] = False
+
+    # 初始化绘图参数并绘制混淆矩阵
     plt.figure(figsize=(12, 8), dpi=100)
     np.set_printoptions(precision=2)
-    # 在混淆矩阵中每格的概率值
+    # 在混淆矩阵中绘制每个格子的概率值
     ind_array = np.arange(len(class_labels))
     x, y = np.meshgrid(ind_array, ind_array)
     for x_val, y_val in zip(x.flatten(), y.flatten()):
         c = cm[y_val][x_val] / (np.sum(cm[:, x_val]) + 1e-6)
-        # 忽略值太小的
+        # 忽略概率值太小的格子
         if c < 1e-4: continue
         plt.text(x_val, y_val, "%0.2f" % (c,), color='red', fontsize=15, va='center', ha='center')
-    m = np.max(cm)
+    m = np.sum(cm, axis=0) + 1e-6
     plt.imshow(cm / m, interpolation='nearest', cmap=plt.cm.binary)
-    plt.title(title)
+    plt.title('Confusion Matrix' if is_ascii else '混合矩阵')
     plt.colorbar()
+    # 设置类别标签
     xlocations = np.array(range(len(class_labels)))
     plt.xticks(xlocations, class_labels, rotation=90)
     plt.yticks(xlocations, class_labels)
-    plt.ylabel('Actual label')
-    plt.xlabel('Predict label')
+    plt.ylabel('Actual label' if is_ascii else '实际标签')
+    plt.xlabel('Predict label' if is_ascii else '预测标签')
 
-    # offset the tick
+    # 调整刻度标记位置，提高可视化效果
     tick_marks = np.array(range(len(class_labels))) + 0.5
     plt.gca().set_xticks(tick_marks, minor=True)
     plt.gca().set_yticks(tick_marks, minor=True)
     plt.gca().xaxis.set_ticks_position('none')
     plt.gca().yaxis.set_ticks_position('none')
     plt.grid(True, which='minor', linestyle='-')
     plt.gcf().subplots_adjust(bottom=0.15)
```

## Comparing `macls-0.4.2.dist-info/LICENSE` & `macls-0.4.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `macls-0.4.2.dist-info/METADATA` & `macls-0.4.3.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,48 +1,47 @@
 Metadata-Version: 2.1
 Name: macls
-Version: 0.4.2
+Version: 0.4.3
 Summary: Audio Classification toolkit on Pytorch
 Home-page: https://github.com/yeyupiaoling/AudioClassification-Pytorch
+Download-URL: https://github.com/yeyupiaoling/AudioClassification-Pytorch.git
 Author: yeyupiaoling
 License: Apache License 2.0
-Download-URL: https://github.com/yeyupiaoling/AudioClassification-Pytorch.git
 Keywords: audio,pytorch
-Platform: UNKNOWN
 Classifier: Intended Audience :: Developers
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Natural Language :: Chinese (Simplified)
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.5
 Classifier: Programming Language :: Python :: 3.6
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Topic :: Utilities
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: numpy (>=1.19.2)
-Requires-Dist: scipy (>=1.6.3)
-Requires-Dist: librosa (~=0.9.1)
-Requires-Dist: soundfile (>=0.12.1)
-Requires-Dist: soundcard (>=0.4.2)
-Requires-Dist: resampy (==0.2.2)
-Requires-Dist: numba (>=0.53.0)
-Requires-Dist: pydub (~=0.25.1)
-Requires-Dist: matplotlib (>=3.5.2)
-Requires-Dist: termcolor (>=1.1.0)
-Requires-Dist: typeguard (==2.13.3)
-Requires-Dist: pillow (>=8.3.2)
-Requires-Dist: tqdm (>=4.64.1)
-Requires-Dist: visualdl (>=2.2.3)
-Requires-Dist: pyyaml (>=5.4.1)
-Requires-Dist: scikit-learn (>=1.0.2)
-Requires-Dist: torchinfo (>=1.7.2)
-Requires-Dist: av (>=10.0.0)
+Requires-Dist: numpy >=1.19.2
+Requires-Dist: scipy >=1.6.3
+Requires-Dist: librosa >=0.9.1
+Requires-Dist: soundfile >=0.12.1
+Requires-Dist: soundcard >=0.4.2
+Requires-Dist: resampy >=0.2.2
+Requires-Dist: numba >=0.53.0
+Requires-Dist: pydub ~=0.25.1
+Requires-Dist: matplotlib >=3.5.2
+Requires-Dist: termcolor >=1.1.0
+Requires-Dist: typeguard ==2.13.3
+Requires-Dist: pillow >=8.3.2
+Requires-Dist: tqdm >=4.64.1
+Requires-Dist: visualdl >=2.2.3
+Requires-Dist: pyyaml >=5.4.1
+Requires-Dist: scikit-learn >=1.0.2
+Requires-Dist: torchinfo >=1.7.2
+Requires-Dist: av >=10.0.0
 
 简体中文 | [English](./README_en.md)
 
 # 基于Pytorch实现的声音分类系统
 
 ![python version](https://img.shields.io/badge/python-3.8+-orange.svg)
 ![GitHub forks](https://img.shields.io/github/forks/yeyupiaoling/AudioClassification-Pytorch)
@@ -84,25 +83,25 @@
 - Res2Net：[Res2Net: A New Multi-scale Backbone Architecture](https://arxiv.org/abs/1904.01169)
 - ResNetSE：[Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)
 - CAMPPlus：[CAM++: A Fast and Efficient Network for Speaker Verification Using Context-Aware Masking](https://arxiv.org/abs/2303.00332v3)
 - ERes2Net：[An Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification](https://arxiv.org/abs/2305.12838v1)
 
 # 模型测试表
 
-|      模型      | Params(M) | 预处理方法 |        数据集        | 类别数量 |   准确率   |   获取模型   |
-|:------------:|:---------:|:-----:|:-----------------:|:----:|:-------:|:--------:|
-|   ResNetSE   |    7.8    | Flank |   UrbanSound8K    |  10  | 0.98863 | 加入知识星球获取 |
-|   CAMPPlus   |    7.1    | Flank |   UrbanSound8K    |  10  | 0.97727 | 加入知识星球获取 |
-|   ERes2Net   |    6.6    | Flank |   UrbanSound8K    |  10  | 0.96590 | 加入知识星球获取 |
-| PANNS（CNN10） |    5.2    | Flank |   UrbanSound8K    |  10  | 0.96590 | 加入知识星球获取 |
-|   Res2Net    |    5.0    | Flank |   UrbanSound8K    |  10  | 0.94318 | 加入知识星球获取 |
-|     TDNN     |    2.6    | Flank |   UrbanSound8K    |  10  | 0.92045 | 加入知识星球获取 |
-|  EcapaTdnn   |    6.1    | Flank |   UrbanSound8K    |  10  | 0.91876 | 加入知识星球获取 |
-|   CAMPPlus   |    6.1    | Flank | CN-Celeb和VoxCeleb |  2   | 0.99320 | 加入知识星球获取 |
-|   ResNetSE   |    9.1    | Flank | CN-Celeb和VoxCeleb |  2   |         | 加入知识星球获取 |
+|      模型      | Params(M) | 预处理方法 |        数据集        |   类别数量   |   准确率   |   获取模型   |
+|:------------:|:---------:|:-----:|:-----------------:|:--------:|:-------:|:--------:|
+|   ResNetSE   |    7.8    | Flank |   UrbanSound8K    |    10    | 0.98863 | 加入知识星球获取 |
+|   CAMPPlus   |    7.1    | Flank |   UrbanSound8K    |    10    | 0.97727 | 加入知识星球获取 |
+|   ERes2Net   |    6.6    | Flank |   UrbanSound8K    |    10    | 0.96590 | 加入知识星球获取 |
+| PANNS（CNN10） |    5.2    | Flank |   UrbanSound8K    |    10    | 0.96590 | 加入知识星球获取 |
+|   Res2Net    |    5.0    | Flank |   UrbanSound8K    |    10    | 0.94318 | 加入知识星球获取 |
+|     TDNN     |    2.6    | Flank |   UrbanSound8K    |    10    | 0.92045 | 加入知识星球获取 |
+|  EcapaTdnn   |    6.1    | Flank |   UrbanSound8K    |    10    | 0.91876 | 加入知识星球获取 |
+|   CAMPPlus   |    6.1    | Flank | CN-Celeb和VoxCeleb | 2(中英文语种) | 0.99320 | 加入知识星球获取 |
+|   ResNetSE   |    9.8    | Flank | CN-Celeb和VoxCeleb | 2(中英文语种) | 0.99056 | 加入知识星球获取 |
 
 ## 安装环境
 
  - 首先安装的是Pytorch的GPU版本，如果已经安装过了，请跳过。
 ```shell
 conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia
 ```
@@ -114,15 +113,15 @@
 python -m pip install macls -U -i https://pypi.tuna.tsinghua.edu.cn/simple
 ```
 
 **建议源码安装**，源码安装能保证使用最新代码。
 ```shell
 git clone https://github.com/yeyupiaoling/AudioClassification-Pytorch.git
 cd AudioClassification-Pytorch/
-python setup.py install
+pip install .
 ```
 
 ## 准备数据
 
 生成数据列表，用于下一步的读取需要，`audio_path`为音频文件路径，用户需要提前把音频数据集存放在`dataset/audio`目录下，每个文件夹存放一个类别的音频数据，每条音频数据长度在3秒以上，如 `dataset/audio/鸟叫声/······`。`audio`是数据列表存放的位置，生成的数据类别的格式为 `音频路径\t音频对应的类别标签`，音频路径和标签用制表符 `\t`分开。读者也可以根据自己存放数据的方式修改以下函数。
 
 以Urbansound8K为例，Urbansound8K是目前应用较为广泛的用于自动城市环境声分类研究的公共数据集，包含10个分类：空调声、汽车鸣笛声、儿童玩耍声、狗叫声、钻孔声、引擎空转声、枪声、手提钻、警笛声和街道音乐声。数据集下载地址：[UrbanSound8K.tar.gz](https://aistudio.baidu.com/aistudio/datasetdetail/36625)。以下是针对Urbansound8K生成数据列表的函数。如果读者想使用该数据集，请下载并解压到 `dataset`目录下，把生成数据列表代码改为以下代码。
@@ -284,16 +283,56 @@
 Estimated Total Size (MB): 35.07
 ==========================================================================================
 [2023-08-07 22:54:26.726095 INFO   ] trainer:train:344 - 训练数据：8644
 [2023-08-07 22:54:30.092504 INFO   ] trainer:__train_epoch:296 - Train epoch: [1/30], batch: [0/4], loss: 2.57033, accuracy: 0.06250, learning rate: 0.00001000, speed: 19.02 data/sec, eta: 0:06:43
 ```
 
 # 评估
-每轮训练结束可以执行评估，评估会出来输出准确率，还保存了混合矩阵图片，保存路径`output/images/`，如下。
-![混合矩阵](docs/images/image1.png)
+
+执行下面命令执行评估。
+
+```shell
+python eval.py --configs=configs/bi_lstm.yml
+```
+
+评估输出如下：
+```shell
+[2024-02-03 15:13:25.469242 INFO   ] trainer:evaluate:461 - 成功加载模型：models/CAMPPlus_Fbank/best_model/model.pth
+100%|██████████████████████████████| 150/150 [00:00<00:00, 1281.96it/s]
+评估消耗时间：1s，loss：0.61840，accuracy：0.87333
+```
+
+评估会出来输出准确率，还保存了混淆矩阵图片，保存路径`output/images/`，如下。
+
+<br/>
+<div align="center">
+<img src="docs/images/image1.png" alt="混淆矩阵" width="600">
+</div>
+
+
+注意：如果类别标签是中文的，需要设置安装字体才能正常显示，一般情况下Windows无需安装，Ubuntu需要安装。如果Windows确实是确实字体，只需要[字体文件](https://github.com/tracyone/program_font)这里下载`.ttf`格式的文件，复制到`C:\Windows\Fonts`即可。Ubuntu系统操作如下。
+
+1. 安装字体
+```shell
+git clone https://github.com/tracyone/program_font && cd program_font && ./install.sh
+```
+
+2. 执行下面Python代码
+```python
+import matplotlib
+import shutil
+import os
+
+path = matplotlib.matplotlib_fname()
+path = path.replace('matplotlibrc', 'fonts/ttf/')
+print(path)
+shutil.copy('/usr/share/fonts/MyFonts/simhei.ttf', path)
+user_dir = os.path.expanduser('~')
+shutil.rmtree(f'{user_dir}/.cache/matplotlib', ignore_errors=True)
+```
 
 # 预测
 
 在训练结束之后，我们得到了一个模型参数文件，我们使用这个模型预测音频。
 
 ```shell
 python infer.py --audio_path=dataset/UrbanSound8K/audio/fold5/156634-5-2-5.wav
@@ -322,9 +361,7 @@
 
 # 参考资料
 
 1. https://github.com/PaddlePaddle/PaddleSpeech
 2. https://github.com/yeyupiaoling/PaddlePaddle-MobileFaceNets
 3. https://github.com/yeyupiaoling/PPASR
 4. https://github.com/alibaba-damo-academy/3D-Speaker
-
-
```

## Comparing `macls-0.4.2.dist-info/RECORD` & `macls-0.4.3.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-macls/__init__.py,sha256=4S-HaNS8dF5PCnQXiMS641z3MqcTyTMkZig7aH25-sE,196
+macls/__init__.py,sha256=iUVruvBi_BFZRV6LsDpD2SIswAjsmjVViarSBK9R6C0,196
 macls/predict.py,sha256=tFBFj0HhxWY0_ep8rpH5Srzxb7miA8iDS2rhKrSHb2c,9557
-macls/trainer.py,sha256=4ZN999InH-iKQxbNq_j3uV3BNHAYtaTIbbiizr7Qqog,27397
+macls/trainer.py,sha256=aEep-Qgz218_-CTY-IJiCTvEw__aZ23LXlTH77ddw08,28617
 macls/data_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-macls/data_utils/audio.py,sha256=xJWwFXKrtitU7GZZuNmqmXPsq1Bzj-qnYd49naBvVyo,22287
+macls/data_utils/audio.py,sha256=iyLME4AECoYfQYAXsSWHqrzlJjuAZV-HiwxzEbDDrxg,22218
 macls/data_utils/collate_fn.py,sha256=GxqMcZ5Q2ScQxj6crIUWeVygzWxIk9KAbC6GWW6awrQ,965
 macls/data_utils/featurizer.py,sha256=-MAiHLattWuHK7fa4Wr1-7Jqw_AqKuY7hr-73nEqPBk,3587
-macls/data_utils/reader.py,sha256=3wJw3iQum199U2n2CVxWd400e1OUTbyrcO2Ib8YJru4,5640
+macls/data_utils/reader.py,sha256=CbCmKAdXIkumhTwrM46fWm7FyHmYTGFXlD6ZPVcx1mo,5630
 macls/data_utils/spec_aug.py,sha256=Sr9-Ss6I7LA3TGMqzML_X-nb_nnMfUxt5KbsA6HJpzM,1572
-macls/data_utils/utils.py,sha256=mNpCY4PcaeKCSjEB6gQWsiyUBRNDvwrY9YizFTRuaww,5652
+macls/data_utils/utils.py,sha256=do9N5iTXwBwBFk2EMVTPuaIWsI1DazxKH-1Nmrlsdmc,5712
 macls/metric/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 macls/metric/metrics.py,sha256=Asj6VaDQXxmcB_HRIJt7v4zdRBgLTyn3Q2mDTFPcgEM,329
 macls/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 macls/models/campplus.py,sha256=vAuddLBPDgUEB85BjVITaqNYO09igDbh_Bxok4N0lH4,13608
 macls/models/ecapa_tdnn.py,sha256=bjiVtLSA01I97CYcAkrvCdx0ekGZMNexOXe5VeXqHb0,5375
 macls/models/eres2net.py,sha256=lnATqi9MKaLoBTe34Jox48M8rpnadQkOU7bA5NnH13k,10504
 macls/models/panns.py,sha256=Ylg4O5TDv3BoPn5v0HXZD0wASdy9jAvhvTTP9FWDPpo,9707
@@ -19,13 +19,13 @@
 macls/models/res2net.py,sha256=N1jKs4PI2mISYWPc9ldM4AmWaQd6c_gQC5rwwPAMfw8,6776
 macls/models/resnet_se.py,sha256=U64nKP2WCuzW6YVROhgrMsy4G7R961Uv-zWT5e-A-9s,5609
 macls/models/tdnn.py,sha256=foMiLbpQ4loRuo-2G31J6yCRq8ZSu48BSpx7OUy4M50,3050
 macls/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 macls/utils/logger.py,sha256=-ssorx8FlHA_wrd2Eq6f4HkOqaOG2YseBGvYAo8NXN8,2839
 macls/utils/record.py,sha256=2i4kz5kPDa9KkbAK_Q34sVIXOkD9TroPROIe5QdzqWg,1067
 macls/utils/scheduler.py,sha256=RXgRf_sntkOjruj30pp6uPE7dqFR06jDa4tRogpl5Us,1496
-macls/utils/utils.py,sha256=-KEdKJYlZ9q3Cjb0re1bNMf3ZVCi6EmpDydGYM9BuJU,3364
-macls-0.4.2.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
-macls-0.4.2.dist-info/METADATA,sha256=ZnLRiKtW7GO1PjWK-Cec-WIWCW2pUk8-2yusTPGhVPE,20301
-macls-0.4.2.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-macls-0.4.2.dist-info/top_level.txt,sha256=fmi1Rmptc1CNX_eoWedGPlbhlTSDi3liMSfVFvHYE1Q,6
-macls-0.4.2.dist-info/RECORD,,
+macls/utils/utils.py,sha256=pIlc9i-Bk4dLkaYkJdyivfu9UOouec0iQOiZb0xhfLA,4300
+macls-0.4.3.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
+macls-0.4.3.dist-info/METADATA,sha256=CWd20a9i0NNvLhyT5YskRgj__6P9DYs95jM6LSzzlqw,21616
+macls-0.4.3.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+macls-0.4.3.dist-info/top_level.txt,sha256=fmi1Rmptc1CNX_eoWedGPlbhlTSDi3liMSfVFvHYE1Q,6
+macls-0.4.3.dist-info/RECORD,,
```

