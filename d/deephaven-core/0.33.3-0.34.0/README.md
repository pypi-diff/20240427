# Comparing `tmp/deephaven_core-0.33.3-py3-none-any.whl.zip` & `tmp/deephaven_core-0.34.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,82 +1,84 @@
-Zip file size: 161509 bytes, number of entries: 80
--rw-r--r--  2.0 unx     1475 b- defN 24-Mar-26 18:08 deephaven/__init__.py
--rw-r--r--  2.0 unx      615 b- defN 24-Mar-26 18:08 deephaven/_dep.py
--rw-r--r--  2.0 unx      970 b- defN 24-Mar-26 18:08 deephaven/_gc.py
--rw-r--r--  2.0 unx      641 b- defN 24-Mar-26 18:08 deephaven/_jpy.py
--rw-r--r--  2.0 unx    21107 b- defN 24-Mar-26 18:08 deephaven/_udf.py
--rw-r--r--  2.0 unx     6578 b- defN 24-Mar-26 18:08 deephaven/_wrapper.py
--rw-r--r--  2.0 unx    15143 b- defN 24-Mar-26 18:08 deephaven/agg.py
--rw-r--r--  2.0 unx     1977 b- defN 24-Mar-26 18:08 deephaven/appmode.py
--rw-r--r--  2.0 unx     5152 b- defN 24-Mar-26 18:08 deephaven/arrow.py
--rw-r--r--  2.0 unx     3635 b- defN 24-Mar-26 18:08 deephaven/calendar.py
--rw-r--r--  2.0 unx     7658 b- defN 24-Mar-26 18:08 deephaven/column.py
--rw-r--r--  2.0 unx     3123 b- defN 24-Mar-26 18:08 deephaven/constants.py
--rw-r--r--  2.0 unx     4946 b- defN 24-Mar-26 18:08 deephaven/csv.py
--rw-r--r--  2.0 unx     2890 b- defN 24-Mar-26 18:08 deephaven/dherror.py
--rw-r--r--  2.0 unx    17171 b- defN 24-Mar-26 18:08 deephaven/dtypes.py
--rw-r--r--  2.0 unx     3710 b- defN 24-Mar-26 18:08 deephaven/execution_context.py
--rw-r--r--  2.0 unx     5153 b- defN 24-Mar-26 18:08 deephaven/filters.py
--rw-r--r--  2.0 unx      650 b- defN 24-Mar-26 18:08 deephaven/html.py
--rw-r--r--  2.0 unx    12067 b- defN 24-Mar-26 18:08 deephaven/jcompat.py
--rw-r--r--  2.0 unx     8988 b- defN 24-Mar-26 18:08 deephaven/liveness_scope.py
--rw-r--r--  2.0 unx     7271 b- defN 24-Mar-26 18:08 deephaven/numpy.py
--rw-r--r--  2.0 unx    10291 b- defN 24-Mar-26 18:08 deephaven/pandas.py
--rw-r--r--  2.0 unx    14629 b- defN 24-Mar-26 18:08 deephaven/parquet.py
--rw-r--r--  2.0 unx     9429 b- defN 24-Mar-26 18:08 deephaven/perfmon.py
--rw-r--r--  2.0 unx        1 b- defN 24-Mar-26 18:08 deephaven/py.typed
--rw-r--r--  2.0 unx     2690 b- defN 24-Mar-26 18:08 deephaven/query_library.py
--rw-r--r--  2.0 unx     3075 b- defN 24-Mar-26 18:08 deephaven/replay.py
--rw-r--r--  2.0 unx   163251 b- defN 24-Mar-26 18:08 deephaven/table.py
--rw-r--r--  2.0 unx    17732 b- defN 24-Mar-26 18:08 deephaven/table_factory.py
--rw-r--r--  2.0 unx    16346 b- defN 24-Mar-26 18:08 deephaven/table_listener.py
--rw-r--r--  2.0 unx    30792 b- defN 24-Mar-26 18:08 deephaven/time.py
--rw-r--r--  2.0 unx     9071 b- defN 24-Mar-26 18:08 deephaven/update_graph.py
--rw-r--r--  2.0 unx    79447 b- defN 24-Mar-26 18:08 deephaven/updateby.py
--rw-r--r--  2.0 unx     1155 b- defN 24-Mar-26 18:08 deephaven/uri.py
--rw-r--r--  2.0 unx     3243 b- defN 24-Mar-26 18:08 deephaven/dbc/__init__.py
--rw-r--r--  2.0 unx     1794 b- defN 24-Mar-26 18:08 deephaven/dbc/adbc.py
--rw-r--r--  2.0 unx     1668 b- defN 24-Mar-26 18:08 deephaven/dbc/odbc.py
--rw-r--r--  2.0 unx     1316 b- defN 24-Mar-26 18:08 deephaven/experimental/__init__.py
--rw-r--r--  2.0 unx     4815 b- defN 24-Mar-26 18:08 deephaven/experimental/ema.py
--rw-r--r--  2.0 unx     4106 b- defN 24-Mar-26 18:08 deephaven/experimental/outer_joins.py
--rw-r--r--  2.0 unx     6654 b- defN 24-Mar-26 18:08 deephaven/experimental/s3.py
--rw-r--r--  2.0 unx     1570 b- defN 24-Mar-26 18:08 deephaven/experimental/sql.py
--rw-r--r--  2.0 unx     7534 b- defN 24-Mar-26 18:08 deephaven/learn/__init__.py
--rw-r--r--  2.0 unx     3120 b- defN 24-Mar-26 18:08 deephaven/learn/gather.py
--rw-r--r--  2.0 unx      345 b- defN 24-Mar-26 18:08 deephaven/pandasplugin/__init__.py
--rw-r--r--  2.0 unx      573 b- defN 24-Mar-26 18:08 deephaven/pandasplugin/pandas_as_table.py
--rw-r--r--  2.0 unx      574 b- defN 24-Mar-26 18:08 deephaven/plot/__init__.py
--rw-r--r--  2.0 unx     2289 b- defN 24-Mar-26 18:08 deephaven/plot/axisformat.py
--rw-r--r--  2.0 unx     1479 b- defN 24-Mar-26 18:08 deephaven/plot/axistransform.py
--rw-r--r--  2.0 unx    16540 b- defN 24-Mar-26 18:08 deephaven/plot/color.py
--rw-r--r--  2.0 unx   110210 b- defN 24-Mar-26 18:08 deephaven/plot/figure.py
--rw-r--r--  2.0 unx     1659 b- defN 24-Mar-26 18:08 deephaven/plot/font.py
--rw-r--r--  2.0 unx     3515 b- defN 24-Mar-26 18:08 deephaven/plot/linestyle.py
--rw-r--r--  2.0 unx     1285 b- defN 24-Mar-26 18:08 deephaven/plot/plotstyle.py
--rw-r--r--  2.0 unx     2493 b- defN 24-Mar-26 18:08 deephaven/plot/selectable_dataset.py
--rw-r--r--  2.0 unx     1213 b- defN 24-Mar-26 18:08 deephaven/plot/shape.py
--rw-r--r--  2.0 unx       69 b- defN 24-Mar-26 18:08 deephaven/server/__init__.py
--rw-r--r--  2.0 unx     2233 b- defN 24-Mar-26 18:08 deephaven/server/executors.py
--rw-r--r--  2.0 unx     1069 b- defN 24-Mar-26 18:08 deephaven/stream/__init__.py
--rw-r--r--  2.0 unx     5640 b- defN 24-Mar-26 18:08 deephaven/stream/table_publisher.py
--rw-r--r--  2.0 unx     1019 b- defN 24-Mar-26 18:08 deephaven/stream/kafka/__init__.py
--rw-r--r--  2.0 unx     7084 b- defN 24-Mar-26 18:08 deephaven/stream/kafka/cdc.py
--rw-r--r--  2.0 unx    23648 b- defN 24-Mar-26 18:08 deephaven/stream/kafka/consumer.py
--rw-r--r--  2.0 unx    13972 b- defN 24-Mar-26 18:08 deephaven/stream/kafka/producer.py
--rw-r--r--  2.0 unx        0 b- defN 24-Mar-26 18:08 deephaven_internal/__init__.py
--rw-r--r--  2.0 unx     1576 b- defN 24-Mar-26 18:08 deephaven_internal/java_threads.py
--rw-r--r--  2.0 unx     3593 b- defN 24-Mar-26 18:08 deephaven_internal/stream.py
--rw-r--r--  2.0 unx      952 b- defN 24-Mar-26 18:08 deephaven_internal/auto_completer/__init__.py
--rw-r--r--  2.0 unx     8476 b- defN 24-Mar-26 18:08 deephaven_internal/auto_completer/_completer.py
--rw-r--r--  2.0 unx     3422 b- defN 24-Mar-26 18:08 deephaven_internal/jvm/__init__.py
--rw-r--r--  2.0 unx      582 b- defN 24-Mar-26 18:08 deephaven_internal/plugin/__init__.py
--rw-r--r--  2.0 unx     1486 b- defN 24-Mar-26 18:08 deephaven_internal/plugin/register.py
--rw-r--r--  2.0 unx      961 b- defN 24-Mar-26 18:08 deephaven_internal/plugin/js/__init__.py
--rw-r--r--  2.0 unx     3470 b- defN 24-Mar-26 18:08 deephaven_internal/plugin/object/__init__.py
--rw-r--r--  2.0 unx     1736 b- defN 24-Mar-26 18:08 deephaven_internal/script_session/__init__.py
--rw-r--r--  2.0 unx     2773 b- defN 24-Mar-26 18:08 deephaven_core-0.33.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-26 18:08 deephaven_core-0.33.3.dist-info/WHEEL
--rw-r--r--  2.0 unx       86 b- defN 24-Mar-26 18:08 deephaven_core-0.33.3.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       29 b- defN 24-Mar-26 18:08 deephaven_core-0.33.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6681 b- defN 24-Mar-26 18:08 deephaven_core-0.33.3.dist-info/RECORD
-80 files, 757473 bytes uncompressed, 151043 bytes compressed:  80.1%
+Zip file size: 169438 bytes, number of entries: 82
+-rw-r--r--  2.0 unx     1475 b- defN 24-Apr-26 21:40 deephaven/__init__.py
+-rw-r--r--  2.0 unx      611 b- defN 24-Apr-26 21:40 deephaven/_dep.py
+-rw-r--r--  2.0 unx      966 b- defN 24-Apr-26 21:40 deephaven/_gc.py
+-rw-r--r--  2.0 unx      641 b- defN 24-Apr-26 21:40 deephaven/_jpy.py
+-rw-r--r--  2.0 unx    27944 b- defN 24-Apr-26 21:40 deephaven/_udf.py
+-rw-r--r--  2.0 unx     7985 b- defN 24-Apr-26 21:40 deephaven/_wrapper.py
+-rw-r--r--  2.0 unx    15143 b- defN 24-Apr-26 21:40 deephaven/agg.py
+-rw-r--r--  2.0 unx     1977 b- defN 24-Apr-26 21:40 deephaven/appmode.py
+-rw-r--r--  2.0 unx     5148 b- defN 24-Apr-26 21:40 deephaven/arrow.py
+-rw-r--r--  2.0 unx    10290 b- defN 24-Apr-26 21:40 deephaven/barrage.py
+-rw-r--r--  2.0 unx     3635 b- defN 24-Apr-26 21:40 deephaven/calendar.py
+-rw-r--r--  2.0 unx     7580 b- defN 24-Apr-26 21:40 deephaven/column.py
+-rw-r--r--  2.0 unx     3123 b- defN 24-Apr-26 21:40 deephaven/constants.py
+-rw-r--r--  2.0 unx     4946 b- defN 24-Apr-26 21:40 deephaven/csv.py
+-rw-r--r--  2.0 unx     2890 b- defN 24-Apr-26 21:40 deephaven/dherror.py
+-rw-r--r--  2.0 unx    14798 b- defN 24-Apr-26 21:40 deephaven/dtypes.py
+-rw-r--r--  2.0 unx     3706 b- defN 24-Apr-26 21:40 deephaven/execution_context.py
+-rw-r--r--  2.0 unx     5153 b- defN 24-Apr-26 21:40 deephaven/filters.py
+-rw-r--r--  2.0 unx      650 b- defN 24-Apr-26 21:40 deephaven/html.py
+-rw-r--r--  2.0 unx    13296 b- defN 24-Apr-26 21:40 deephaven/jcompat.py
+-rw-r--r--  2.0 unx     8963 b- defN 24-Apr-26 21:40 deephaven/liveness_scope.py
+-rw-r--r--  2.0 unx     7443 b- defN 24-Apr-26 21:40 deephaven/numpy.py
+-rw-r--r--  2.0 unx    10292 b- defN 24-Apr-26 21:40 deephaven/pandas.py
+-rw-r--r--  2.0 unx    26505 b- defN 24-Apr-26 21:40 deephaven/parquet.py
+-rw-r--r--  2.0 unx     9429 b- defN 24-Apr-26 21:40 deephaven/perfmon.py
+-rw-r--r--  2.0 unx        1 b- defN 24-Apr-26 21:40 deephaven/py.typed
+-rw-r--r--  2.0 unx     2690 b- defN 24-Apr-26 21:40 deephaven/query_library.py
+-rw-r--r--  2.0 unx     3075 b- defN 24-Apr-26 21:40 deephaven/replay.py
+-rw-r--r--  2.0 unx   163251 b- defN 24-Apr-26 21:40 deephaven/table.py
+-rw-r--r--  2.0 unx    17849 b- defN 24-Apr-26 21:40 deephaven/table_factory.py
+-rw-r--r--  2.0 unx    16348 b- defN 24-Apr-26 21:40 deephaven/table_listener.py
+-rw-r--r--  2.0 unx    31656 b- defN 24-Apr-26 21:40 deephaven/time.py
+-rw-r--r--  2.0 unx     9071 b- defN 24-Apr-26 21:40 deephaven/update_graph.py
+-rw-r--r--  2.0 unx    79443 b- defN 24-Apr-26 21:40 deephaven/updateby.py
+-rw-r--r--  2.0 unx     1155 b- defN 24-Apr-26 21:40 deephaven/uri.py
+-rw-r--r--  2.0 unx     3239 b- defN 24-Apr-26 21:40 deephaven/dbc/__init__.py
+-rw-r--r--  2.0 unx     1790 b- defN 24-Apr-26 21:40 deephaven/dbc/adbc.py
+-rw-r--r--  2.0 unx     1664 b- defN 24-Apr-26 21:40 deephaven/dbc/odbc.py
+-rw-r--r--  2.0 unx     1316 b- defN 24-Apr-26 21:40 deephaven/experimental/__init__.py
+-rw-r--r--  2.0 unx     3195 b- defN 24-Apr-26 21:40 deephaven/experimental/data_index.py
+-rw-r--r--  2.0 unx     4815 b- defN 24-Apr-26 21:40 deephaven/experimental/ema.py
+-rw-r--r--  2.0 unx     4102 b- defN 24-Apr-26 21:40 deephaven/experimental/outer_joins.py
+-rw-r--r--  2.0 unx     6654 b- defN 24-Apr-26 21:40 deephaven/experimental/s3.py
+-rw-r--r--  2.0 unx     1570 b- defN 24-Apr-26 21:40 deephaven/experimental/sql.py
+-rw-r--r--  2.0 unx     7534 b- defN 24-Apr-26 21:40 deephaven/learn/__init__.py
+-rw-r--r--  2.0 unx     3120 b- defN 24-Apr-26 21:40 deephaven/learn/gather.py
+-rw-r--r--  2.0 unx      345 b- defN 24-Apr-26 21:40 deephaven/pandasplugin/__init__.py
+-rw-r--r--  2.0 unx      573 b- defN 24-Apr-26 21:40 deephaven/pandasplugin/pandas_as_table.py
+-rw-r--r--  2.0 unx      574 b- defN 24-Apr-26 21:40 deephaven/plot/__init__.py
+-rw-r--r--  2.0 unx     2289 b- defN 24-Apr-26 21:40 deephaven/plot/axisformat.py
+-rw-r--r--  2.0 unx     1479 b- defN 24-Apr-26 21:40 deephaven/plot/axistransform.py
+-rw-r--r--  2.0 unx    16540 b- defN 24-Apr-26 21:40 deephaven/plot/color.py
+-rw-r--r--  2.0 unx   110210 b- defN 24-Apr-26 21:40 deephaven/plot/figure.py
+-rw-r--r--  2.0 unx     1659 b- defN 24-Apr-26 21:40 deephaven/plot/font.py
+-rw-r--r--  2.0 unx     3515 b- defN 24-Apr-26 21:40 deephaven/plot/linestyle.py
+-rw-r--r--  2.0 unx     1285 b- defN 24-Apr-26 21:40 deephaven/plot/plotstyle.py
+-rw-r--r--  2.0 unx     2493 b- defN 24-Apr-26 21:40 deephaven/plot/selectable_dataset.py
+-rw-r--r--  2.0 unx     1213 b- defN 24-Apr-26 21:40 deephaven/plot/shape.py
+-rw-r--r--  2.0 unx       71 b- defN 24-Apr-26 21:40 deephaven/server/__init__.py
+-rw-r--r--  2.0 unx     2233 b- defN 24-Apr-26 21:40 deephaven/server/executors.py
+-rw-r--r--  2.0 unx     1069 b- defN 24-Apr-26 21:40 deephaven/stream/__init__.py
+-rw-r--r--  2.0 unx     5640 b- defN 24-Apr-26 21:40 deephaven/stream/table_publisher.py
+-rw-r--r--  2.0 unx     1019 b- defN 24-Apr-26 21:40 deephaven/stream/kafka/__init__.py
+-rw-r--r--  2.0 unx     7084 b- defN 24-Apr-26 21:40 deephaven/stream/kafka/cdc.py
+-rw-r--r--  2.0 unx    23648 b- defN 24-Apr-26 21:40 deephaven/stream/kafka/consumer.py
+-rw-r--r--  2.0 unx    13972 b- defN 24-Apr-26 21:40 deephaven/stream/kafka/producer.py
+-rw-r--r--  2.0 unx       71 b- defN 24-Apr-26 21:40 deephaven_internal/__init__.py
+-rw-r--r--  2.0 unx     1645 b- defN 24-Apr-26 21:40 deephaven_internal/java_threads.py
+-rw-r--r--  2.0 unx     3662 b- defN 24-Apr-26 21:40 deephaven_internal/stream.py
+-rw-r--r--  2.0 unx      952 b- defN 24-Apr-26 21:40 deephaven_internal/auto_completer/__init__.py
+-rw-r--r--  2.0 unx     8441 b- defN 24-Apr-26 21:40 deephaven_internal/auto_completer/_completer.py
+-rw-r--r--  2.0 unx     3491 b- defN 24-Apr-26 21:40 deephaven_internal/jvm/__init__.py
+-rw-r--r--  2.0 unx      582 b- defN 24-Apr-26 21:40 deephaven_internal/plugin/__init__.py
+-rw-r--r--  2.0 unx     1486 b- defN 24-Apr-26 21:40 deephaven_internal/plugin/register.py
+-rw-r--r--  2.0 unx      961 b- defN 24-Apr-26 21:40 deephaven_internal/plugin/js/__init__.py
+-rw-r--r--  2.0 unx     3470 b- defN 24-Apr-26 21:40 deephaven_internal/plugin/object/__init__.py
+-rw-r--r--  2.0 unx     1736 b- defN 24-Apr-26 21:40 deephaven_internal/script_session/__init__.py
+-rw-r--r--  2.0 unx     2773 b- defN 24-Apr-26 21:40 deephaven_core-0.34.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-26 21:40 deephaven_core-0.34.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       86 b- defN 24-Apr-26 21:40 deephaven_core-0.34.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       29 b- defN 24-Apr-26 21:40 deephaven_core-0.34.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6853 b- defN 24-Apr-26 21:40 deephaven_core-0.34.0.dist-info/RECORD
+82 files, 791368 bytes uncompressed, 158708 bytes compressed:  79.9%
```

## zipnote {}

```diff
@@ -21,14 +21,17 @@
 
 Filename: deephaven/appmode.py
 Comment: 
 
 Filename: deephaven/arrow.py
 Comment: 
 
+Filename: deephaven/barrage.py
+Comment: 
+
 Filename: deephaven/calendar.py
 Comment: 
 
 Filename: deephaven/column.py
 Comment: 
 
 Filename: deephaven/constants.py
@@ -108,14 +111,17 @@
 
 Filename: deephaven/dbc/odbc.py
 Comment: 
 
 Filename: deephaven/experimental/__init__.py
 Comment: 
 
+Filename: deephaven/experimental/data_index.py
+Comment: 
+
 Filename: deephaven/experimental/ema.py
 Comment: 
 
 Filename: deephaven/experimental/outer_joins.py
 Comment: 
 
 Filename: deephaven/experimental/s3.py
@@ -219,23 +225,23 @@
 
 Filename: deephaven_internal/plugin/object/__init__.py
 Comment: 
 
 Filename: deephaven_internal/script_session/__init__.py
 Comment: 
 
-Filename: deephaven_core-0.33.3.dist-info/METADATA
+Filename: deephaven_core-0.34.0.dist-info/METADATA
 Comment: 
 
-Filename: deephaven_core-0.33.3.dist-info/WHEEL
+Filename: deephaven_core-0.34.0.dist-info/WHEEL
 Comment: 
 
-Filename: deephaven_core-0.33.3.dist-info/entry_points.txt
+Filename: deephaven_core-0.34.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: deephaven_core-0.33.3.dist-info/top_level.txt
+Filename: deephaven_core-0.34.0.dist-info/top_level.txt
 Comment: 
 
-Filename: deephaven_core-0.33.3.dist-info/RECORD
+Filename: deephaven_core-0.34.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deephaven/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """Deephaven Python Integration Package provides the ability to access the Deephaven's query engine natively and thus
 unlocks the unique power of Deephaven to the Python community.
 
 """
```

## deephaven/_dep.py

```diff
@@ -1,9 +1,9 @@
 #
-#     Copyright (c) 2016-2023 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """This module is an internal module to support dependency management"""
 
 import importlib
 from types import ModuleType
 from typing import Optional
```

## deephaven/_gc.py

```diff
@@ -1,9 +1,9 @@
 #
-#     Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """This module defines a convenience function for running both Python and Java garbage collection utilities."""
 
 import gc
 
 import jpy
```

## deephaven/_jpy.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module is an internal module to simplify usage patterns around jpy.
 """
 
 import jpy
```

## deephaven/_udf.py

```diff
@@ -1,57 +1,183 @@
 #
-#     Copyright (c) 2016-2023 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 import inspect
 import re
 import sys
+import typing
+import warnings
 from dataclasses import dataclass, field
-from functools import wraps
-from typing import Callable, List, Any, Union, Tuple, _GenericAlias, Set
+from datetime import datetime
+from functools import wraps, partial
+from typing import Callable, List, Any, Union, _GenericAlias, Optional, Sequence
+
+import pandas as pd
 
 from deephaven._dep import soft_dependency
 
 numba = soft_dependency("numba")
 
 import numpy
 import numpy as np
+import jpy
 
 from deephaven import DHError, dtypes
-from deephaven.dtypes import _np_ndarray_component_type, _np_dtype_char, _NUMPY_INT_TYPE_CODES, \
-    _NUMPY_FLOATING_TYPE_CODES, _component_np_dtype_char, _J_ARRAY_NP_TYPE_MAP, _PRIMITIVE_DTYPE_NULL_MAP, _scalar, \
-    _BUILDABLE_ARRAY_DTYPE_MAP
+from deephaven.dtypes import _NUMPY_INT_TYPE_CODES, _NUMPY_FLOATING_TYPE_CODES, _PRIMITIVE_DTYPE_NULL_MAP, \
+    _BUILDABLE_ARRAY_DTYPE_MAP, DType
 from deephaven.jcompat import _j_array_to_numpy_array
-from deephaven.time import to_np_datetime64
+from deephaven.time import to_np_datetime64, to_datetime, to_pd_timestamp
 
 # For unittest vectorization
 test_vectorization = False
 vectorized_count = 0
 
-
 _SUPPORTED_NP_TYPE_CODES = {"b", "h", "H", "i", "l", "f", "d", "?", "U", "M", "O"}
 
 
+def _is_lossless_convertible(from_type: str, to_type: str) -> bool:
+    """ Check if the conversion from one type to another is lossless. """
+    if from_type == to_type:
+        return True
+
+    if from_type == 'b':
+        return to_type in {'h', 'H', 'i', 'l'}
+    elif from_type == 'h' or from_type == 'H':
+        return to_type in {'i', 'l'}
+    elif from_type == 'i':
+        return to_type == 'l'
+    elif from_type == 'f':
+        return to_type == 'd'
+
+    return False
+
+
 @dataclass
 class _ParsedParam:
     name: Union[str, int] = field(init=True)
-    orig_types: Set[type] = field(default_factory=set)
-    encoded_types: Set[str] = field(default_factory=set)
+    orig_types: List[type] = field(default_factory=list)
+    effective_types: List[type] = field(default_factory=list)
+    encoded_types: List[str] = field(default_factory=list)
     none_allowed: bool = False
     has_array: bool = False
-    int_char: str = None
-    floating_char: str = None
+    arg_converter: Optional[Callable] = None
+
+    def setup_arg_converter(self, arg_type_str: str) -> None:
+        """ Set up the converter function for the parameter based on the encoded argument type string. """
+        for param_type_str, effective_type in zip(self.encoded_types, self.effective_types):
+            # unsupported type is treated as object type, no conversion. We'll let the runtime handle them
+            # since we want to trust that the user knows what they are doing.
+            if arg_type_str in {"X", "O"}:
+                self.arg_converter = None
+                break
+
+            if _is_lossless_convertible(arg_type_str, param_type_str):
+                if arg_type_str.startswith("["):  # array type (corresponding to numpy ndarray, Sequence, etc.)
+                    dtype = dtypes.from_np_dtype(np.dtype(arg_type_str[1]))
+                    self.arg_converter = partial(_j_array_to_numpy_array, dtype, conv_null=False,
+                                                 type_promotion=False)
+                elif arg_type_str == 'N':
+                    self.arg_converter = None
+                else:
+                    if effective_type in {object, str}:
+                        self.arg_converter = None
+                    elif arg_type_str == 'M':  # datetime types (datetime, pd.Timestamp, np.datetime64)
+                        self._setup_datetime_arg_converter(effective_type)
+                    else:
+                        self.arg_converter = effective_type
+
+                        # Optional typehint on primitive types requires checking for DH nulls
+                        if "N" in self.encoded_types:
+                            if null_value := _PRIMITIVE_DTYPE_NULL_MAP.get(
+                                    dtypes.from_np_dtype(np.dtype(arg_type_str))):
+                                if effective_type in {int, float, bool}:
+                                    self.arg_converter = partial(lambda nv, x: None if x == nv else x, null_value)
+                                else:
+                                    self.arg_converter = partial(lambda nv, x: None if x == nv else effective_type(x),
+                                                                 null_value)
+
+                        # JPY does the conversion for these types
+                        if self.arg_converter in {int, float, bool}:
+                            self.arg_converter = None
+
+                        if self.arg_converter and isinstance(self.arg_converter, type) and issubclass(
+                                self.arg_converter, np.generic):
+                            # because numpy scalar types are significantly slower than Python built-in scalar types,
+                            # we'll continue to find a Python scalar type that can be used to convert the argument
+                            continue
+                        else:
+                            break
+
+        # we have found the most suitable converter function, but if it is a numpy scalar type, we'll warn the user
+        if self.arg_converter and isinstance(self.arg_converter, type) and issubclass(self.arg_converter, np.generic):
+            warnings.warn(
+                f"numpy scalar type {self.arg_converter} is used to annotate parameter '{self.name}'. Note that "
+                f"conversion of "
+                f"arguments to numpy scalar types is significantly slower than to Python built-in scalar "
+                f"types such as int, float, bool, etc. If possible, consider using Python built-in scalar types "
+                f"instead.")
+
+    def _setup_datetime_arg_converter(self, effective_type):
+        if effective_type is datetime:
+            self.arg_converter = to_datetime
+        elif effective_type is pd.Timestamp:
+            if "N" in self.encoded_types:
+                self.arg_converter = to_pd_timestamp
+            else:
+                self.arg_converter = lambda x: to_np_datetime64(x) if x is not None else pd.Timestamp('NaT')
+        else:
+            if "N" in self.encoded_types:
+                self.arg_converter = to_np_datetime64
+            else:
+                self.arg_converter = lambda x: to_np_datetime64(x) if x is not None else np.datetime64('NaT')
 
 
 @dataclass
 class _ParsedReturnAnnotation:
     orig_type: type = None
     encoded_type: str = None
     none_allowed: bool = False
     has_array: bool = False
+    ret_converter: Optional[Callable] = None
+
+    def setup_return_converter(self) -> None:
+        """ Set the converter function for the return value of UDF based on the return type annotation."""
+        t = self.encoded_type
+        if t == 'H':
+            if self.none_allowed:
+                self.ret_converter = lambda x: dtypes.Character(int(x)) if x is not None else None
+            else:
+                self.ret_converter = lambda x: dtypes.Character(int(x))
+        elif t in _NUMPY_INT_TYPE_CODES:
+            if self.none_allowed:
+                null_value = _PRIMITIVE_DTYPE_NULL_MAP.get(dtypes.from_np_dtype(np.dtype(t)))
+                self.ret_converter = partial(lambda nv, x: nv if x is None else int(x), null_value)
+            else:
+                self.ret_converter = int if self.orig_type is not int else None
+        elif t in _NUMPY_FLOATING_TYPE_CODES:
+            if self.none_allowed:
+                null_value = _PRIMITIVE_DTYPE_NULL_MAP.get(dtypes.from_np_dtype(np.dtype(t)))
+                self.ret_converter = partial(lambda nv, x: nv if x is None else float(x), null_value)
+            else:
+                self.ret_converter = float if self.orig_type is not float else None
+        elif t == '?':
+            if self.none_allowed:
+                self.ret_converter = lambda x: bool(x) if x is not None else None
+            else:
+                # Note this is not ideal because in the case where actual return is np.bool_ but type hint is bool,
+                # we raise a mystic error (PyObject can't be cast to Boolean). We need to revisit this when
+                # https://github.com/deephaven/deephaven-core/issues/5397 and/or
+                # https://github.com/deephaven/deephaven-core/issues/4068 is resolved.
+                self.ret_converter = None if self.orig_type is bool else bool
+        elif t == 'M':
+            from deephaven.time import to_j_instant
+            self.ret_converter = to_j_instant
+        else:
+            self.ret_converter = None
 
 
 @dataclass
 class _ParsedSignature:
     fn: Callable = None
     params: List[_ParsedParam] = field(default_factory=list)
     ret_annotation: _ParsedReturnAnnotation = None
@@ -60,163 +186,272 @@
     def encoded(self) -> str:
         """Encode the signature of a Python function by mapping the annotations of the parameter types and the return
         type to numpy dtype chars (i,l,h,f,d,b,?,U,M,O) and '[' for array, 'N' for NoneType. and pack them into a
         string with parameter type chars first, in their original order, followed by the delimiter string '->',
         then the return type char. If a parameter or the return of the function is not annotated,
         the default 'O' - object type, will be used.
         """
-        param_str = ",".join(["".join(p.encoded_types) for p in self.params])
+        param_str = ",".join([str(p.name) + ":" + "".join(p.encoded_types) for p in self.params])
         # ret_annotation has only one parsed annotation, and it might be Optional which means it contains 'N' in the
         # encoded type. We need to remove it.
         return_type_code = re.sub(r"[N]", "", self.ret_annotation.encoded_type)
         return param_str + "->" + return_type_code
 
+    def prepare_auto_arg_conv(self, encoded_arg_types: str) -> bool:
+        """ Determine whether the auto argument conversion should be used and set the converter functions for the
+        parameters."""
+        if not self.params or not encoded_arg_types:
+            return False
+
+        arg_conv_needed = True
+        arg_type_strs = encoded_arg_types.split(",")
+        if all([t == "O" for t in arg_type_strs]):
+            arg_conv_needed = False
+
+        for arg_type_str, param in zip(arg_type_strs, self.params):
+            param.setup_arg_converter(arg_type_str)
+
+        if all([param.arg_converter is None for param in self.params]):
+            arg_conv_needed = False
+
+        return arg_conv_needed
+
 
 def _encode_param_type(t: type) -> str:
     """Returns the numpy based char codes for the given type.
     If the type is a numpy ndarray, prefix the numpy dtype char with '[' using Java convention
     If the type is a NoneType (as in Optional or as None in Union), return 'N'
+    If the type is not a supported numpy type, return 'X' (stands for unsupported)
     """
     if t is type(None):
         return "N"
 
+    if t is typing.Any or t is object or t is jpy.JType:
+        return "O"
+
     # find the component type if it is numpy ndarray
     component_type = _np_ndarray_component_type(t)
     if component_type:
         t = component_type
 
     tc = _np_dtype_char(t)
-    tc = tc if tc in _SUPPORTED_NP_TYPE_CODES else "O"
+    tc = tc if tc in _SUPPORTED_NP_TYPE_CODES else "X"
 
-    if component_type:
+    if component_type and tc != "X":
         tc = "[" + tc
     return tc
 
 
-def _parse_param(name: str, annotation: Any) -> _ParsedParam:
+def _np_dtype_char(t: Union[type, str]) -> str:
+    """Returns the numpy dtype character code for the given type."""
+    if t is None:
+        return "O"
+
+    if t in (datetime, pd.Timestamp):
+        return "M"
+
+    try:
+        np_dtype = np.dtype(t)
+        if np_dtype.char == "O" and t is not object: # np.dtype() returns np.dtype('O') for unrecognized types
+            return "X"
+        return np_dtype.char
+    except TypeError:
+        return 'X'
+
+
+def _component_np_dtype_char(t: type) -> Optional[str]:
+    """Returns the numpy dtype character code for the given type's component type if the type is a Sequence type or
+    numpy ndarray, otherwise return None. """
+    component_type = _py_sequence_component_type(t)
+
+    if not component_type:
+        if t is bytes or t is bytearray:
+            return "b"
+
+    if not component_type:
+        component_type = _np_ndarray_component_type(t)
+
+    if component_type:
+        return _np_dtype_char(component_type)
+    else:
+        return None
+
+
+def _py_sequence_component_type(t: type) -> Optional[type]:
+    """Returns the component type of Python subscribed sequence type, otherwise return None."""
+    component_type = None
+    if sys.version_info > (3, 8):
+        import types
+        if isinstance(t, types.GenericAlias) and issubclass(t.__origin__, Sequence):  # novermin
+            component_type = t.__args__[0]
+
+    if not component_type:
+        if isinstance(t, _GenericAlias) and issubclass(t.__origin__, Sequence):
+            component_type = t.__args__[0]
+
+    # if the component type is a DType, get its numpy type
+    if isinstance(component_type, DType):
+        component_type = component_type.np_type
+
+    return component_type
+
+
+def _np_ndarray_component_type(t: type) -> Optional[type]:
+    """Returns the numpy ndarray component type if the type is a numpy ndarray, otherwise return None."""
+
+    # Py3.8: npt.NDArray can be used in Py 3.8 as a generic alias, but a specific alias (e.g. npt.NDArray[np.int64])
+    # is an instance of a private class of np, yet we don't have a choice but to use it. And when npt.NDArray is used,
+    # the 1st argument is typing.Any, the 2nd argument is another generic alias of which the 1st argument is the
+    # component type
+    component_type = None
+    if (3, 9) > sys.version_info >= (3, 8):
+        if isinstance(t, np._typing._generic_alias._GenericAlias) and t.__origin__ is np.ndarray:
+            component_type = t.__args__[1].__args__[0]
+    # Py3.9+, np.ndarray as a generic alias is only supported in Python 3.9+, also npt.NDArray is still available but a
+    # specific alias (e.g. npt.NDArray[np.int64]) now is an instance of typing.GenericAlias.
+    # when npt.NDArray is used, the 1st argument is typing.Any, the 2nd argument is another generic alias of which
+    # the 1st argument is the component type
+    # when np.ndarray is used, the 1st argument is the component type
+    if not component_type and sys.version_info >= (3, 9):
+        import types
+        if isinstance(t, types.GenericAlias) and t.__origin__ is np.ndarray:  # novermin
+            nargs = len(t.__args__)
+            if nargs == 1:
+                component_type = t.__args__[0]
+            elif nargs == 2:  # for npt.NDArray[np.int64], etc.
+                a0 = t.__args__[0]
+                a1 = t.__args__[1]
+                if a0 == typing.Any and isinstance(a1, types.GenericAlias):  # novermin
+                    component_type = a1.__args__[0]
+    return component_type
+
+
+def _is_union_type(t: type) -> bool:
+    """Return True if the type is a Union type"""
+    if sys.version_info >= (3, 10):
+        import types
+        if isinstance(t, types.UnionType):  # novermin
+            return True
+
+    return isinstance(t, _GenericAlias) and t.__origin__ is Union
+
+
+def _parse_param(name: str, annotation: Union[type, dtypes.DType]) -> _ParsedParam:
     """ Parse a parameter annotation in a function's signature """
     p_param = _ParsedParam(name)
 
     if annotation is inspect._empty:
-        p_param.encoded_types.add("O")
+        p_param.effective_types.append(object)
+        p_param.encoded_types.append("O")
         p_param.none_allowed = True
-    elif isinstance(annotation, _GenericAlias) and annotation.__origin__ == Union:
+    elif _is_union_type(annotation):
         for t in annotation.__args__:
             _parse_type_no_nested(annotation, p_param, t)
     else:
         _parse_type_no_nested(annotation, p_param, annotation)
     return p_param
 
 
-def _parse_type_no_nested(annotation: Any, p_param: _ParsedParam, t: Union[type, str]) -> None:
+def _parse_type_no_nested(annotation: Any, p_param: _ParsedParam, t: Union[type, dtypes.DType]) -> None:
     """ Parse a specific type (top level or nested in a top-level Union annotation) without handling nested types
     (e.g. a nested Union). The result is stored in the given _ParsedAnnotation object.
     """
-    p_param.orig_types.add(t)
+    p_param.orig_types.append(t)
 
     # if the annotation is a DH DType instance, we'll use its numpy type
     if isinstance(t, dtypes.DType):
         t = t.np_type
+        p_param.effective_types.append(np.dtype(t).type)
+    else:
+        p_param.effective_types.append(t)
 
     tc = _encode_param_type(t)
     if "[" in tc:
         p_param.has_array = True
     if tc in {"N", "O"}:
         p_param.none_allowed = True
-    if tc in _NUMPY_INT_TYPE_CODES:
-        if p_param.int_char and p_param.int_char != tc:
-            raise DHError(message=f"multiple integer types in annotation: {annotation}, "
-                                  f"types: {p_param.int_char}, {tc}. this is not supported because it is not "
-                                  f"clear which Deephaven null value to use when checking for nulls in the argument")
-        p_param.int_char = tc
-    if tc in _NUMPY_FLOATING_TYPE_CODES:
-        if p_param.floating_char and p_param.floating_char != tc:
-            raise DHError(message=f"multiple floating types in annotation: {annotation}, "
-                                  f"types: {p_param.floating_char}, {tc}. this is not supported because it is not "
-                                  f"clear which Deephaven null value to use when checking for nulls in the argument")
-        p_param.floating_char = tc
-    p_param.encoded_types.add(tc)
+    p_param.encoded_types.append(tc)
 
 
 def _parse_return_annotation(annotation: Any) -> _ParsedReturnAnnotation:
     """ Parse a function's return annotation
 
     The return annotation is treated differently from the parameter annotations. We don't apply the same check and are
     only interested in getting the array-like type right. Any nonsensical annotation will be treated as object type.
     This definitely can be improved in the future.
     """
 
     pra = _ParsedReturnAnnotation()
 
     t = annotation
     pra.orig_type = t
-    if isinstance(annotation, _GenericAlias) and annotation.__origin__ == Union and len(annotation.__args__) == 2:
+    if _is_union_type(annotation) and len(annotation.__args__) == 2:
         # if the annotation is a Union of two types, we'll use the non-None type
-        if annotation.__args__[1] == type(None):  # noqa: E721
+        if annotation.__args__[1] is type(None):  # noqa: E721
+            pra.none_allowed = True
             t = annotation.__args__[0]
-        elif annotation.__args__[0] == type(None):  # noqa: E721
+        elif annotation.__args__[0] is type(None):  # noqa: E721
+            pra.none_allowed = True
             t = annotation.__args__[1]
 
     # if the annotation is a DH DType instance, we'll use its numpy type
     if isinstance(t, dtypes.DType):
         t = t.np_type
 
     component_char = _component_np_dtype_char(t)
     if component_char:
         pra.encoded_type = "[" + component_char
         pra.has_array = True
     else:
         pra.encoded_type = _np_dtype_char(t)
+
     return pra
 
 
 if numba:
-    def _parse_numba_signature(fn: Union[numba.np.ufunc.gufunc.GUFunc, numba.np.ufunc.dufunc.DUFunc]) -> _ParsedSignature:
+    def _parse_numba_signature(
+            fn: Union[numba.np.ufunc.gufunc.GUFunc, numba.np.ufunc.dufunc.DUFunc]) -> _ParsedSignature:
         """ Parse a numba function's signature"""
         sigs = fn.types  # in the format of ll->l, ff->f,dd->d,OO->O, etc.
         if sigs:
             p_sig = _ParsedSignature(fn)
 
-            # for now, we only support one signature for a numba function because the query engine is not ready to handle
+            # for now, we only support one signature for a numba function because the query engine is not ready to
+            # handle
             # multiple signatures for vectorization https://github.com/deephaven/deephaven-core/issues/4762
             sig = sigs[0]
             params, rt_char = sig.split("->")
 
             p_sig.params = []
             p_sig.ret_annotation = _ParsedReturnAnnotation()
             p_sig.ret_annotation.encoded_type = rt_char
 
             if isinstance(fn, numba.np.ufunc.dufunc.DUFunc):
                 for i, p in enumerate(params):
                     pa = _ParsedParam(i + 1)
-                    pa.encoded_types.add(p)
-                    if p in _NUMPY_INT_TYPE_CODES:
-                        pa.int_char = p
-                    if p in _NUMPY_FLOATING_TYPE_CODES:
-                        pa.floating_char = p
+                    pa.encoded_types.append(p)
+                    pa.effective_types.append(np.dtype(p).type)
                     p_sig.params.append(pa)
             else:  # GUFunc
                 # An example: @guvectorize([(int64[:], int64[:], int64[:])], "(m),(n)->(n)"
                 input_output_decl = fn.signature  # "(m),(n)->(n)" in the above example
                 input_decl, output_decl = input_output_decl.split("->")
                 # remove the parentheses so that empty string indicates no array, non-empty string indicates array
                 input_decl = re.sub("[()]", "", input_decl).split(",")
                 output_decl = re.sub("[()]", "", output_decl)
 
                 for i, (p, d) in enumerate(zip(params, input_decl)):
                     pa = _ParsedParam(i + 1)
                     if d:
-                        pa.encoded_types.add("[" + p)
+                        pa.encoded_types.append("[" + p)
+                        pa.effective_types.append(np.dtype(p).type)
                         pa.has_array = True
                     else:
-                        pa.encoded_types.add(p)
-                        if p in _NUMPY_INT_TYPE_CODES:
-                            pa.int_char = p
-                        if p in _NUMPY_FLOATING_TYPE_CODES:
-                            pa.floating_char = p
+                        pa.encoded_types.append(p)
+                        pa.effective_types.append(np.dtype(p).type)
                     p_sig.params.append(pa)
 
                 if output_decl:
                     p_sig.ret_annotation.has_array = True
             return p_sig
         else:
             raise DHError(message=f"numba decorated functions must have an explicitly defined signature: {fn}")
@@ -227,252 +462,165 @@
 
     # numpy ufuncs actually have signature encoded in their 'types' attribute, we want to better support
     # them in the future (https://github.com/deephaven/deephaven-core/issues/4762)
     p_sig = _ParsedSignature(fn)
     if fn.nin > 0:
         for i in range(fn.nin):
             pa = _ParsedParam(i + 1)
-            pa.encoded_types.add("O")
+            pa.encoded_types.append("O")
+            pa.effective_types.append(object)
             p_sig.params.append(pa)
     p_sig.ret_annotation = _ParsedReturnAnnotation()
     p_sig.ret_annotation.encoded_type = "O"
+
     return p_sig
 
 
 def _parse_signature(fn: Callable) -> _ParsedSignature:
     """ Parse the signature of a function """
 
     if numba:
         if isinstance(fn, (numba.np.ufunc.gufunc.GUFunc, numba.np.ufunc.dufunc.DUFunc)):
             return _parse_numba_signature(fn)
 
     if isinstance(fn, numpy.ufunc):
         return _parse_np_ufunc_signature(fn)
     else:
         p_sig = _ParsedSignature(fn=fn)
-        if sys.version_info.major == 3 and sys.version_info.minor >= 10:
-            sig = inspect.signature(fn, eval_str=True)
+        if sys.version_info >= (3, 10):
+            sig = inspect.signature(fn, eval_str=True)  # novermin
         else:
             sig = inspect.signature(fn)
 
         for n, p in sig.parameters.items():
-            # when from __future__ import annotations is used, the annotation is a string, we need to eval it to get the type
-            # when the minimum Python version is bumped to 3.10, we'll always use eval_str in _parse_signature, so that
-            # annotation is already a type, and we can skip this step.
+            # when from __future__ import annotations is used, the annotation is a string, we need to eval it to get
+            # the type when the minimum Python version is bumped to 3.10, we'll always use eval_str in _parse_signature,
+            # so that annotation is already a type, and we can skip this step.
             t = eval(p.annotation, fn.__globals__) if isinstance(p.annotation, str) else p.annotation
             p_sig.params.append(_parse_param(n, t))
 
-        t = eval(sig.return_annotation, fn.__globals__) if isinstance(sig.return_annotation, str) else sig.return_annotation
+        t = eval(sig.return_annotation, fn.__globals__) if isinstance(sig.return_annotation,
+                                                                      str) else sig.return_annotation
         p_sig.ret_annotation = _parse_return_annotation(t)
         return p_sig
 
 
-def _is_from_np_type(param_types: Set[type], np_type_char: str) -> bool:
-    """ Determine if the given numpy type char comes for a numpy type in the given set of parameter type annotations"""
-    for t in param_types:
-        if issubclass(t, np.generic) and np.dtype(t).char == np_type_char:
-            return True
-    return False
-
-
-def _convert_arg(param: _ParsedParam, arg: Any) -> Any:
-    """ Convert a single argument to the type specified by the annotation """
-    if arg is None:
-        if not param.none_allowed:
-            raise TypeError(f"Argument {param.name!r}: {arg} is not compatible with annotation {param.orig_types}")
-        else:
-            return None
-
-    # if the arg is a Java array
-    if np_dtype := _J_ARRAY_NP_TYPE_MAP.get(type(arg)):
-        encoded_type = "[" + np_dtype.char
-        # if it matches one of the encoded types, convert it
-        if encoded_type in param.encoded_types:
-            dtype = dtypes.from_np_dtype(np_dtype)
-            try:
-                return _j_array_to_numpy_array(dtype, arg, conv_null=True, type_promotion=False)
-            except Exception as e:
-                raise TypeError(f"Argument {param.name!r}: {arg} is not compatible with annotation"
-                                f" {param.encoded_types}"
-                                f"\n{str(e)}") from e
-        # if the annotation is missing, or it is a generic object type, return the arg
-        elif "O" in param.encoded_types:
-            return arg
-        else:
-            raise TypeError(f"Argument {param.name!r}: {arg} is not compatible with annotation {param.encoded_types}")
-    else:  # if the arg is not a Java array
-        specific_types = param.encoded_types - {"N", "O"}  # remove NoneType and object type
-        if specific_types:
-            for t in specific_types:
-                if t.startswith("["):
-                    if isinstance(arg, np.ndarray) and arg.dtype.char == t[1]:
-                        return arg
-                    continue
-
-                dtype = dtypes.from_np_dtype(np.dtype(t))
-                dh_null = _PRIMITIVE_DTYPE_NULL_MAP.get(dtype)
-
-                if param.int_char and isinstance(arg, int):
-                    if arg == dh_null:
-                        if param.none_allowed:
-                            return None
-                        else:
-                            raise DHError(f"Argument {param.name!r}: {arg} is not compatible with annotation"
-                                          f" {param.orig_types}")
-                    else:
-                        # return a numpy integer instance only if the annotation is a numpy type
-                        if _is_from_np_type(param.orig_types, param.int_char):
-                            return np.dtype(param.int_char).type(arg)
-                        else:
-                            return arg
-                elif param.floating_char and isinstance(arg, float):
-                    if isinstance(arg, float):
-                        if arg == dh_null:
-                            return np.nan if "N" not in param.encoded_types else None
-                        else:
-                            # return a numpy floating instance only if the annotation is a numpy type
-                            if _is_from_np_type(param.orig_types, param.floating_char):
-                                return np.dtype(param.floating_char).type(arg)
-                            else:
-                                return arg
-                elif t == "?" and isinstance(arg, bool):
-                    return arg
-                elif t == "M":
-                    try:
-                        return to_np_datetime64(arg)
-                    except Exception as e:
-                        # don't raise an error, if this is the only annotation, the else block of the for loop will
-                        # catch it and raise a TypeError
-                        pass
-                elif t == "U" and isinstance(arg, str):
-                    return arg
-            else:  # didn't return from inside the for loop
-                if "O" in param.encoded_types:
-                    return arg
-                else:
-                    raise TypeError(f"Argument {param.name!r}: {arg} is not compatible with annotation"
-                                    f" {param.orig_types}")
-        else:  # if no annotation or generic object, return arg
-            return arg
-
-
-def _convert_args(p_sig: _ParsedSignature, args: Tuple[Any, ...]) -> List[Any]:
-    """ Convert all arguments to the types specified by the annotations.
-    Given that the number of arguments and the number of parameters may not match (in the presence of keyword,
-    var-positional, or var-keyword parameters), we have the following rules:
-     If the number of arguments is less than the number of parameters, the remaining parameters are left as is.
-     If the number of arguments is greater than the number of parameters, the extra arguments are left as is.
-
-    Python's function call mechanism will raise an exception if it can't resolve the parameters with the arguments.
-    """
-    converted_args = [_convert_arg(param, arg) for param, arg in zip(p_sig.params, args)]
-    converted_args.extend(args[len(converted_args):])
-    return converted_args
-
-
-def _py_udf(fn: Callable):
+def _udf_parser(fn: Callable):
     """A decorator that acts as a transparent translator for Python UDFs used in Deephaven query formulas between
-    Python and Java. This decorator is intended for use by the Deephaven query engine and should not be used by
+    Python and Java. This decorator is intended for internal use by the Deephaven query engine and should not be used by
     users.
 
     It carries out two conversions:
     1. convert Python function return values to Java values.
         For properly annotated functions, including numba vectorized and guvectorized ones, this decorator inspects the
         signature of the function and determines its return type, including supported primitive types and arrays of
         the supported primitive types. It then converts the return value of the function to the corresponding Java value
         of the same type. For unsupported types, the decorator returns the original Python value which appears as
         org.jpy.PyObject in Java.
     2. convert Java function arguments to Python values based on the signature of the function.
     """
     if hasattr(fn, "return_type"):
         return fn
+
     p_sig = _parse_signature(fn)
-    # build a signature string for vectorization by removing NoneType, array char '[', and comma from the encoded types
-    # since vectorization only supports UDFs with a single signature and enforces an exact match, any non-compliant
-    # signature (e.g. Union with more than 1 non-NoneType) will be rejected by the vectorizer.
-    sig_str_vectorization = re.sub(r"[\[N,]", "", p_sig.encoded)
     return_array = p_sig.ret_annotation.has_array
-    ret_dtype = dtypes.from_np_dtype(np.dtype(p_sig.ret_annotation.encoded_type[-1]))
+    ret_np_char = p_sig.ret_annotation.encoded_type[-1]
+    ret_dtype = dtypes.from_np_dtype(np.dtype(ret_np_char if ret_np_char != "X" else "O"))
 
     @wraps(fn)
-    def wrapper(*args, **kwargs):
-        converted_args = _convert_args(p_sig, args)
-        # kwargs are not converted because they are not used in the UDFs
-        ret = fn(*converted_args, **kwargs)
-        if return_array:
-            return dtypes.array(ret_dtype, ret)
-        elif ret_dtype == dtypes.PyObject:
-            return ret
-        else:
-            return _scalar(ret, ret_dtype)
+    def _udf_decorator(encoded_arg_types: str, for_vectorization: bool):
+        """The actual decorator that wraps the Python UDF and converts the arguments and return values.
+        It is called by the query engine with the runtime argument types to create a wrapper that can efficiently
+        convert the arguments and return values based on the provided argument types and the parsed parameters of the
+        UDF.
+        """
+        arg_conv_needed = p_sig.prepare_auto_arg_conv(encoded_arg_types)
+        p_sig.ret_annotation.setup_return_converter()
+
+        if not for_vectorization:
+            if not arg_conv_needed and p_sig.ret_annotation.encoded_type == "O":
+                return fn
+
+            def _wrapper(*args, **kwargs):
+                if arg_conv_needed:
+                    converted_args = [param.arg_converter(arg) if param.arg_converter else arg
+                                      for param, arg in zip(p_sig.params, args)]
+
+                    # if the number of arguments is more than the number of parameters, treat the last parameter as a
+                    # vararg and use its arg_converter to convert the rest of the arguments
+                    if len(args) > len(p_sig.params):
+                        arg_converter = p_sig.params[-1].arg_converter
+                        converted_args.extend([arg_converter(arg) if arg_converter else arg
+                                               for arg in args[len(converted_args):]])
+                else:
+                    converted_args = args
+                # kwargs are not converted because they are not used in the UDFs
+                ret = fn(*converted_args, **kwargs)
+                if return_array:
+                    return dtypes.array(ret_dtype, ret)
+                else:
+                    return p_sig.ret_annotation.ret_converter(ret) if p_sig.ret_annotation.ret_converter else ret
+
+            return _wrapper
+        else:  # for vectorization
+            def _vectorization_wrapper(*args):
+                if len(args) != len(p_sig.params) + 2:
+                    raise ValueError(
+                        f"The number of arguments doesn't match the function ({p_sig.fn.__name__}) signature. "
+                        f"{len(args) - 2}, {p_sig.encoded}")
+                if args[0] <= 0:
+                    raise ValueError(
+                        f"The chunk size argument must be a positive integer for vectorized function ("
+                        f"{p_sig.fn.__name__}). {args[0]}")
+
+                chunk_size = args[0]
+                chunk_result = args[1]
+                if args[2:]:
+                    vectorized_args = zip(*args[2:])
+                    for i in range(chunk_size):
+                        scalar_args = next(vectorized_args)
+                        if arg_conv_needed:
+                            converted_args = [param.arg_converter(arg) if param.arg_converter else arg
+                                              for param, arg in zip(p_sig.params, scalar_args)]
+
+                            # if the number of arguments is more than the number of parameters, treat the last parameter
+                            # as a vararg and use its arg_converter to convert the rest of the arguments
+                            if len(args) > len(p_sig.params):
+                                arg_converter = p_sig.params[-1].arg_converter
+                                converted_args.extend([arg_converter(arg) if arg_converter else arg
+                                                       for arg in scalar_args[len(converted_args):]])
+                        else:
+                            converted_args = scalar_args
+
+                        ret = fn(*converted_args)
+                        if return_array:
+                            chunk_result[i] = dtypes.array(ret_dtype, ret)
+                        else:
+                            chunk_result[i] = p_sig.ret_annotation.ret_converter(
+                                ret) if p_sig.ret_annotation.ret_converter else ret
+                else:
+                    for i in range(chunk_size):
+                        ret = fn()
+                        if return_array:
+                            chunk_result[i] = dtypes.array(ret_dtype, ret)
+                        else:
+                            chunk_result[i] = p_sig.ret_annotation.ret_converter(
+                                ret) if p_sig.ret_annotation.ret_converter else ret
+                return chunk_result
+
+            if test_vectorization:
+                global vectorized_count
+                vectorized_count += 1
+            return _vectorization_wrapper
 
-    wrapper.j_name = ret_dtype.j_name
+    _udf_decorator.j_name = ret_dtype.j_name
     real_ret_dtype = _BUILDABLE_ARRAY_DTYPE_MAP.get(ret_dtype, dtypes.PyObject) if return_array else ret_dtype
 
     if hasattr(ret_dtype.j_type, 'jclass'):
         j_class = real_ret_dtype.j_type.jclass
     else:
         j_class = real_ret_dtype.qst_type.clazz()
 
-    wrapper.return_type = j_class
-    wrapper.signature = sig_str_vectorization
-
-    return wrapper
-
-
-def _dh_vectorize(fn):
-    """A decorator to vectorize a Python function used in Deephaven query formulas and invoked on a row basis.
-
-    If this annotation is not used on a query function, the Deephaven query engine will make an effort to vectorize
-    the function. If vectorization is not possible, the query engine will use the original, non-vectorized function.
-    If this annotation is used on a function, the Deephaven query engine will use the vectorized function in a query,
-    or an error will result if the function can not be vectorized.
-
-    When this decorator is used on a function, the number and type of input and output arguments are changed.
-    These changes are only intended for use by the Deephaven query engine. Users are discouraged from using
-    vectorized functions in non-query code, since the function signature may change in future versions.
-
-    The current vectorized function signature includes (1) the size of the input arrays, (2) the output array,
-    and (3) the input arrays.
-    """
-    p_sig = _parse_signature(fn)
-    return_array = p_sig.ret_annotation.has_array
-    ret_dtype = dtypes.from_np_dtype(np.dtype(p_sig.ret_annotation.encoded_type[-1]))
-
-    @wraps(fn)
-    def wrapper(*args):
-        if len(args) != len(p_sig.params) + 2:
-            raise ValueError(
-                f"The number of arguments doesn't match the function signature. {len(args) - 2}, {p_sig.encoded}")
-        if args[0] <= 0:
-            raise ValueError(f"The chunk size argument must be a positive integer. {args[0]}")
-
-        chunk_size = args[0]
-        chunk_result = args[1]
-        if args[2:]:
-            vectorized_args = zip(*args[2:])
-            for i in range(chunk_size):
-                scalar_args = next(vectorized_args)
-                converted_args = _convert_args(p_sig, scalar_args)
-                ret = fn(*converted_args)
-                if return_array:
-                    chunk_result[i] = dtypes.array(ret_dtype, ret)
-                else:
-                    chunk_result[i] = _scalar(ret, ret_dtype)
-        else:
-            for i in range(chunk_size):
-                ret = fn()
-                if return_array:
-                    chunk_result[i] = dtypes.array(ret_dtype, ret)
-                else:
-                    chunk_result[i] = _scalar(ret, ret_dtype)
-
-        return chunk_result
-
-    wrapper.callable = fn
-    wrapper.dh_vectorized = True
-
-    if test_vectorization:
-        global vectorized_count
-        vectorized_count += 1
+    _udf_decorator.return_type = j_class
+    _udf_decorator.signature = p_sig.encoded
 
-    return wrapper
+    return _udf_decorator
```

## deephaven/_wrapper.py

```diff
@@ -1,32 +1,34 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module defines an Abstract Class for Java object wrappers.
 
 The primary purpose of this ABC is to enable downstream code to retrieve the wrapped Java objects in a uniform way.
 """
 from __future__ import annotations
 
 import importlib
 import inspect
 import pkgutil
 import sys
+import threading
 from abc import ABC, abstractmethod
-from typing import Set, Union, Optional, Any
+from typing import Set, Union, Optional, Any, List
 
 import jpy
 
 # a set of all the directly initializable wrapper classes
 _di_wrapper_classes: Set[JObjectWrapper] = set()
 _has_all_wrappers_imported = False
 
 JLivePyObjectWrapper = jpy.get_type('io.deephaven.server.plugin.python.LivePyObjectWrapper')
 
+_recursive_import_lock = threading.Lock()
 
 def _recursive_import(package_path: str) -> None:
     """ Recursively import every module in a package. """
     try:
         pkg = importlib.import_module(package_path)
     except ModuleNotFoundError:
         return
@@ -98,29 +100,26 @@
             _, param_meta = list(sig.parameters.items())[1]
             if param_meta.annotation == 'jpy.JType' or param_meta.annotation == jpy.JType:
                 return True
 
     return False
 
 
-def _lookup_wrapped_class(j_obj: jpy.JType) -> Optional[type]:
-    """ Returns the wrapper class for the specified Java object. """
+def _lookup_wrapped_class(j_obj: jpy.JType) -> List[JObjectWrapper]:
+    """ Returns the wrapper classes for the specified Java object. """
     # load every module in the deephaven package so that all the wrapper classes are loaded and available to wrap
     # the Java objects returned by calling resolve()
     global _has_all_wrappers_imported
     if not _has_all_wrappers_imported:
-        _recursive_import(__package__.partition(".")[0])
-        _has_all_wrappers_imported = True
+        with _recursive_import_lock:
+            if not _has_all_wrappers_imported:
+                _recursive_import(__package__.partition(".")[0])
+                _has_all_wrappers_imported = True
 
-    for wc in _di_wrapper_classes:
-        j_clz = wc.j_object_type
-        if j_clz.jclass.isInstance(j_obj):
-            return wc
-
-    return None
+    return [wc for wc in _di_wrapper_classes if wc.j_object_type.jclass.isInstance(j_obj)]
 
 
 def javaify(obj: Any) -> Optional[jpy.JType]:
     """
     Returns an object that is safe to pass to Java. Callers should take care to ensure that this happens
     in a liveness scope that reflects the lifetime of the reference to be passed to Java.
 
@@ -158,23 +157,51 @@
     # Definitely a JType, check if it is a LivePyObjectWrapper
     if j_obj.jclass == JLivePyObjectWrapper.jclass:
         return j_obj.getPythonObject()
     # Vanilla Java object, see if we have explicit wrapping for it
     return wrap_j_object(j_obj)
 
 
-def wrap_j_object(j_obj: jpy.JType) -> Union[JObjectWrapper, jpy.JType]:
-    """ Wraps the specified Java object as an instance of a custom wrapper class if one is available, otherwise returns
-    the raw Java object. """
+def _wrap_with_subclass(j_obj: jpy.JType, cls: type) -> Optional[JObjectWrapper]:
+    """ Returns a wrapper instance for the specified Java object by trying the entire subclasses' hierarchy. The
+    function employs a Depth First Search strategy to try the most specific subclass first. If no matching wrapper class is found,
+    returns None.
+
+    The premises for this function are as follows:
+    - The subclasses all share the same class attribute `j_object_type` (guaranteed by subclassing JObjectWrapper)
+    - The subclasses are all direct initialisable (guaranteed by subclassing JObjectWrapper)
+    - The subclasses are all distinct from each other and check for their uniqueness in the initializer (__init__), e.g.
+      InputTable checks for the presence of the INPUT_TABLE_ATTRIBUTE attribute on the Java object.
+    """
+    for subclass in cls.__subclasses__():
+        try:
+            if (wrapper := _wrap_with_subclass(j_obj, subclass)) is not None:
+                return wrapper
+            return subclass(j_obj)
+        except:
+            continue
+    return None
+
+
+def wrap_j_object(j_obj: jpy.JType) -> Optional[Union[JObjectWrapper, jpy.JType]]:
+    """ Wraps the specified Java object as an instance of the most specific custom wrapper class if one is available,
+    otherwise returns the raw Java object. """
     if j_obj is None:
         return None
 
-    wc = _lookup_wrapped_class(j_obj)
+    wcs = _lookup_wrapped_class(j_obj)
+    for wc in wcs:
+        try:
+            if (wrapper:= _wrap_with_subclass(j_obj, wc)) is not None:
+                return wrapper
+            return wc(j_obj)
+        except:
+            continue
 
-    return wc(j_obj) if wc else j_obj
+    return j_obj
 
 
 def unwrap(obj: Any) -> Union[jpy.JType, Any]:
     """ Returns the wrapped raw Java object if this is a wrapped Java object. Otherwise, returns the same object. """
     if isinstance(obj, JObjectWrapper):
         return obj.j_object
```

## deephaven/agg.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """This module implement various aggregations that can be used in deephaven table's aggregation operations."""
 
 from typing import List, Union, Any
 
 import jpy
```

## deephaven/appmode.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module supports writing Deephaven application mode Python scripts. """
 from typing import Dict
 
 import jpy
```

## deephaven/arrow.py

```diff
@@ -1,9 +1,9 @@
 #
-#     Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """This module supports conversions between pyarrow tables and Deephaven tables."""
 
 from typing import List, Dict
 
 import jpy
 import pyarrow as pa
```

## deephaven/calendar.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2023 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module defines functions for working with business calendars. """
 
 from typing import Optional, Union, List
 
 import jpy
```

## deephaven/column.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module implements the Column class and functions that work with Columns. """
 
 from dataclasses import dataclass, field
 from enum import Enum
 from typing import Sequence, Any
@@ -21,16 +21,14 @@
 _JColumnDefinitionType = jpy.get_type("io.deephaven.engine.table.ColumnDefinition$ColumnType")
 _JPrimitiveArrayConversionUtility = jpy.get_type("io.deephaven.integrations.common.PrimitiveArrayConversionUtility")
 
 
 class ColumnType(Enum):
     NORMAL = _JColumnDefinitionType.Normal
     """ A regular column. """
-    GROUPING = _JColumnDefinitionType.Grouping
-    """ A grouping column. """
     PARTITIONING = _JColumnDefinitionType.Partitioning
     """ A partitioning column. """
 
     def __repr__(self):
         return self.name
```

## deephaven/constants.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ The module defines the global constants including Deephaven's special numerical values. Other constants are defined
 at the individual module level because they are only locally applicable. """
 
 import jpy
```

## deephaven/csv.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ The deephaven.csv module supports reading an external CSV file into a Deephaven table and writing a
 Deephaven table out as a CSV file.
 """
 from typing import Dict, List
```

## deephaven/dherror.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module defines a custom exception for the Deephaven Python Integration Package.
 
 The custom exception is named DHError. It encapsulates exceptions thrown by the Deephaven engine and the
 Python/Java integration layer and provides 3 convenient properties: root_cause, compact_traceback, and
 traceback for easy debugging.
```

## deephaven/dtypes.py

```diff
@@ -1,25 +1,22 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module defines the data types supported by the Deephaven engine.
 
 Each data type is represented by a DType class which supports creating arrays of the same type and more.
 """
 from __future__ import annotations
 
 import datetime
-import sys
-import typing
-from typing import Any, Sequence, Callable, Dict, Type, Union, _GenericAlias, Optional
+from typing import Any, Sequence, Callable, Dict, Type, Union, Optional
 
 import jpy
 import numpy as np
-import numpy._typing as npt
 import pandas as pd
 
 from deephaven import DHError
 from deephaven.constants import NULL_BYTE, NULL_SHORT, NULL_INT, NULL_LONG, NULL_FLOAT, NULL_DOUBLE, NULL_CHAR
 
 _JQstType = jpy.get_type("io.deephaven.qst.type.Type")
 _JTableTools = jpy.get_type("io.deephaven.engine.util.TableTools")
@@ -175,15 +172,15 @@
     int32: NULL_INT,
     int64: NULL_LONG,
     float32: NULL_FLOAT,
     float64: NULL_DOUBLE,
 }
 
 _BUILDABLE_ARRAY_DTYPE_MAP = {
-    bool_: bool_array,
+    bool_: boolean_array,
     byte: int8_array,
     char: char_array,
     int16: int16_array,
     int32: int32_array,
     int64: int64_array,
     float32: float32_array,
     float64: float64_array,
@@ -224,31 +221,45 @@
 
     return lambda v: null_value if v is None else v
 
 
 def _instant_array(data: Sequence) -> jpy.JType:
     """Converts a sequence of either datetime64[ns], datetime.datetime, pandas.Timestamp, datetime strings,
     or integers in nanoseconds, to a Java array of Instant values. """
+
+    if len(data) == 0:
+        return jpy.array(Instant.j_type, [])
+
+    if isinstance(data, np.ndarray) and data.dtype.kind == 'U':
+        return _JPrimitiveArrayConversionUtility.translateArrayStringToInstant(data)
+
+    if all((d == None or isinstance(d, str)) for d in data):
+        jdata = jpy.array('java.lang.String', data)
+        return _JPrimitiveArrayConversionUtility.translateArrayStringToInstant(jdata)
+
     # try to convert to numpy array of datetime64 if not already, so that we can call translateArrayLongToInstant on
     # it to reduce the number of round trips to the JVM
     if not isinstance(data, np.ndarray):
         try:
-            data = np.array([pd.Timestamp(dt).to_numpy() for dt in data], dtype=np.datetime64)
+            # Pandas drops unrecognized time zones, so it may handle time zones incorrectly when parsing strings
+            if not any(isinstance(i, str) for i in data):
+                data = np.array([pd.Timestamp(dt).to_numpy() for dt in data], dtype=np.datetime64)
         except Exception as e:
             ...
 
-    if isinstance(data, np.ndarray) and data.dtype.kind in ('M', 'i', 'U'):
+    # Pandas drops unrecognized time zones, so it may handle time zones incorrectly, so do not handle 'U' dtype
+    if isinstance(data, np.ndarray) and data.dtype.kind in ('M', 'i'):
         if data.dtype.kind == 'M':
             longs = jpy.array('long', data.astype('datetime64[ns]').astype('int64'))
         elif data.dtype.kind == 'i':
             longs = jpy.array('long', data.astype('int64'))
-        else:  # data.dtype.kind == 'U'
-            longs = jpy.array('long', [pd.Timestamp(str(dt)).to_numpy().astype('int64') for dt in data])
-        data = _JPrimitiveArrayConversionUtility.translateArrayLongToInstant(longs)
-        return data
+        else:
+            raise Exception(f"Unexpected dtype: {data.dtype.kind}")
+
+        return _JPrimitiveArrayConversionUtility.translateArrayLongToInstant(longs)
 
     if not isinstance(data, instant_array.j_type):
         from deephaven.time import to_j_instant
         data = [to_j_instant(d) for d in data]
 
     return jpy.array(Instant.j_type, data)
 
@@ -300,15 +311,15 @@
                 seq = _JPrimitiveArrayConversionUtility.translateArrayByteToBoolean(j_bytes)
 
         return jpy.array(dtype.j_type, seq)
     except Exception as e:
         raise DHError(e, f"failed to create a Java {dtype.j_name} array.") from e
 
 
-def from_jtype(j_class: Any) -> DType:
+def from_jtype(j_class: Any) -> Optional[DType]:
     """ looks up a DType that matches the java type, if not found, creates a DType for it. """
     if not j_class:
         return None
 
     j_name = j_class.getName()
     dtype = _j_name_type_map.get(j_name)
     if not dtype:
@@ -387,69 +398,7 @@
                 return to_j_instant(x)
         elif isinstance(x, (datetime.datetime, pd.Timestamp)):
                 from deephaven.time import to_j_instant
                 return to_j_instant(x)
         return x
     except:
         return x
-
-
-def _np_dtype_char(t: Union[type, str]) -> str:
-    """Returns the numpy dtype character code for the given type."""
-    try:
-        np_dtype = np.dtype(t if t else "object")
-        if np_dtype.kind == "O":
-            if t in (datetime.datetime, pd.Timestamp):
-                return "M"
-    except TypeError:
-        np_dtype = np.dtype("object")
-
-    return np_dtype.char
-
-
-def _component_np_dtype_char(t: type) -> Optional[str]:
-    """Returns the numpy dtype character code for the given type's component type if the type is a Sequence type or
-    numpy ndarray, otherwise return None. """
-    component_type = None
-    if isinstance(t, _GenericAlias) and issubclass(t.__origin__, Sequence):
-        component_type = t.__args__[0]
-        # if the component type is a DType, get its numpy type
-        if isinstance(component_type, DType):
-            component_type = component_type.np_type
-
-    if not component_type:
-        component_type = _np_ndarray_component_type(t)
-
-    if component_type:
-        return _np_dtype_char(component_type)
-    else:
-        return None
-
-
-def _np_ndarray_component_type(t: type) -> Optional[type]:
-    """Returns the numpy ndarray component type if the type is a numpy ndarray, otherwise return None."""
-
-    # Py3.8: npt.NDArray can be used in Py 3.8 as a generic alias, but a specific alias (e.g. npt.NDArray[np.int64])
-    # is an instance of a private class of np, yet we don't have a choice but to use it. And when npt.NDArray is used,
-    # the 1st argument is typing.Any, the 2nd argument is another generic alias of which the 1st argument is the
-    # component type
-    component_type = None
-    if sys.version_info.major == 3 and sys.version_info.minor == 8:
-        if isinstance(t, np._typing._generic_alias._GenericAlias) and t.__origin__ == np.ndarray:
-            component_type = t.__args__[1].__args__[0]
-    # Py3.9+, np.ndarray as a generic alias is only supported in Python 3.9+, also npt.NDArray is still available but a
-    # specific alias (e.g. npt.NDArray[np.int64]) now is an instance of typing.GenericAlias.
-    # when npt.NDArray is used, the 1st argument is typing.Any, the 2nd argument is another generic alias of which
-    # the 1st argument is the component type
-    # when np.ndarray is used, the 1st argument is the component type
-    if not component_type and sys.version_info.major == 3 and sys.version_info.minor > 8:
-        import types
-        if isinstance(t, types.GenericAlias) and (issubclass(t.__origin__, Sequence) or t.__origin__ == np.ndarray):
-            nargs = len(t.__args__)
-            if nargs == 1:
-                component_type = t.__args__[0]
-            elif nargs == 2:  # for npt.NDArray[np.int64], etc.
-                a0 = t.__args__[0]
-                a1 = t.__args__[1]
-                if a0 == typing.Any and isinstance(a1, types.GenericAlias):
-                    component_type = a1.__args__[0]
-    return component_type
```

## deephaven/execution_context.py

```diff
@@ -1,9 +1,9 @@
 #
-#     Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """This module gives users the ability to directly manage the Deephaven query execution context on threads, which is
 critical for applications to correctly launch deferred query evaluations, such as table update operations in threads.
 """
 
 from typing import Sequence, Union
 from contextlib import ContextDecorator
```

## deephaven/filters.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module implement various filters that can be used in deephaven table's filter operations."""
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Union
```

## deephaven/html.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module supports exporting Deephaven data in the HTML format. """
 
 import jpy
 
 from deephaven import DHError
```

## deephaven/jcompat.py

```diff
@@ -1,23 +1,23 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module provides Java compatibility support including convenience functions to create some widely used Java
 data structures from corresponding Python ones in order to be able to call Java methods. """
 
-from typing import Any, Callable, Dict, Iterable, List, Sequence, Set, TypeVar, Union, Tuple, Literal
+from typing import Any, Callable, Dict, Iterable, List, Sequence, Set, TypeVar, Union, Optional
 
 import jpy
 import numpy as np
 import pandas as pd
 
 from deephaven import dtypes, DHError
-from deephaven._wrapper import unwrap, wrap_j_object
-from deephaven.dtypes import DType, _PRIMITIVE_DTYPE_NULL_MAP, _J_ARRAY_NP_TYPE_MAP
+from deephaven._wrapper import unwrap, wrap_j_object, JObjectWrapper
+from deephaven.dtypes import DType, _PRIMITIVE_DTYPE_NULL_MAP
 
 _NULL_BOOLEAN_AS_BYTE = jpy.get_type("io.deephaven.util.BooleanUtils").NULL_BOOLEAN_AS_BYTE
 _JPrimitiveArrayConversionUtility = jpy.get_type("io.deephaven.integrations.common.PrimitiveArrayConversionUtility")
 
 _DH_PANDAS_NULLABLE_TYPE_MAP: Dict[DType, pd.api.extensions.ExtensionDtype] = {
     dtypes.bool_: pd.BooleanDtype,
     dtypes.byte: pd.Int8Dtype,
@@ -205,36 +205,33 @@
     if not isinstance(v, Sequence) or isinstance(v, str):
         return (unwrap(v),)
     else:
         return tuple((unwrap(o) for o in v))
 
 
 def _j_array_to_numpy_array(dtype: DType, j_array: jpy.JType, conv_null: bool, type_promotion: bool = False) -> \
-        np.ndarray:
+        Optional[np.ndarray]:
     """ Produces a numpy array from the DType and given Java array.
 
     Args:
         dtype (DType): The dtype of the Java array
         j_array (jpy.JType): The Java array to convert
         conv_null (bool): If True, convert nulls to the null value for the dtype
-        type_promotion (bool): Ignored when conv_null is False.  When type_promotion is False, (1) input Java integer,
-            boolean, or character arrays containing Deephaven nulls yield an exception, (2) input Java float or double
-            arrays containing Deephaven nulls have null values converted to np.nan, and (3) input Java arrays without
-            Deephaven nulls are converted to the target type.  When type_promotion is True, (1) input Java integer,
-            boolean, or character arrays containing Deephaven nulls are converted to np.float64 arrays and Deephaven
-            null values are converted to np.nan, (2) input Java float or double arrays containing Deephaven nulls have
-            null values converted to np.nan, and (3) input Java arrays without Deephaven nulls are converted to the
-            target type.  Defaults to False.
+        type_promotion (bool): Ignored when conv_null is False. When conv_null is True, see the description for the same
+            named parameter in dh_nulls_to_nan().
 
     Returns:
-        np.ndarray: The numpy array
+        np.ndarray: The numpy array or None if the Java array is None
 
     Raises:
         DHError
     """
+    if j_array is None:
+        return None
+
     if dtype.is_primitive:
         np_array = np.frombuffer(j_array, dtype.np_type)
     elif dtype == dtypes.Instant:
         longs = _JPrimitiveArrayConversionUtility.translateArrayInstantToLong(j_array)
         np_long_array = np.frombuffer(longs, np.int64)
         np_array = np_long_array.view(dtype.np_type)
     elif dtype == dtypes.bool_:
@@ -248,34 +245,57 @@
             np_array = np.frombuffer(j_array, dtype.np_type)
         except:
             np_array = np.array(j_array, np.object_)
     else:
         np_array = np.array(j_array, np.object_)
 
     if conv_null:
-        if dh_null := _PRIMITIVE_DTYPE_NULL_MAP.get(dtype):
-            if dtype in (dtypes.float32, dtypes.float64):
-                np_array = np.copy(np_array)
-                np_array[np_array == dh_null] = np.nan
-            else:
-                if dtype is dtypes.bool_:  # needs to change its type to byte for dh null detection
-                    np_array = np.frombuffer(np_array, np.byte)
-
-                if any(np_array[np_array == dh_null]):
-                    if not type_promotion:
-                        raise DHError(f"Problem creating numpy array.  Java {dtype} array contains Deephaven null values, but numpy {np_array.dtype} array does not support null values")
-                    np_array = np_array.astype(np.float64)
-                    np_array[np_array == dh_null] = np.nan
-                else:
-                    if dtype is dtypes.bool_:  # needs to change its type back to bool
-                        np_array = np.frombuffer(np_array, np.bool_)
-                    return np_array
+        return dh_null_to_nan(np_array, type_promotion)
 
     return np_array
 
+def dh_null_to_nan(np_array: np.ndarray, type_promotion: bool = False) -> np.ndarray:
+    """Converts Deephaven primitive null values in the given numpy array to np.nan. No conversion is performed on
+    non-primitive types.
+
+    Note, the input numpy array is modified in place if it is of a float or double type. If that's not a desired behavior,
+    pass a copy of the array instead. For input arrays of other types, a new array is always returned.
+
+    Args:
+        np_array (np.ndarray): The numpy array to convert
+        type_promotion (bool): When False, integer, boolean, or character arrays will cause an exception to be raised.
+            When True, integer, boolean, or character arrays are converted to new np.float64 arrays and Deephaven null
+            values in them are converted to np.nan. Numpy arrays of float or double types are not affected by this flag
+            and Deephaven nulls will always be converted to np.nan in place. Defaults to False.
+
+    Returns:
+        np.ndarray: The numpy array with Deephaven nulls converted to np.nan.
+
+    Raises:
+        DHError
+    """
+    if not isinstance(np_array, np.ndarray):
+        raise DHError(message="The given np_array argument is not a numpy array.")
+
+    dtype = dtypes.from_np_dtype(np_array.dtype)
+    if dh_null := _PRIMITIVE_DTYPE_NULL_MAP.get(dtype):
+        if dtype in (dtypes.float32, dtypes.float64):
+            np_array = np.copy(np_array)
+            np_array[np_array == dh_null] = np.nan
+        else:
+            if not type_promotion:
+                raise DHError(message=f"failed to convert DH nulls to np.nan in the numpy array. The array is "
+                                      f"of {np_array.dtype.type} type  but type_promotion is False")
+            if dtype is dtypes.bool_:  # needs to change its type to byte for dh null detection
+                np_array = np.frombuffer(np_array, np.byte)
+
+            np_array = np_array.astype(np.float64)
+            np_array[np_array == dh_null] = np.nan
+
+    return np_array
 
 def _j_array_to_series(dtype: DType, j_array: jpy.JType, conv_null: bool) -> pd.Series:
     """Produce a copy of the specified Java array as a pandas.Series object.
 
     Args:
         dtype (DType): the dtype of the Java array
         j_array (jpy.JType): the Java array
@@ -300,7 +320,37 @@
         pd_ex_dtype = _DH_PANDAS_NULLABLE_TYPE_MAP.get(dtype)
         s = pd.Series(data=np_array, dtype=pd_ex_dtype(), copy=False)
         s.mask(s == nv, inplace=True)
     else:
         s = pd.Series(data=np_array, copy=False)
 
     return s
+
+
+class AutoCloseable(JObjectWrapper):
+    """A context manager wrapper to allow Java AutoCloseable to be used in with statements.
+    
+    When constructing a new instance, the Java AutoCloseable must not be closed."""
+
+    j_object_type = jpy.get_type("java.lang.AutoCloseable")
+
+    def __init__(self, j_auto_closeable):
+        self._j_auto_closeable = j_auto_closeable
+        self.closed = False
+
+    def __enter__(self):
+        return self
+
+    def close(self):
+        if not self.closed:
+            self.closed = True
+            self._j_auto_closeable.close()
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        self.close()
+
+    def __del__(self):
+        self.close()
+
+    @property
+    def j_object(self) -> jpy.JType:
+        return self._j_auto_closeable
```

## deephaven/liveness_scope.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """This module gives the users a finer degree of control over when to clean up unreferenced nodes in the query update
 graph instead of solely relying on garbage collection.
 
 Examples:
 
@@ -52,15 +52,15 @@
        was managed to begin with.
 """
 import contextlib
 
 import jpy
 
 from typing import Union, Iterator
-from warnings import warn
+
 
 from deephaven import DHError
 from deephaven._wrapper import JObjectWrapper
 
 _JLivenessScopeStack = jpy.get_type("io.deephaven.engine.liveness.LivenessScopeStack")
 _JLivenessScope = jpy.get_type("io.deephaven.engine.liveness.LivenessScope")
 _JLivenessReferent = jpy.get_type("io.deephaven.engine.liveness.LivenessReferent")
```

## deephaven/numpy.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module supports the conversion between Deephaven tables and numpy arrays. """
 import re
 from typing import List
 
 import jpy
@@ -23,16 +23,27 @@
 
 def _to_column_name(name: str) -> str:
     """ Transforms the given name string into a valid table column name. """
     tmp_name = re.sub(r"\W+", " ", str(name)).strip()
     return re.sub(r"\s+", "_", tmp_name)
 
 
-def column_to_numpy_array(col_def: Column, j_array: jpy.JType) -> np.ndarray:
-    """ Produces a numpy array from the given Java array and the Table column definition."""
+def _column_to_numpy_array(col_def: Column, j_array: jpy.JType) -> np.ndarray:
+    """ Produces a numpy array from the given Java array and the Table column definition.
+
+    Args:
+        col_def (Column): the column definition
+        j_array (jpy.JType): the Java array
+
+    Returns:
+        np.ndarray
+
+    Raises:
+        DHError
+    """
     try:
         return _j_array_to_numpy_array(col_def.data_type, j_array, conv_null=False, type_promotion=False)
     except DHError:
         raise
     except Exception as e:
         raise DHError(e, f"failed to create a numpy array for the column {col_def.name}") from e
 
@@ -45,15 +56,15 @@
             np_array = np.empty(shape=(len(j_arrays[0]), len(j_arrays)), dtype=col_def.data_type.np_type)
             for i, j_array in enumerate(j_arrays):
                 np_array[:, i] = np.frombuffer(j_array, col_def.data_type.np_type)
             return np_array
         else:
             np_arrays = []
             for j_array in j_arrays:
-                np_arrays.append(column_to_numpy_array(col_def=col_def, j_array=j_array))
+                np_arrays.append(_column_to_numpy_array(col_def=col_def, j_array=j_array))
             return np.stack(np_arrays, axis=1)
     except DHError:
         raise
     except Exception as e:
         raise DHError(e, f"failed to create a numpy array for the column {col_def.name}") from e
```

## deephaven/pandas.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module supports the conversion between Deephaven tables and pandas DataFrames. """
 from typing import List, Literal
 
 import jpy
 import numpy as np
@@ -110,15 +110,15 @@
     Returns:
         a pandas DataFrame
 
     Raise:
         DHError
     """
     try:
-        if dtype_backend is not None and not _is_dtype_backend_supported:
+        if dtype_backend == "pyarrow" and not _is_dtype_backend_supported:
             raise DHError(message=f"the dtype_backend ({dtype_backend}) option is only available for pandas 2.0.0 and "
                                   f"above. {pd.__version__} is being used.")
 
         if dtype_backend is not None and not conv_null:
             raise DHError(message="conv_null can't be turned off when dtype_backend is either numpy_nullable or "
                                   "pyarrow")
```

## deephaven/parquet.py

```diff
@@ -1,68 +1,105 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module supports reading an external Parquet files into Deephaven tables and writing Deephaven tables out as
 Parquet files. """
+from warnings import warn
 from dataclasses import dataclass
 from enum import Enum
-from typing import List, Optional, Union, Dict
+from typing import List, Optional, Union, Dict, Sequence
 
 import jpy
 
 from deephaven import DHError
 from deephaven.column import Column
 from deephaven.dtypes import DType
-from deephaven.table import Table
+from deephaven.jcompat import j_array_list
+from deephaven.table import Table, PartitionedTable
 from deephaven.experimental import s3
 
 _JParquetTools = jpy.get_type("io.deephaven.parquet.table.ParquetTools")
 _JFile = jpy.get_type("java.io.File")
 _JCompressionCodecName = jpy.get_type("org.apache.parquet.hadoop.metadata.CompressionCodecName")
 _JParquetInstructions = jpy.get_type("io.deephaven.parquet.table.ParquetInstructions")
+_JParquetFileLayout = jpy.get_type("io.deephaven.parquet.table.ParquetInstructions$ParquetFileLayout")
 _JTableDefinition = jpy.get_type("io.deephaven.engine.table.TableDefinition")
 
 
 @dataclass
 class ColumnInstruction:
     """  This class specifies the instructions for reading/writing a Parquet column. """
     column_name: Optional[str] = None
     parquet_column_name: Optional[str] = None
     codec_name: Optional[str] = None
     codec_args: Optional[str] = None
     use_dictionary: bool = False
 
 
+class ParquetFileLayout(Enum):
+    """ The parquet file layout. """
+
+    SINGLE_FILE = 1
+    """ A single parquet file. """
+
+    FLAT_PARTITIONED = 2
+    """ A single directory of parquet files. """
+
+    KV_PARTITIONED = 3
+    """ A hierarchically partitioned directory layout of parquet files. Directory names are of the format "key=value" 
+     with keys derived from the partitioning columns. """
+
+    METADATA_PARTITIONED = 4
+    """
+    Layout can be used to describe:
+        - A directory containing a METADATA_FILE_NAME parquet file and an optional COMMON_METADATA_FILE_NAME parquet file
+        - A single parquet METADATA_FILE_NAME file
+        - A single parquet COMMON_METADATA_FILE_NAME file
+    """
+
+
 def _build_parquet_instructions(
     col_instructions: Optional[List[ColumnInstruction]] = None,
     compression_codec_name: Optional[str] = None,
     max_dictionary_keys: Optional[int] = None,
     max_dictionary_size: Optional[int] = None,
     is_legacy_parquet: bool = False,
     target_page_size: Optional[int] = None,
     is_refreshing: bool = False,
     for_read: bool = True,
     force_build: bool = False,
+    generate_metadata_files: Optional[bool] = None,
+    base_name: Optional[str] = None,
+    file_layout: Optional[ParquetFileLayout] = None,
+    table_definition: Optional[Union[Dict[str, DType], List[Column]]] = None,
+    col_definitions: Optional[List[Column]] = None,
+    index_columns: Optional[Sequence[Sequence[str]]] = None,
     special_instructions: Optional[s3.S3Instructions] = None,
 ):
     if not any(
         [
             force_build,
             col_instructions,
             compression_codec_name,
             max_dictionary_keys is not None,
             max_dictionary_size is not None,
             is_legacy_parquet,
             target_page_size is not None,
             is_refreshing,
+            generate_metadata_files is not None,
+            base_name is not None,
+            file_layout is not None,
+            table_definition is not None,
+            col_definitions is not None,
+            index_columns is not None,
             special_instructions is not None
         ]
     ):
-        return None
+        return _JParquetInstructions.EMPTY
 
     builder = _JParquetInstructions.builder()
     if col_instructions is not None:
         for ci in col_instructions:
             if for_read and not ci.parquet_column_name:
                 raise ValueError("must specify the parquet column name for read.")
             if not for_read and not ci.column_name:
@@ -88,14 +125,35 @@
 
     if target_page_size is not None:
         builder.setTargetPageSize(target_page_size)
 
     if is_refreshing:
         builder.setIsRefreshing(is_refreshing)
 
+    if generate_metadata_files:
+        builder.setGenerateMetadataFiles(generate_metadata_files)
+
+    if base_name:
+        builder.setBaseNameForPartitionedParquetData(base_name)
+
+    if file_layout is not None:
+        builder.setFileLayout(_j_file_layout(file_layout))
+
+    if table_definition is not None and col_definitions is not None:
+        raise ValueError("table_definition and col_definitions cannot both be specified.")
+
+    if table_definition is not None:
+        builder.setTableDefinition(_j_table_definition(table_definition))
+
+    if col_definitions is not None:
+        builder.setTableDefinition(_JTableDefinition.of([col.j_column_definition for col in col_definitions]))
+
+    if index_columns:
+        builder.addAllIndexColumns(_j_list_of_list_of_string(index_columns))
+
     if special_instructions is not None:
         builder.setSpecialInstructions(special_instructions.j_object)
 
     return builder.build()
 
 def _j_table_definition(table_definition: Union[Dict[str, DType], List[Column], None]) -> Optional[jpy.JType]:
     if table_definition is None:
@@ -111,28 +169,26 @@
         return _JTableDefinition.of(
             [col.j_column_definition for col in table_definition]
         )
     else:
         raise DHError(f"Unexpected table_definition type: {type(table_definition)}")
 
 
-class ParquetFileLayout(Enum):
-    """ The parquet file layout. """
-
-    SINGLE_FILE = 1
-    """ A single parquet file. """
-
-    FLAT_PARTITIONED = 2
-    """ A single directory of parquet files. """
-
-    KV_PARTITIONED = 3
-    """ A key-value directory partitioning of parquet files. """
-
-    METADATA_PARTITIONED = 4
-    """ A directory containing a _metadata parquet file and an optional _common_metadata parquet file. """
+def _j_file_layout(file_layout: Optional[ParquetFileLayout]) -> Optional[jpy.JType]:
+    if file_layout is None:
+        return None
+    if file_layout == ParquetFileLayout.SINGLE_FILE:
+        return _JParquetFileLayout.SINGLE_FILE
+    if file_layout == ParquetFileLayout.FLAT_PARTITIONED:
+        return _JParquetFileLayout.FLAT_PARTITIONED
+    if file_layout == ParquetFileLayout.KV_PARTITIONED:
+        return _JParquetFileLayout.KV_PARTITIONED
+    if file_layout == ParquetFileLayout.METADATA_PARTITIONED:
+        return _JParquetFileLayout.METADATA_PARTITIONED
+    raise DHError(f"Invalid parquet file_layout '{file_layout}'")
 
 
 def read(
     path: str,
     col_instructions: Optional[List[ColumnInstruction]] = None,
     is_legacy_parquet: bool = False,
     is_refreshing: bool = False,
@@ -140,25 +196,24 @@
     table_definition: Union[Dict[str, DType], List[Column], None] = None,
     special_instructions: Optional[s3.S3Instructions] = None,
 ) -> Table:
     """ Reads in a table from a single parquet, metadata file, or directory with recognized layout.
 
     Args:
         path (str): the file or directory to examine
-        col_instructions (Optional[List[ColumnInstruction]]): instructions for customizations while reading, None by
-            default.
+        col_instructions (Optional[List[ColumnInstruction]]): instructions for customizations while reading particular
+            columns, default is None, which means no specialization for any column
         is_legacy_parquet (bool): if the parquet data is legacy
         is_refreshing (bool): if the parquet data represents a refreshing source
         file_layout (Optional[ParquetFileLayout]): the parquet file layout, by default None. When None, the layout is
             inferred.
         table_definition (Union[Dict[str, DType], List[Column], None]): the table definition, by default None. When None,
             the definition is inferred from the parquet file(s). Setting a definition guarantees the returned table will
             have that definition. This is useful for bootstrapping purposes when the initially partitioned directory is
-            empty and is_refreshing=True. It is also useful for specifying a subset of the parquet definition. When set,
-            file_layout must also be set.
+            empty and is_refreshing=True. It is also useful for specifying a subset of the parquet definition.
         special_instructions (Optional[s3.S3Instructions]): Special instructions for reading parquet files, useful when
             reading files from a non-local file system, like S3. By default, None.
 
     Returns:
         a table
 
     Raises:
@@ -169,50 +224,26 @@
         read_instructions = _build_parquet_instructions(
             col_instructions=col_instructions,
             is_legacy_parquet=is_legacy_parquet,
             is_refreshing=is_refreshing,
             for_read=True,
             force_build=True,
             special_instructions=special_instructions,
+            file_layout=file_layout,
+            table_definition=table_definition,
         )
-        j_table_definition = _j_table_definition(table_definition)
-        if j_table_definition is not None:
-            if not file_layout:
-                raise DHError("Must provide file_layout when table_definition is set")
-            if file_layout == ParquetFileLayout.SINGLE_FILE:
-                j_table = _JParquetTools.readSingleFileTable(path, read_instructions, j_table_definition)
-            elif file_layout == ParquetFileLayout.FLAT_PARTITIONED:
-                j_table = _JParquetTools.readFlatPartitionedTable(_JFile(path), read_instructions, j_table_definition)
-            elif file_layout == ParquetFileLayout.KV_PARTITIONED:
-                j_table = _JParquetTools.readKeyValuePartitionedTable(_JFile(path), read_instructions, j_table_definition)
-            elif file_layout == ParquetFileLayout.METADATA_PARTITIONED:
-                raise DHError(f"file_layout={ParquetFileLayout.METADATA_PARTITIONED} with table_definition not currently supported")
-            else:
-                raise DHError(f"Invalid parquet file_layout '{file_layout}'")
-        else:
-            if not file_layout:
-                j_table = _JParquetTools.readTable(path, read_instructions)
-            elif file_layout == ParquetFileLayout.SINGLE_FILE:
-                j_table = _JParquetTools.readSingleFileTable(path, read_instructions)
-            elif file_layout == ParquetFileLayout.FLAT_PARTITIONED:
-                j_table = _JParquetTools.readFlatPartitionedTable(_JFile(path), read_instructions)
-            elif file_layout == ParquetFileLayout.KV_PARTITIONED:
-                j_table = _JParquetTools.readKeyValuePartitionedTable(_JFile(path), read_instructions)
-            elif file_layout == ParquetFileLayout.METADATA_PARTITIONED:
-                j_table = _JParquetTools.readPartitionedTableWithMetadata(_JFile(path), read_instructions)
-            else:
-                raise DHError(f"Invalid parquet file_layout '{file_layout}'")
-        return Table(j_table=j_table)
+        return Table(_JParquetTools.readTable(path, read_instructions))
     except Exception as e:
         raise DHError(e, "failed to read parquet data.") from e
 
+def _j_string_array(str_seq: Sequence[str]):
+    return jpy.array("java.lang.String", str_seq)
 
-def _j_file_array(paths: List[str]):
-    return jpy.array("java.io.File", [_JFile(el) for el in paths])
-
+def _j_list_of_list_of_string(str_seq_seq: Sequence[Sequence[str]]):
+    return j_array_list([j_array_list(str_seq) for str_seq in str_seq_seq])
 
 def delete(path: str) -> None:
     """ Deletes a Parquet table on disk.
 
     Args:
         path (str): path to delete
 
@@ -224,113 +255,237 @@
     except Exception as e:
         raise DHError(e, f"failed to delete a parquet table: {path} on disk.") from e
 
 
 def write(
     table: Table,
     path: str,
+    table_definition: Optional[Union[Dict[str, DType], List[Column]]] = None,
     col_definitions: Optional[List[Column]] = None,
     col_instructions: Optional[List[ColumnInstruction]] = None,
     compression_codec_name: Optional[str] = None,
     max_dictionary_keys: Optional[int] = None,
     max_dictionary_size: Optional[int] = None,
     target_page_size: Optional[int] = None,
+    generate_metadata_files: Optional[bool] = None,
+    index_columns: Optional[Sequence[Sequence[str]]] = None
 ) -> None:
     """ Write a table to a Parquet file.
 
     Args:
         table (Table): the source table
         path (str): the destination file path; the file name should end in a ".parquet" extension. If the path
-            includes non-existing directories they are created. If there is an error, any intermediate directories
+            includes any non-existing directories, they are created. If there is an error, any intermediate directories
             previously created are removed; note this makes this method unsafe for concurrent use
-        col_definitions (Optional[List[Column]]): the column definitions to use, default is None
-        col_instructions (Optional[List[ColumnInstruction]]): instructions for customizations while writing, default is None
-        compression_codec_name (Optional[str]): the default compression codec to use, if not specified, defaults to SNAPPY
-        max_dictionary_keys (Optional[int]): the maximum dictionary keys allowed, if not specified, defaults to 2^20 (1,048,576)
-        max_dictionary_size (Optional[int]): the maximum dictionary size (in bytes) allowed, defaults to 2^20 (1,048,576)
+        table_definition (Optional[Union[Dict[str, DType], List[Column]]): the table definition to use for writing,
+            instead of the definitions implied by the table. Default is None, which means use the column definitions
+            implied by the table. This definition can be used to skip some columns or add additional columns with
+            null values. Both table_definition and col_definitions cannot be specified at the same time.
+        col_definitions (Optional[List[Column]]): the column definitions to use for writing, instead of the
+            definitions implied by the table. Default is None, which means use the column definitions implied by the
+            table. This argument is deprecated and will be removed in a future release. Use table_definition instead.
+        col_instructions (Optional[List[ColumnInstruction]]): instructions for customizations while writing particular
+            columns, default is None, which means no specialization for any column
+        compression_codec_name (Optional[str]): the compression codec to use. Allowed values include "UNCOMPRESSED",
+            "SNAPPY", "GZIP", "LZO", "LZ4", "LZ4_RAW", "ZSTD", etc. If not specified, defaults to "SNAPPY".
+        max_dictionary_keys (Optional[int]): the maximum number of unique keys the writer should add to a dictionary page
+            before switching to non-dictionary encoding, never evaluated for non-String columns, defaults to 2^20 (1,048,576)
+        max_dictionary_size (Optional[int]): the maximum number of bytes the writer should add to the dictionary before
+            switching to non-dictionary encoding, never evaluated for non-String columns, defaults to 2^20 (1,048,576)
         target_page_size (Optional[int]): the target page size in bytes, if not specified, defaults to 2^20 bytes (1 MiB)
-
+        generate_metadata_files (Optional[bool]): whether to generate parquet _metadata and _common_metadata files,
+            defaults to False. Generating these files can help speed up reading of partitioned parquet data because these
+            files contain metadata (including schema) about the entire dataset, which can be used to skip reading some
+            files.
+        index_columns (Optional[Sequence[Sequence[str]]]): sequence of sequence containing the column names for indexes
+            to persist. The write operation will store the index info for the provided columns as sidecar tables. For
+            example, if the input is [["Col1"], ["Col1", "Col2"]], the write operation will store the index info for
+            ["Col1"] and for ["Col1", "Col2"]. By default, data indexes to write are determined by those present on the
+            source table. This argument can be used to narrow the set of indexes to write, or to be explicit about the
+            expected set of indexes present on all sources. Indexes that are specified but missing will be computed on
+            demand.
     Raises:
         DHError
     """
+    if col_definitions is not None:
+        warn("col_definitions is deprecated and will be removed in a future release. Use table_definition "
+             "instead.", DeprecationWarning, stacklevel=2)
     try:
         write_instructions = _build_parquet_instructions(
             col_instructions=col_instructions,
             compression_codec_name=compression_codec_name,
             max_dictionary_keys=max_dictionary_keys,
             max_dictionary_size=max_dictionary_size,
             target_page_size=target_page_size,
             for_read=False,
+            generate_metadata_files=generate_metadata_files,
+            table_definition=table_definition,
+            col_definitions=col_definitions,
+            index_columns=index_columns,
         )
+        _JParquetTools.writeTable(table.j_table, path, write_instructions)
+    except Exception as e:
+        raise DHError(e, "failed to write to parquet data.") from e
+
+
+def write_partitioned(
+        table: Union[Table, PartitionedTable],
+        destination_dir: str,
+        table_definition: Optional[Union[Dict[str, DType], List[Column]]] = None,
+        col_definitions: Optional[List[Column]] = None,
+        col_instructions: Optional[List[ColumnInstruction]] = None,
+        compression_codec_name: Optional[str] = None,
+        max_dictionary_keys: Optional[int] = None,
+        max_dictionary_size: Optional[int] = None,
+        target_page_size: Optional[int] = None,
+        base_name: Optional[str] = None,
+        generate_metadata_files: Optional[bool] = None,
+        index_columns: Optional[Sequence[Sequence[str]]] = None
+) -> None:
+    """ Write table to disk in parquet format with the partitioning columns written as "key=value" format in a nested
+    directory structure. For example, for a partitioned column "date", we will have a directory structure like
+    "date=2021-01-01/<base_name>.parquet", "date=2021-01-02/<base_name>.parquet", etc. where "2021-01-01" and
+    "2021-01-02" are the partition values and "<base_name>" is passed as an optional parameter. All the necessary
+    subdirectories are created if they do not exist.
+
+    Args:
+        table (Table): the source table or partitioned table
+        destination_dir (str): The path to destination root directory in which the partitioned parquet data will be stored
+            in a nested directory structure format. Non-existing directories in the provided path will be created.
+        table_definition (Optional[Union[Dict[str, DType], List[Column]]): the table definition to use for writing,
+            instead of the definitions implied by the table. Default is None, which means use the column definitions
+            implied by the table. This definition can be used to skip some columns or add additional columns with
+            null values. Both table_definition and col_definitions cannot be specified at the same time.
+        col_definitions (Optional[List[Column]]): the column definitions to use for writing, instead of the definitions
+            implied by the table. Default is None, which means use the column definitions implied by the table. This
+            argument is deprecated and will be removed in a future release. Use table_definition instead.
+        col_instructions (Optional[List[ColumnInstruction]]): instructions for customizations while writing particular
+            columns, default is None, which means no specialization for any column
+        compression_codec_name (Optional[str]): the compression codec to use. Allowed values include "UNCOMPRESSED",
+            "SNAPPY", "GZIP", "LZO", "LZ4", "LZ4_RAW", "ZSTD", etc. If not specified, defaults to "SNAPPY".
+        max_dictionary_keys (Optional[int]): the maximum number of unique keys the writer should add to a dictionary page
+            before switching to non-dictionary encoding; never evaluated for non-String columns , defaults to 2^20 (1,048,576)
+        max_dictionary_size (Optional[int]): the maximum number of bytes the writer should add to the dictionary before
+            switching to non-dictionary encoding, never evaluated for non-String columns, defaults to 2^20 (1,048,576)
+        target_page_size (Optional[int]): the target page size in bytes, if not specified, defaults to 2^20 bytes (1 MiB)
+        base_name (Optional[str]): The base name for the individual partitioned tables, if not specified, defaults to
+            `{uuid}`, so files will have names of the format `<uuid>.parquet` where `uuid` is a randomly generated UUID.
+            Users can provide the following tokens in the base_name:
+            - The token `{uuid}` will be replaced with a random UUID. For example, a base name of
+            "table-{uuid}" will result in files named like "table-8e8ab6b2-62f2-40d1-8191-1c5b70c5f330.parquet.parquet".
+            - The token `{partitions}` will be replaced with an underscore-delimited, concatenated string of
+            partition values. For example, for a base name of "{partitions}-table" and partitioning columns "PC1" and
+            "PC2", the file name is generated by concatenating the partition values "PC1=pc1" and "PC2=pc2"
+            with an underscore followed by "-table.parquet", like "PC1=pc1_PC2=pc2-table.parquet".
+            - The token `{i}` will be replaced with an automatically incremented integer for files in a directory. For
+            example, a base name of "table-{i}" will result in files named like "PC=partition1/table-0.parquet",
+            "PC=partition1/table-1.parquet", etc.
+        generate_metadata_files (Optional[bool]): whether to generate parquet _metadata and _common_metadata files,
+            defaults to False. Generating these files can help speed up reading of partitioned parquet data because these
+            files contain metadata (including schema) about the entire dataset, which can be used to skip reading some
+            files.
+        index_columns (Optional[Sequence[Sequence[str]]]): sequence of sequence containing the column names for indexes
+            to persist. The write operation will store the index info for the provided columns as sidecar tables. For
+            example, if the input is [["Col1"], ["Col1", "Col2"]], the write operation will store the index info for
+            ["Col1"] and for ["Col1", "Col2"]. By default, data indexes to write are determined by those present on the
+            source table. This argument can be used to narrow the set of indexes to write, or to be explicit about the
+            expected set of indexes present on all sources. Indexes that are specified but missing will be computed on
+            demand.
 
-        table_definition = None
-        if col_definitions is not None:
-            table_definition = _JTableDefinition.of([col.j_column_definition for col in col_definitions])
-
-        if table_definition:
-            if write_instructions:
-                _JParquetTools.writeTable(table.j_table, path, table_definition, write_instructions)
-            else:
-                _JParquetTools.writeTable(table.j_table, _JFile(path), table_definition)
-        else:
-            if write_instructions:
-                _JParquetTools.writeTable(table.j_table, _JFile(path), write_instructions)
-            else:
-                _JParquetTools.writeTable(table.j_table, path)
+    Raises:
+        DHError
+    """
+    if col_definitions is not None:
+        warn("col_definitions is deprecated and will be removed in a future release. Use table_definition "
+             "instead.", DeprecationWarning, stacklevel=2)
+    try:
+        write_instructions = _build_parquet_instructions(
+            col_instructions=col_instructions,
+            compression_codec_name=compression_codec_name,
+            max_dictionary_keys=max_dictionary_keys,
+            max_dictionary_size=max_dictionary_size,
+            target_page_size=target_page_size,
+            for_read=False,
+            generate_metadata_files=generate_metadata_files,
+            base_name=base_name,
+            table_definition=table_definition,
+            col_definitions=col_definitions,
+            index_columns=index_columns,
+        )
+        _JParquetTools.writeKeyValuePartitionedTable(table.j_object, destination_dir, write_instructions)
     except Exception as e:
         raise DHError(e, "failed to write to parquet data.") from e
 
 
 def batch_write(
     tables: List[Table],
     paths: List[str],
-    col_definitions: List[Column],
+    table_definition: Optional[Union[Dict[str, DType], List[Column]]] = None,
+    col_definitions: Optional[List[Column]] = None,
     col_instructions: Optional[List[ColumnInstruction]] = None,
     compression_codec_name: Optional[str] = None,
     max_dictionary_keys: Optional[int] = None,
     max_dictionary_size: Optional[int] = None,
     target_page_size: Optional[int] = None,
-    grouping_cols: Optional[List[str]] = None,
+    generate_metadata_files: Optional[bool] = None,
+    index_columns: Optional[Sequence[Sequence[str]]] = None
 ):
     """ Writes tables to disk in parquet format to a supplied set of paths.
 
-    If you specify grouping columns, there must already be grouping information for those columns in the sources.
-    This can be accomplished with .groupBy(<grouping columns>).ungroup() or .sort(<grouping column>).
-
     Note that either all the tables are written out successfully or none is.
 
     Args:
         tables (List[Table]): the source tables
-        paths (List[str]): the destinations paths. Any non existing directories in the paths provided are
+        paths (List[str]): the destination paths. Any non-existing directories in the paths provided are
             created. If there is an error, any intermediate directories previously created are removed; note this makes
             this method unsafe for concurrent use
-        col_definitions (List[Column]): the column definitions to use
+        table_definition (Optional[Union[Dict[str, DType], List[Column]]): the table definition to use for writing,
+            instead of the definitions implied by the table. Default is None, which means use the column definitions
+            implied by the table. This definition can be used to skip some columns or add additional columns with
+            null values. Both table_definition and col_definitions cannot be specified at the same time.
+        col_definitions (List[Column]): the column definitions to use for writing. This argument is deprecated and will
+            be removed in a future release. Use table_definition instead.
         col_instructions (Optional[List[ColumnInstruction]]): instructions for customizations while writing
-        compression_codec_name (Optional[str]): the compression codec to use, if not specified, defaults to SNAPPY
-        max_dictionary_keys (Optional[int]): the maximum dictionary keys allowed, if not specified, defaults to 2^20 (1,048,576)
-        max_dictionary_size (Optional[int]): the maximum dictionary size (in bytes) allowed, defaults to 2^20 (1,048,576)
+        compression_codec_name (Optional[str]): the compression codec to use. Allowed values include "UNCOMPRESSED",
+            "SNAPPY", "GZIP", "LZO", "LZ4", "LZ4_RAW", "ZSTD", etc. If not specified, defaults to "SNAPPY".
+        max_dictionary_keys (Optional[int]): the maximum number of unique keys the writer should add to a dictionary page
+            before switching to non-dictionary encoding; never evaluated for non-String columns , defaults to 2^20 (1,048,576)
+        max_dictionary_size (Optional[int]): the maximum number of bytes the writer should add to the dictionary before
+            switching to non-dictionary encoding, never evaluated for non-String columns, defaults to 2^20 (1,048,576)
         target_page_size (Optional[int]): the target page size in bytes, if not specified, defaults to 2^20 bytes (1 MiB)
-        grouping_cols (Optional[List[str]]): the group column names
+        generate_metadata_files (Optional[bool]): whether to generate parquet _metadata and _common_metadata files,
+            defaults to False. Generating these files can help speed up reading of partitioned parquet data because these
+            files contain metadata (including schema) about the entire dataset, which can be used to skip reading some
+            files.
+        index_columns (Optional[Sequence[Sequence[str]]]): sequence of sequence containing the column names for indexes
+            to persist. The write operation will store the index info for the provided columns as sidecar tables. For
+            example, if the input is [["Col1"], ["Col1", "Col2"]], the write operation will store the index info for
+            ["Col1"] and for ["Col1", "Col2"]. By default, data indexes to write are determined by those present on the
+            source table. This argument can be used to narrow the set of indexes to write, or to be explicit about the
+            expected set of indexes present on all sources. Indexes that are specified but missing will be computed on
+            demand.
 
     Raises:
         DHError
     """
+    if col_definitions is not None:
+        warn("col_definitions is deprecated and will be removed in a future release. Use table_definition "
+             "instead.", DeprecationWarning, stacklevel=2)
+        #TODO(deephaven-core#5362): Remove col_definitions parameter
+    elif table_definition is None:
+        raise ValueError("Either table_definition or col_definitions must be specified.")
     try:
         write_instructions = _build_parquet_instructions(
             col_instructions=col_instructions,
             compression_codec_name=compression_codec_name,
             max_dictionary_keys=max_dictionary_keys,
             max_dictionary_size=max_dictionary_size,
             target_page_size=target_page_size,
             for_read=False,
+            generate_metadata_files=generate_metadata_files,
+            table_definition=table_definition,
+            col_definitions=col_definitions,
+            index_columns=index_columns,
         )
-
-        table_definition = _JTableDefinition.of([col.j_column_definition for col in col_definitions])
-
-        if grouping_cols:
-            _JParquetTools.writeParquetTables([t.j_table for t in tables], table_definition, write_instructions,
-                                              _j_file_array(paths), grouping_cols)
-        else:
-            _JParquetTools.writeTables([t.j_table for t in tables], table_definition,
-                                       _j_file_array(paths))
+        _JParquetTools.writeTables([t.j_table for t in tables], _j_string_array(paths), write_instructions)
     except Exception as e:
         raise DHError(e, "write multiple tables to parquet data failed.") from e
```

## deephaven/perfmon.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ Tools to obtain internal, Deephaven logs as tables, and tools to analyze the performance of the Deephaven
 system and Deephaven queries.
 """
 from typing import Dict
```

## deephaven/query_library.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """This module allows users to import Java classes or packages into the query library for the Deephaven query engine.
 These classes or packages can then be used in Deephaven queries. """
 from typing import List
 
 import jpy
```

## deephaven/replay.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module provides support for replaying historical data. """
 
 from typing import Union
 import jpy
```

## deephaven/table.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module implements the Table, PartitionedTable and PartitionedTableProxy classes which are the main
 instruments for working with Deephaven refreshing and static data. """
 
 from __future__ import annotations
```

## deephaven/table_factory.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module provides various ways to make a Deephaven table. """
 
 import datetime
 from typing import Callable, List, Dict, Any, Union, Sequence, Tuple, Mapping
 
@@ -19,21 +19,21 @@
 from deephaven.jcompat import j_lambda, j_list_to_list, to_sequence
 from deephaven.table import Table
 from deephaven.update_graph import auto_locking_ctx
 
 _JTableFactory = jpy.get_type("io.deephaven.engine.table.TableFactory")
 _JTableTools = jpy.get_type("io.deephaven.engine.util.TableTools")
 _JDynamicTableWriter = jpy.get_type("io.deephaven.engine.table.impl.util.DynamicTableWriter")
-_JBaseArrayBackedInputTable = jpy.get_type("io.deephaven.engine.table.impl.util.BaseArrayBackedInputTable")
 _JAppendOnlyArrayBackedInputTable = jpy.get_type(
     "io.deephaven.engine.table.impl.util.AppendOnlyArrayBackedInputTable")
 _JKeyedArrayBackedInputTable = jpy.get_type("io.deephaven.engine.table.impl.util.KeyedArrayBackedInputTable")
 _JTableDefinition = jpy.get_type("io.deephaven.engine.table.TableDefinition")
 _JTable = jpy.get_type("io.deephaven.engine.table.Table")
 _J_INPUT_TABLE_ATTRIBUTE = _JTable.INPUT_TABLE_ATTRIBUTE
+_J_InputTableUpdater = jpy.get_type("io.deephaven.engine.util.input.InputTableUpdater")
 _JRingTableTools = jpy.get_type("io.deephaven.engine.table.impl.sources.ring.RingTableTools")
 _JSupplier = jpy.get_type('java.util.function.Supplier')
 _JFunctionGeneratedTableFactory = jpy.get_type("io.deephaven.engine.table.impl.util.FunctionGeneratedTableFactory")
 
 
 def empty_table(size: int) -> Table:
     """Creates a table with rows but no columns.
@@ -231,21 +231,22 @@
 
 
 class InputTable(Table):
     """InputTable is a subclass of Table that allows the users to dynamically add/delete/modify data in it.
 
     Users should always create InputTables through factory methods rather than directly from the constructor.
     """
-    j_object_type = _JBaseArrayBackedInputTable
 
     def __init__(self, j_table: jpy.JType):
         super().__init__(j_table)
         self.j_input_table = self.j_table.getAttribute(_J_INPUT_TABLE_ATTRIBUTE)
         if not self.j_input_table:
             raise DHError("the provided table input is not suitable for input tables.")
+        if not _J_InputTableUpdater.jclass.isInstance(self.j_input_table):
+            raise DHError("the provided table's InputTable attribute type is not of InputTableUpdater type.")
 
     def add(self, table: Table) -> None:
         """Synchronously writes rows from the provided table to this input table. If this is a keyed input table, added rows with keys
         that match existing rows will replace those rows.
 
         Args:
             table (Table): the table that provides the rows to write
```

## deephaven/table_listener.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """ This module provides utilities for listening to table changes. """
 
 from abc import ABC, abstractmethod
 from functools import wraps
 from inspect import signature
 from typing import Callable, Union, List, Generator, Dict, Optional, Literal
@@ -12,15 +12,15 @@
 import numpy
 
 from deephaven import DHError
 from deephaven import update_graph
 from deephaven._wrapper import JObjectWrapper
 from deephaven.column import Column
 from deephaven.jcompat import to_sequence
-from deephaven.numpy import column_to_numpy_array
+from deephaven.numpy import _column_to_numpy_array
 from deephaven.table import Table
 from deephaven.update_graph import UpdateGraph
 
 _JPythonReplayListenerAdapter = jpy.get_type("io.deephaven.integrations.python.PythonReplayListenerAdapter")
 _JTableUpdate = jpy.get_type("io.deephaven.engine.table.TableUpdate")
 _JTableUpdateDataReader = jpy.get_type("io.deephaven.integrations.python.PythonListenerTableUpdateDataReader")
 
@@ -47,15 +47,15 @@
         while row_sequence_iterator.hasMore():
             chunk_row_set = row_sequence_iterator.getNextRowSequenceWithLength(chunk_size)
 
             j_array = _JTableUpdateDataReader.readChunkColumnMajor(j_reader_context, chunk_row_set, col_sources, prev)
 
             col_dict = {}
             for i, col_def in enumerate(col_defs):
-                np_array = column_to_numpy_array(col_def, j_array[i])
+                np_array = _column_to_numpy_array(col_def, j_array[i])
                 col_dict[col_def.name] = np_array
 
             yield col_dict
     finally:
         j_reader_context.close()
         row_sequence_iterator.close()
```

## deephaven/time.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2023 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module defines functions for handling Deephaven date/time data. """
 
 import datetime
 import sys
 import pytz
@@ -20,14 +20,15 @@
 _JPythonTimeComponents = jpy.get_type("io.deephaven.integrations.python.PythonTimeComponents")
 _JLocalDate = jpy.get_type("java.time.LocalDate")
 _JLocalTime = jpy.get_type("java.time.LocalTime")
 _JInstant = jpy.get_type("java.time.Instant")
 _JZonedDateTime = jpy.get_type("java.time.ZonedDateTime")
 _JDuration = jpy.get_type("java.time.Duration")
 _JPeriod = jpy.get_type("java.time.Period")
+_JSimpleDateFormat = jpy.get_type("java.text.SimpleDateFormat")
 
 _NANOS_PER_SECOND = 1000000000
 _NANOS_PER_MICRO = 1000
 
 
 # region Clock
 
@@ -181,16 +182,16 @@
 
     # Handle pytz time zones
 
     if isinstance(tzi, pytz.tzinfo.BaseTzInfo):
         return _JDateTimeUtils.parseTimeZone(tzi.zone)
 
     # Handle zoneinfo time zones
-
     if sys.version_info >= (3, 9):
+        # novermin
         import zoneinfo
         if isinstance(tzi, zoneinfo.ZoneInfo):
             return _JDateTimeUtils.parseTimeZone(tzi.key)
 
     # Handle constant UTC offset time zones (datetime.timezone)
 
     if isinstance(tzi, datetime.timezone):
@@ -873,7 +874,34 @@
         raise e
     except TypeError as e:
         raise e
     except Exception as e:
         raise DHError(e) from e
 
 # endregion
+
+# region Utility
+
+def simple_date_format(pattern: str) -> jpy.JType:
+    """ Creates a Java SimpleDateFormat from a date-time format pattern.
+
+    This method is intended for use in Python code when a SimpleDateFormat is needed.
+    It should not be used directly in query strings.
+    The most common use case will use this function to construct a SimpleDateFormat
+    in Python and then use the result in query strings.
+
+    Args:
+        pattern (str): A date-time format pattern string.
+
+    Returns:
+        JObject
+
+    Raises:
+        DHError
+    """
+    try:
+        # Returning a Java object directly to avoid Python/Java boundary crossings in query strings
+        return _JSimpleDateFormat(pattern)
+    except Exception as e:
+        raise DHError(e) from e
+
+# endregion
```

## deephaven/update_graph.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """This module provides access to the Update Graph (UG)'s locks that must be acquired to perform certain
 table operations. When working with refreshing tables, UG locks must be held in order to have a consistent view of
 the data between table operations.
 """
 
 import contextlib
```

## deephaven/updateby.py

```diff
@@ -1,9 +1,9 @@
 #
-#     Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """This module supports building various operations for use with the update-by Table operation."""
 from enum import Enum
 from typing import Union, List
 
 import jpy
```

## deephaven/uri.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ Tools for resolving Uniform Resource Identifiers (URIs) into objects. """
 
 from typing import Any
 
 import jpy
```

## deephaven/dbc/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-#     Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """The dbc package includes the modules and functions for using external databases with Deephaven."""
 from typing import Any, Literal
 
 import deephaven.arrow as dharrow
 from deephaven import DHError
 from deephaven.table import Table
```

## deephaven/dbc/adbc.py

```diff
@@ -1,9 +1,9 @@
 #
-#     Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """This module supports ingesting data from external databases (relational and other types) into Deephaven via the
 Python DB-API 2.0 (PEP 249) and the Apache Arrow Database Connectivity (ADBC) interfaces. (Please refer to
 https://arrow.apache.org/docs/dev/format/ADBC.html for more details on ADBC).
 
 ADBC defines a standard API to fetch data in Arrow format from databases that support Arrow natively as well as
 from databases that only support ODBC/JDBC. By relying on ADBC, Deephaven is able to ingest data efficiently from a
```

## deephaven/dbc/odbc.py

```diff
@@ -1,9 +1,9 @@
 #
-#     Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """This module supports ingesting data from external relational databases into Deephaven via the Python DB-API 2.0
 (PEP 249) and the Open Database Connectivity (ODBC) interfaces by using the Turbodbc module.
 
 Turbodbc is DB-API 2.0 compliant, provides access to relational databases via the ODBC interface and more
 importantly it has optimized, built-in Apache Arrow support when fetching ODBC result sets. This enables Deephaven to
 achieve maximum efficiency when ingesting relational data. """
```

## deephaven/experimental/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This package is a place for Deephaven experimental features. """
 
 import jpy
 from deephaven import DHError
 from deephaven.table import Table
```

## deephaven/experimental/ema.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """This modules allows users to define exponential moving averages that can be used in the Deephaven query language."""
 import numpy
 import jpy
 
 _JAbstractMAType = jpy.get_type('io.deephaven.numerics.movingaverages.AbstractMa$Type')
```

## deephaven/experimental/outer_joins.py

```diff
@@ -1,9 +1,9 @@
 #
-#     Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 #
 #
 """This module allows users to perform SQL-style left outer and full outer joins on tables."""
 
 from typing import Union, Sequence
```

## deephaven/experimental/s3.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2023 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 import datetime
 from typing import Optional, Union
 
 import jpy
 import numpy as np
 import pandas as pd
```

## deephaven/experimental/sql.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2023 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 import contextlib
 import inspect
 import jpy
 from typing import Dict, Union, Mapping, Optional, Any
 
 from deephaven import DHError
```

## deephaven/learn/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ Deephaven's learn module provides utilities for efficient data transfer between Deephaven tables and Python objects,
 as well as a framework for using popular machine-learning / deep-learning libraries with Deephaven tables.
 """
 
 from typing import List, Union, Callable, Type
```

## deephaven/learn/gather.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ Utilities for gathering Deephaven table data into Python objects """
 
 import enum
 from typing import Any, Type
```

## deephaven/pandasplugin/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 from deephaven.plugin import Registration, Callback
 from . import pandas_as_table
 
 
 class PandasPluginRegistration(Registration):
```

## deephaven/pandasplugin/pandas_as_table.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 from deephaven.pandas import to_table
 from deephaven.plugin.object_type import Exporter, FetchOnlyObjectType
 from pandas import DataFrame
 
 NAME = "pandas.DataFrame"
```

## deephaven/plot/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ The plot package includes all the modules for creating plots. """
 
 from .axisformat import AxisFormat, DecimalAxisFormat, NanosAxisFormat
 from .axistransform import AxisTransform, axis_transform_names, axis_transform
 from .color import Color, Colors
```

## deephaven/plot/axisformat.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module implements the AxisFormat class that can be applied to format axis tick labels on a plot. """
 
 import jpy
 from deephaven.time import TimeZone
```

## deephaven/plot/axistransform.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module implements the AxisTransform class for performing axis transformation on plots before rendering. """
 
 from typing import List
 
 import jpy
```

## deephaven/plot/color.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module implements the Color class and provides a list of predefined colors that can be used to paint a plot.
 """
 
 from __future__ import annotations
```

## deephaven/plot/figure.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 ######################################################################################################################
 #               This code is auto generated. DO NOT EDIT FILE!
 # Run generatePythonFigureWrapper or "./gradlew :Generators:generatePythonFigureWrapper" to generate
 ######################################################################################################################
 """ This module implements the Figure class for creating plots, charts, line, axis, color, etc. """
```

## deephaven/plot/font.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module implements the Font class that can be used to set the fonts on a plot. """
 
 from enum import Enum
 
 import jpy
```

## deephaven/plot/linestyle.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ The module implements the LineStyle class that can be used to define the line style of a plot. """
 
 from enum import Enum
 from numbers import Number
 from typing import List
```

## deephaven/plot/plotstyle.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module provides support for different plot styles (e.g. line, bar, area, etc.). """
 
 from enum import Enum
 
 import jpy
```

## deephaven/plot/selectable_dataset.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module defines the SelectableDateSet which is used to provides a view of a selectable subset of a table.
 For example, in some selectable data sets, a GUI click can be used to select a portion of a table. """
 
 from typing import List
```

## deephaven/plot/shape.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module defines the Shape enum for all supported shapes that are used to paint points on a plot. """
 
 from enum import Enum
 
 import jpy
```

## deephaven/server/__init__.py

```diff
@@ -1,3 +1,5 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
+
+#
```

## deephaven/server/executors.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2023 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """
 Support for running operations on JVM server threads, so that they can be given work from python. Initially, there
 are two executors, "serial" and "concurrent". Any task that will take an exclusive UGP lock should use the serial
 executor, otherwise the concurrent executor should be used. In the future there may be a "fast" executor, for use
 when there is no chance of using either lock.
 """
```

## deephaven/stream/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ Utility module for the stream subpackage. """
 from warnings import warn
 
 import jpy
```

## deephaven/stream/table_publisher.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2023 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 """The table_publisher module supports publishing Deephaven Tables into blink Tables."""
 
 import jpy
 
 from typing import Callable, Dict, Optional, Tuple
```

## deephaven/stream/kafka/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ The kafka module provides the utilities to both consume and produce Kakfa streams. """
 
 from typing import Dict, List
 
 import jpy
```

## deephaven/stream/kafka/cdc.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module provides Change Data Capture(CDC) support for streaming relational database changes into Deephaven
 tables. """
 
 from typing import Dict, List
```

## deephaven/stream/kafka/consumer.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ The kafka.consumer module supports consuming a Kakfa topic as a Deephaven live table. """
 import jpy
 from typing import Dict, Tuple, List, Callable, Union, Optional
 from warnings import warn
```

## deephaven/stream/kafka/producer.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ The kafka.producer module supports publishing Deephaven tables to Kafka streams. """
 from typing import Dict, Callable, List, Optional
 
 import jpy
```

## deephaven_internal/__init__.py

```diff
@@ -0,0 +1,5 @@
+00000000: 230a 2320 436f 7079 7269 6768 7420 2863  #.# Copyright (c
+00000010: 2920 3230 3136 2d32 3032 3420 4465 6570  ) 2016-2024 Deep
+00000020: 6861 7665 6e20 4461 7461 204c 6162 7320  haven Data Labs 
+00000030: 616e 6420 5061 7465 6e74 2050 656e 6469  and Patent Pendi
+00000040: 6e67 0a23 0a0a 23                        ng.#..#
```

## deephaven_internal/java_threads.py

```diff
@@ -1,7 +1,10 @@
+#
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
+#
 import threading
 
 
 def create_thread_entry(thread_name):
     """
     Helper to call from the JVM into python to set up py thread state exactly once per jvm thread, and support debugging
     """
```

## deephaven_internal/stream.py

```diff
@@ -1,7 +1,10 @@
+#
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
+#
 import io
 
 
 class TeeStream(io.TextIOBase):
     """TextIOBase subclass that splits output between a delegate instance and a set of lambdas.
 
     Can delegate some calls to the provided stream but actually write only to the lambdas. Useful to adapt any existing
```

## deephaven_internal/auto_completer/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 """ This module allows the user to configure if and how we use jedi to perform autocompletion.
 See https://github.com/davidhalter/jedi for information on jedi.
 
 # To disable autocompletion
 from deephaven_internal.auto_completer import jedi_settings, Mode
```

## deephaven_internal/auto_completer/_completer.py

```diff
@@ -1,8 +1,10 @@
-# only python 3.8 needs this, but it must be the first expression in the file, so we can't predicate it
+#
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
+#
 from __future__ import annotations
 from enum import Enum
 from typing import Any, Union, List
 from jedi import Interpreter, Script
 from jedi.api.classes import Completion, Signature
```

## deephaven_internal/jvm/__init__.py

```diff
@@ -1,7 +1,10 @@
+#
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
+#
 _is_ready = False
 
 
 def ready():
     """Marks the JVM as ready. Should be called by Deephaven implementation code. In the case of the Java server process,
     this should be called right after Python has been initialized. In the case of the embedded Python server process,
     this should be called right after the JVM has been initialized."""
```

## deephaven_internal/plugin/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 import jpy
 
 JCallbackAdapter = jpy.get_type('io.deephaven.server.plugin.python.CallbackAdapter')
 
 def initialize_all_and_register_into(callback: JCallbackAdapter):
```

## deephaven_internal/plugin/register.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 import jpy
 import deephaven.plugin
 
 from typing import Union, Type
 from deephaven.plugin import Plugin, Registration, Callback
```

## deephaven_internal/plugin/js/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2023 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 import jpy
 import pathlib
 
 from deephaven.plugin.js import JsPlugin
```

## deephaven_internal/plugin/object/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 import jpy
 
 from typing import Optional, List, Any
 from deephaven.plugin.object_type import Exporter, ObjectType, Reference, MessageStream, FetchOnlyObjectType
 from deephaven._wrapper import pythonify, javaify
```

## deephaven_internal/script_session/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
+# Copyright (c) 2016-2024 Deephaven Data Labs and Patent Pending
 #
 
 # Implementation utilities for io.deephaven.engine.util.PythonDeephavenSession
 from jpy import JType
 
 from deephaven import _wrapper
```

## Comparing `deephaven_core-0.33.3.dist-info/METADATA` & `deephaven_core-0.34.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: deephaven-core
-Version: 0.33.3
+Version: 0.34.0
 Summary: Deephaven Engine Python Package
 Home-page: https://deephaven.io/
 Author: Deephaven Data Labs
 Author-email: python@deephaven.io
 License: Deephaven Community License
 Keywords: Deephaven Development
 Classifier: Development Status :: 4 - Beta
@@ -17,15 +17,15 @@
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: Programming Language :: Python :: 3.12
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
-Requires-Dist: jpy >=0.15.0
+Requires-Dist: jpy >=0.16.0
 Requires-Dist: deephaven-plugin >=0.6.0
 Requires-Dist: numpy
 Requires-Dist: pandas >=1.5.0
 Requires-Dist: pyarrow
 Requires-Dist: numba ; python_version < "3.13"
 Provides-Extra: autocomplete
 Requires-Dist: jedi ==0.18.2 ; extra == 'autocomplete'
```

## Comparing `deephaven_core-0.33.3.dist-info/RECORD` & `deephaven_core-0.34.0.dist-info/RECORD`

 * *Files 12% similar despite different names*

```diff
@@ -1,80 +1,82 @@
-deephaven/__init__.py,sha256=BvueS7UvOv39baPRpDMuCqtP9jFDH61G74z_BpjwXPs,1475
-deephaven/_dep.py,sha256=mZZ3aQdtBSTvVLREOgzfvIEMQkvrwuQCabYxEOZBO58,615
-deephaven/_gc.py,sha256=Ej3zzkhgOEdT6lqj7LIzDH8eLgybW68716zpHjeNSgQ,970
-deephaven/_jpy.py,sha256=c9NebwicYG9DT_6qd_ClMR5OXfuTRj9SJnVhJx-pu94,641
-deephaven/_udf.py,sha256=Vp-Z-5p0gjFVLUcqlMTLZ9pfJB39k61AUBXAzhvpY1c,21107
-deephaven/_wrapper.py,sha256=ud_hyzaR8Ijsby2PhKDc832BpR8LYuJTClsIzaANx_4,6578
-deephaven/agg.py,sha256=1C1vCVdkkZ_RXt2qvVXw_dtev5VgaPiiGHZq7dsfyQw,15143
-deephaven/appmode.py,sha256=xTOuvxsdBly-9XzY3P-tCRaCQQinYdaiHe-do-vDuNw,1977
-deephaven/arrow.py,sha256=KPjwO1LRcBCVGVgvcUUZQa14J0YFjG7dhiy4skP_lc0,5152
-deephaven/calendar.py,sha256=6yllYMWw7afb2ypVSw-PEsv9yFNPjV7Oaq71qBV_MBs,3635
-deephaven/column.py,sha256=GtshN_JmlHVagld_a3KIEjwlc-cO9gTsL21Xdrr8xgk,7658
-deephaven/constants.py,sha256=rhvNKqZoG0iwmjVdY0GPvycj1-c1pcSiXT0umzPahVo,3123
-deephaven/csv.py,sha256=6rV0ddFJiL0rL482igOEuz2r5N3d8mXV0yoYRAgNI9o,4946
-deephaven/dherror.py,sha256=QTHto3BTLGzThFsB2yaUvLrFG9b9slwM3DAdimmQm6A,2890
-deephaven/dtypes.py,sha256=ozz4Xej8UIKpv4N3MkEAxkyWJgmyxOh5NraOXBu0BIg,17171
-deephaven/execution_context.py,sha256=2VKpxCSD7Llg6pS7EEYMVPAM0p4SnjIWwd7ZplYn8pU,3710
-deephaven/filters.py,sha256=U2NfZ-EmYa1FcG-AMB_lhk26jqdF70a4xnNhz3fgpLI,5153
-deephaven/html.py,sha256=zJM2Qa7mwa-bmsRq91mJgdQ63nZ8zYbT6TgBmJPZx7g,650
-deephaven/jcompat.py,sha256=jGonosUCWf5lgFCcr-_3QShhNAHKpHR7j0KsdGFtdQA,12067
-deephaven/liveness_scope.py,sha256=aepxZYG0aaA8h9-ggBi3tBAWo0Q2Kv0zpmthGpUgnh4,8988
-deephaven/numpy.py,sha256=m9zHa04muk-fxmLUBrnQpVBHcyvOegL45hcD7wodXpo,7271
-deephaven/pandas.py,sha256=W9a61LH-35uVxY70qtJ2sKXh2vRd-qE6KbGkP9iSBdU,10291
-deephaven/parquet.py,sha256=nxFbLblIuP0ON6mO3E6VtyH3aMRvb1rVXS_e6B2ap7k,14629
-deephaven/perfmon.py,sha256=27kVhps1bXMjT76CFOQBrI3hGfzdsnfpUVtyTXGLZuw,9429
+deephaven/__init__.py,sha256=7R4lzqC3eRCHGxczo2CWhg_dWlAPMByWjRzNqPIMUZA,1475
+deephaven/_dep.py,sha256=mUh4foCSKZ8X9VFhHFaqzxDs7auDSuquKI3J-_0A3wQ,611
+deephaven/_gc.py,sha256=GpX8sLO2BQP3eopF3TrUaXOJ3-FZczHDNHHRzMSIQgI,966
+deephaven/_jpy.py,sha256=PYA33NhF2C8d5jFd4SSK2JHgA1u1EcslW7Eb1HMASmo,641
+deephaven/_udf.py,sha256=iafJzYlx5sQTT5vu_0xIc-9OJ9f4iKackkPyantu2c8,27944
+deephaven/_wrapper.py,sha256=gEeisjvjrkm-ZZk6-BzQKBJ9BPw_8hTl7fOr3-N2Qlk,7985
+deephaven/agg.py,sha256=_uhjvY4xapwWoGqP2ZgADVnkNzpqqF0NjvP02kbMkKc,15143
+deephaven/appmode.py,sha256=arK-jGyXXp-gunPxVoaUD9AJ0UFkk6TALHMiquCr07Y,1977
+deephaven/arrow.py,sha256=pFixSfkSreqDy9j5AoGLGWgmNPsRVqIhK-yc0yRKn9w,5148
+deephaven/barrage.py,sha256=DuL5GugPIiK9UX7hzGvta2WU8BJH32Wt76t3-jDeReE,10290
+deephaven/calendar.py,sha256=iJ-5l0bRPMrPdWPTA07NdcifISukkE1uBCYTWIYW4Lo,3635
+deephaven/column.py,sha256=aOBqQytSNpd5rts_1DE1gfGAKTFvkZizaZolr6ELxeE,7580
+deephaven/constants.py,sha256=GEVfpQixJirO1qmuCCifaWrDUIKyfABCe2bECIFKkpg,3123
+deephaven/csv.py,sha256=fSWK4sexxhYSoQTg9nWI17QdJ-_BzQwcH29AKqoYrJk,4946
+deephaven/dherror.py,sha256=bTpaUgNa91JIvtjHolxkHSGGcTU8yrYg9N2StZWKpqg,2890
+deephaven/dtypes.py,sha256=9gjLYpR7eHHjLGqFFiFXihUzuVuW_nIIJNrsY72pApE,14798
+deephaven/execution_context.py,sha256=cu-WLluicP_Igtfwwwf6R1jqVj7hKYMIgm8F34EG8o4,3706
+deephaven/filters.py,sha256=_QVeJ84lag6YHXYlvltLeFgdWiL01IvMI__AmWBw2NY,5153
+deephaven/html.py,sha256=Wm1f5D0QfKf7j-OIaFMoOn2CKbo_2Mete8F3cuftbXQ,650
+deephaven/jcompat.py,sha256=8pnuBOG-ZHSvW67rfn0MbhQdLTg9xnjjMaS1codg99M,13296
+deephaven/liveness_scope.py,sha256=j3A2eZ3Oq_zVKI0cKx6bjolAEDatL3k_YVlT8NRnHRc,8963
+deephaven/numpy.py,sha256=qRlsBQBjE2ElYeLZa_tByC-UBet_IQZpp1GQl8msokY,7443
+deephaven/pandas.py,sha256=tPBftb_-55tS6hkZInOf22gudiEUANFGAKjZ77w4Fs0,10292
+deephaven/parquet.py,sha256=ShS2JAogRMxNCCADt-KTrv0c4p5tIPgl4Onxoakn5Vc,26505
+deephaven/perfmon.py,sha256=U1YOB4-8Lp_ig1f9q92uBD-nzM1JMK6dusHscYT8qSI,9429
 deephaven/py.typed,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-deephaven/query_library.py,sha256=8z_SsylV4ZbANjiWCVqDgEOnjXHyKj9xlHfz3FMPT5Q,2690
-deephaven/replay.py,sha256=DAAlGv9xTVwcPtX-XC_aNlR_Jml4Uo-nDxoNTIclm1Y,3075
-deephaven/table.py,sha256=6dQu2M0ppJJGWu1gOnUJ4Z2ILTz-oaq1vxmP8h-Na6g,163251
-deephaven/table_factory.py,sha256=qR3_uajr0ALiQU26Ab_WUzbixQnAprvZegIssblLZhU,17732
-deephaven/table_listener.py,sha256=H6ZjzInqvf39a2wXrIOf6LJ4mvzZ5wgvJ5rT0ySLd3M,16346
-deephaven/time.py,sha256=GLaDkiqaPiPPG2loWVOQWYALqaMMFjx9tXwsdMzoi7E,30792
-deephaven/update_graph.py,sha256=QKyQfKQVRyI2_CQYq6NzA_UE4l2epm2t1RcVLhrfb_Q,9071
-deephaven/updateby.py,sha256=HeXp1Y9pwPUjlywI5gNY-0cW0TePsi5caVWAntkSMdU,79447
-deephaven/uri.py,sha256=lUD5LT_cIcKAkOVJd9bBmjVhxzB9OVJJowqIvBMPWDk,1155
-deephaven/dbc/__init__.py,sha256=_u-yOnsf0AKxk3X4OSc2NDwtLS1kdDPvViMJnxrsnbQ,3243
-deephaven/dbc/adbc.py,sha256=bnl0nS-fgpaS1ZV8OLYph7WaqRs8uFv9GiE861sW9TA,1794
-deephaven/dbc/odbc.py,sha256=Sm54IOlEucnZk8N7XlOizjrKIJ1NHxmMbEsrJwv_V8U,1668
-deephaven/experimental/__init__.py,sha256=dJEBTCuRgxLJx45AEBpju6YkE9ShLAiNPJkDiTtEDeY,1316
-deephaven/experimental/ema.py,sha256=Ncdf1C4tbrI_Z8o1f9UZ0Lw_GFsH2HZDxR8XA8Y3sfI,4815
-deephaven/experimental/outer_joins.py,sha256=Na3MOL4wixf16WseYkEA6skGpDd2p6jWnbwCBB4FSRs,4106
-deephaven/experimental/s3.py,sha256=PH5rfYLCyz4VconXzVljks_3OTz3QsSAuSUYDJU5w0k,6654
-deephaven/experimental/sql.py,sha256=YvtsYvAorCJIr3NThuCzOPVAMLhogW41RfDLBDEoGss,1570
-deephaven/learn/__init__.py,sha256=l7_ccdLJ8WH2GGFMLj03PfzlvMTAATUpmLAY_YMUW5A,7534
-deephaven/learn/gather.py,sha256=jqCHN1yITCV3ximqXJDbQ-GhDxfOpCVNgDrse6vpIK8,3120
-deephaven/pandasplugin/__init__.py,sha256=0YtEAIMGh2A_6eLWpaj6Q3u6NftkGyPAaoNEWaf7gDU,345
-deephaven/pandasplugin/pandas_as_table.py,sha256=iUJzKlFpjwetvjZHHHFDdZxwIWlJcfILFaL92zvu0Ug,573
-deephaven/plot/__init__.py,sha256=G2pf1xnMUf1hDAd3cFXzRBqgNNvjoCmGUyQdqAiK9Bs,574
-deephaven/plot/axisformat.py,sha256=mNQi6_XgEmLL2nSlN6BRitH-KTFsSCp3Q4OXJehA9lo,2289
-deephaven/plot/axistransform.py,sha256=8LaLfpqeQ5asjzaVY6ufhOv7GgkDorM96e2DRwoxmF0,1479
-deephaven/plot/color.py,sha256=g9xb4ZL1kast2ZXaizK-Aqbhi4z2MextbXyZvSaEivs,16540
-deephaven/plot/figure.py,sha256=wJszNRxEW1dY-qthYjMO2k8QzEIUliX2LjFEIc9QQjk,110210
-deephaven/plot/font.py,sha256=DE8itMivEpbZGZvlXZjWLp6qFnvkRe8cKtNSMsKA3BQ,1659
-deephaven/plot/linestyle.py,sha256=Yv9uGlFBImyDb8S8qYX5sFl-Wb_aLfagiD5QGNlv28A,3515
-deephaven/plot/plotstyle.py,sha256=pFGqJ7jkg4pTEUfDbhCCtURJKkF94IPjYZzLnnm855E,1285
-deephaven/plot/selectable_dataset.py,sha256=EzErxDkqnavztdEghUfWXZageMicU4STF-OIsYXWBzk,2493
-deephaven/plot/shape.py,sha256=gBlrOAcRHDAF8975y1Jk2VsGRzzTB-HnbuZoVGgZoj0,1213
-deephaven/server/__init__.py,sha256=VOFbDQVc-WpwwDV_DWFLADmZu1ke7ShFxeNz1Rj-mmA,69
-deephaven/server/executors.py,sha256=8RcXX21gmzw9JW5i_XetLjDdctuM9axtbvDfP_IJ4JE,2233
-deephaven/stream/__init__.py,sha256=CNCh5qOzS14XN5tmSNwpeQorNmdi5Poy_fkTO05Iahc,1069
-deephaven/stream/table_publisher.py,sha256=CI1ZY_n2aA1fvguQxf3caLgRR80xZtgu2lAXWok-aA0,5640
-deephaven/stream/kafka/__init__.py,sha256=i9DTYDIPyoDUSe_kE1KzjC7CvjRq3e264gTOX37Cf0I,1019
-deephaven/stream/kafka/cdc.py,sha256=HLFAI0BaaeW6spFN3JWnzSLQKVTs6iZloyNiGvWDQ1c,7084
-deephaven/stream/kafka/consumer.py,sha256=9lFEPZcWJRMMLMou8Kn2JQrOBl6eAhB3gz00VPI6eJU,23648
-deephaven/stream/kafka/producer.py,sha256=rP7VGdVQ6B7YgR3pVMJVb5htBxyb8qxXaG7GDE2VVUc,13972
-deephaven_internal/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-deephaven_internal/java_threads.py,sha256=xuCtrDHJNzPkbKoHBTAj7j-lAWNlaQJqleQ-LdP4aT8,1576
-deephaven_internal/stream.py,sha256=jDOU8B9I1a6CfUTzYXeLn_R6lrzTYlgNUI5bB60pQ64,3593
-deephaven_internal/auto_completer/__init__.py,sha256=2zPuxVzSsw5ZGpOFrvU3UY59s7qC13vCNTOWZoNKqiA,952
-deephaven_internal/auto_completer/_completer.py,sha256=pfUj0TOa8GkPssf9nhwK0r19kDtXdL_Rd1alrdqvB-M,8476
-deephaven_internal/jvm/__init__.py,sha256=KBCDz1h9pv-r57_5NYhRaLy777WbZ6sOmxTmcHpBgdw,3422
-deephaven_internal/plugin/__init__.py,sha256=fG6Mw6kCRcCin0Wz8qsavAjv0KGzRDHcQE0iMFI5nkM,582
-deephaven_internal/plugin/register.py,sha256=dJmLjEtSS2ng6S8THF1F3dlTxV-vtV6c89ZeiGQBDrA,1486
-deephaven_internal/plugin/js/__init__.py,sha256=HbwoCkBo4XWfS0zcrCRGgQJxJEOxGMPzo2HASRktAG0,961
-deephaven_internal/plugin/object/__init__.py,sha256=MPJIWnVlKyRx-b0I9ROJhaYyiE17dIT8x2EWmi5JUA4,3470
-deephaven_internal/script_session/__init__.py,sha256=T13NF_Sv1DZvwLvFMT-dc0TozEiED3f-vSibEly3Qe0,1736
-deephaven_core-0.33.3.dist-info/METADATA,sha256=itOWJ0Of8unAwpewAE9UBGb2GS2YOOP-kiz8BRLoz6U,2773
-deephaven_core-0.33.3.dist-info/WHEEL,sha256=Xo9-1PvkuimrydujYJAjF7pCkriuXBpUPEjma1nZyJ0,92
-deephaven_core-0.33.3.dist-info/entry_points.txt,sha256=nPbG7R-xmBfHuuX4Tz4G6YJA0ptGi1tC0yM1ijswhaw,86
-deephaven_core-0.33.3.dist-info/top_level.txt,sha256=EshtqOS_YqliZnhyqyyQOCTJARwRdEW_XueIFQ9gJB4,29
-deephaven_core-0.33.3.dist-info/RECORD,,
+deephaven/query_library.py,sha256=65AFFYB1II49B6Z9yUbs76IEGmKSp4fJlo3wBUFJvJI,2690
+deephaven/replay.py,sha256=y5vfUIGDoc-yI_QrS_TMSL7BAFwp1VNkQzaCfkxrYXE,3075
+deephaven/table.py,sha256=OZylGTn-pTUwN5WDfW2HSFpwNjLkhID6cePr40hetbU,163251
+deephaven/table_factory.py,sha256=M3jTKw_uifuBj7ZvqYdD2bSmO4KKySXxTa1Hv1DIGYw,17849
+deephaven/table_listener.py,sha256=pBkNfRI0GZy-vztE5UAooHGoae8EEcQJdCiqA4g1AUU,16348
+deephaven/time.py,sha256=jNzp3PrKef8BAHWsgvRwyVSc3D2ZTrrcCOj4MnQN4ec,31656
+deephaven/update_graph.py,sha256=7A736rbd2PDcXjWSCSZ6YmX2ZU7x-YyyedvfXQ5SldU,9071
+deephaven/updateby.py,sha256=LpIBjDWmSNxJab2hL86WaoHilfqdyELHAIwP7zZord0,79443
+deephaven/uri.py,sha256=5SZZRnNRzxX6kPEZo7uW8vslWL4FJSZgFtIG7iCKfDQ,1155
+deephaven/dbc/__init__.py,sha256=GWiNWx82ff4tmHH4yroAeg_llSLNLabsu4UdA01NqGs,3239
+deephaven/dbc/adbc.py,sha256=Ed_yKKIjAEc9USe3bVqYCLzhBAlY13KzX-s1ru-Rab8,1790
+deephaven/dbc/odbc.py,sha256=J7jl_wY-Wmkjwrm9Gs6qvYs_2X37YKaztsqoKwIPJy4,1664
+deephaven/experimental/__init__.py,sha256=HeoEh8abZJZ1ZDCI_f1nnteRCGcpfi-wQOSnkqI9O8k,1316
+deephaven/experimental/data_index.py,sha256=FWmxC8lJ605Q-xkUZkL4uIFsRb6g3wOhM1pUSRPdQig,3195
+deephaven/experimental/ema.py,sha256=lQ8S32E7NpcQMwB9wn615HWCoLw8U_twpIDZ2dLiGoc,4815
+deephaven/experimental/outer_joins.py,sha256=_iZhXt_7W7kCjnDNjZVwTclnLom9-u4ezYW2iU72SZQ,4102
+deephaven/experimental/s3.py,sha256=8GDiK0XA5Qe2vOok889-LS0Eite-_oCsbAz2gWqKiGU,6654
+deephaven/experimental/sql.py,sha256=XgZZCKsWA8l-hhxvo937Gjya_g6ShrJayQU0sYjrQGA,1570
+deephaven/learn/__init__.py,sha256=EuWArWd-k85Cg5wWEuTpM7Rgz3vxkp1QIzgfhwKNVvQ,7534
+deephaven/learn/gather.py,sha256=dUGPF38HB75kVExIJxWHGAhI6qSA4KaLj5TJvaKgl9Y,3120
+deephaven/pandasplugin/__init__.py,sha256=QVozmLp7DNpNEZNkMr7ITxLEaqnnVJTTZR0NxpehoNI,345
+deephaven/pandasplugin/pandas_as_table.py,sha256=1ytWm-kZzdj83_UosLgZz39nrbdgIzT2f-tYQvvlWB0,573
+deephaven/plot/__init__.py,sha256=hkuikhIUpuP1MsjTKSieSsPDS6HsN0J0r5UgBwFsHLo,574
+deephaven/plot/axisformat.py,sha256=UZUyfMGTSmySRpy5rMoujTlHx44b1jrTOVl7fO80GBQ,2289
+deephaven/plot/axistransform.py,sha256=4sD1GqIPz367PU_8WQbP49Au-aTAvfT1CeE5sxm7eKw,1479
+deephaven/plot/color.py,sha256=jGEwTMzYJXhdSnXocoreYEtiwAJlM4PH_s3FIOHRq5g,16540
+deephaven/plot/figure.py,sha256=t_dsxJQ2ZRqj7HR2myAaRsYK5sKFTwOdA00uaN_-RL4,110210
+deephaven/plot/font.py,sha256=8JUCPSyO2w3y5X3ePHilf8QUkPL002KY-uRq-ffBooA,1659
+deephaven/plot/linestyle.py,sha256=n6PYQVgzh6tAHh27qWMFzo1P7MLX4w4iH_DjgZsdJkU,3515
+deephaven/plot/plotstyle.py,sha256=rp7CWWnjbE-goVwcikOg_ygymkXYqemWcAkCdnaMZRw,1285
+deephaven/plot/selectable_dataset.py,sha256=3Q5SqwSSUfMAAN-lcSrcScknr6rrMkV4QgaPnpheUVA,2493
+deephaven/plot/shape.py,sha256=AH3C6tc9SwhzRBPCjgaTBZmFl2noquHSqXkal1or6ZI,1213
+deephaven/server/__init__.py,sha256=7fRdw7wVHVaUZzEZ_TyKQ3BVJBibGxNyEmTU7ipHt4Y,71
+deephaven/server/executors.py,sha256=5jjEEA8s25aUhCbcAtWoqzX383cAIimIYi-2DHqQoaw,2233
+deephaven/stream/__init__.py,sha256=lo2vnkHVZ-CrrVuW9IE6ncTThUUfXNLQmtIWgtThDa8,1069
+deephaven/stream/table_publisher.py,sha256=8MJ6Ju1sNHx6kdki8UYdoD9YZTQ72tCWxT1xwjEU3bo,5640
+deephaven/stream/kafka/__init__.py,sha256=pELeKeXpHzLZe8XrDN25flV33bRJylDjQRvzE7NadDc,1019
+deephaven/stream/kafka/cdc.py,sha256=XUqtN-avNBMj0JCm8qkE4qHi7O70kHVWtPES-9dLPxw,7084
+deephaven/stream/kafka/consumer.py,sha256=v8l-p7sTdkVEa3PHP1_pMr_AXUuWd58TcNz9fK4KUog,23648
+deephaven/stream/kafka/producer.py,sha256=SczqjOCVfe09iJ6sll0AuYGMwYzakN-l7OaxmfDIBNc,13972
+deephaven_internal/__init__.py,sha256=7fRdw7wVHVaUZzEZ_TyKQ3BVJBibGxNyEmTU7ipHt4Y,71
+deephaven_internal/java_threads.py,sha256=k7PFcxgCHoCcXukviWQ0ttdk1wqXzi9N6_6tQLlPlks,1645
+deephaven_internal/stream.py,sha256=jRaXPrVhRQbvtWiwXoQ0oRgRxWbvKKjGuLVFiDS7Lco,3662
+deephaven_internal/auto_completer/__init__.py,sha256=ZsgLCXuohUneynusoTXHHUjH1vFCnpjgSnR59nA0L5Y,952
+deephaven_internal/auto_completer/_completer.py,sha256=CDRtEd_tl6ss7i812MQ0I6d1aNTZ2UjJmPO_NxrEcv0,8441
+deephaven_internal/jvm/__init__.py,sha256=Wfzr0Vl4hMW7hwIfhQbYhO_p_qOFxvdJ5PhhKAoyzlo,3491
+deephaven_internal/plugin/__init__.py,sha256=15CavaEOFsayk0zXeHy00JTROi82NlNIu8OWSakEnmo,582
+deephaven_internal/plugin/register.py,sha256=vIfpa-yVrHkvfEZuc8QqwrLtnfgE6sRIiwz9HVDHo3U,1486
+deephaven_internal/plugin/js/__init__.py,sha256=CSnLnwx3cmuFPoc1gnrmmfbC5uFwhepnMYu6cPjl7xc,961
+deephaven_internal/plugin/object/__init__.py,sha256=80dN7kOQYGjR_INmRzTR84wsoHQBjMARE_yi0HNXi5Y,3470
+deephaven_internal/script_session/__init__.py,sha256=ELkusnNVNIKMwDOZXTFEbnYyrXX4sYPRY6bToM6CJys,1736
+deephaven_core-0.34.0.dist-info/METADATA,sha256=QOxbeqEEOBQG6ulas9cM9VjnTgXZb_s8lSFJ1AiC1XQ,2773
+deephaven_core-0.34.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+deephaven_core-0.34.0.dist-info/entry_points.txt,sha256=nPbG7R-xmBfHuuX4Tz4G6YJA0ptGi1tC0yM1ijswhaw,86
+deephaven_core-0.34.0.dist-info/top_level.txt,sha256=EshtqOS_YqliZnhyqyyQOCTJARwRdEW_XueIFQ9gJB4,29
+deephaven_core-0.34.0.dist-info/RECORD,,
```

