# Comparing `tmp/ppasr-2.4.6-py3-none-any.whl.zip` & `tmp/ppasr-2.4.7-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,19 +1,19 @@
-Zip file size: 1648821 bytes, number of entries: 83
--rw-rw-rw-  2.0 fat      134 b- defN 23-Sep-07 11:28 ppasr/__init__.py
+Zip file size: 1648972 bytes, number of entries: 83
+-rw-rw-rw-  2.0 fat      134 b- defN 24-Apr-27 06:48 ppasr/__init__.py
 -rw-rw-rw-  2.0 fat    17895 b- defN 23-Jul-17 11:28 ppasr/predict.py
--rw-rw-rw-  2.0 fat    37501 b- defN 23-Jul-30 02:56 ppasr/trainer.py
+-rw-rw-rw-  2.0 fat    38580 b- defN 24-Apr-21 12:12 ppasr/trainer.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jul-12 12:38 ppasr/data_utils/__init__.py
--rw-rw-rw-  2.0 fat    23922 b- defN 23-Apr-08 01:32 ppasr/data_utils/audio.py
+-rw-rw-rw-  2.0 fat    23844 b- defN 24-Apr-27 05:34 ppasr/data_utils/audio.py
 -rw-rw-rw-  2.0 fat     2377 b- defN 22-Oct-29 02:25 ppasr/data_utils/binary.py
 -rw-rw-rw-  2.0 fat     1584 b- defN 23-Apr-05 02:33 ppasr/data_utils/collate_fn.py
 -rw-rw-rw-  2.0 fat     5192 b- defN 23-Jan-30 11:56 ppasr/data_utils/normalizer.py
 -rw-rw-rw-  2.0 fat     4244 b- defN 23-Jan-30 11:56 ppasr/data_utils/reader.py
 -rw-rw-rw-  2.0 fat     8477 b- defN 22-Aug-05 13:41 ppasr/data_utils/sampler.py
--rw-rw-rw-  2.0 fat    15626 b- defN 23-Mar-29 14:26 ppasr/data_utils/utils.py
+-rw-rw-rw-  2.0 fat    15686 b- defN 23-Dec-08 11:32 ppasr/data_utils/utils.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jul-12 12:38 ppasr/data_utils/augmentor/__init__.py
 -rw-rw-rw-  2.0 fat     5787 b- defN 22-Nov-06 06:19 ppasr/data_utils/augmentor/augmentation.py
 -rw-rw-rw-  2.0 fat      965 b- defN 22-Jul-12 12:38 ppasr/data_utils/augmentor/base.py
 -rw-rw-rw-  2.0 fat     2570 b- defN 23-Apr-08 01:30 ppasr/data_utils/augmentor/noise_perturb.py
 -rw-rw-rw-  2.0 fat      977 b- defN 22-Sep-29 13:43 ppasr/data_utils/augmentor/resample.py
 -rw-rw-rw-  2.0 fat     1007 b- defN 22-Sep-29 13:43 ppasr/data_utils/augmentor/shift_perturb.py
 -rw-rw-rw-  2.0 fat     5015 b- defN 22-Sep-29 13:43 ppasr/data_utils/augmentor/spec_augment.py
@@ -25,15 +25,15 @@
 -rw-rw-rw-  2.0 fat     1984 b- defN 22-Aug-05 13:41 ppasr/data_utils/featurizer/text_featurizer.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jul-12 12:38 ppasr/decoders/__init__.py
 -rw-rw-rw-  2.0 fat     5354 b- defN 23-Feb-22 11:57 ppasr/decoders/beam_search_decoder.py
 -rw-rw-rw-  2.0 fat     3794 b- defN 22-Aug-05 13:41 ppasr/decoders/ctc_greedy_decoder.py
 -rw-rw-rw-  2.0 fat     5403 b- defN 22-Aug-05 13:41 ppasr/decoders/swig_wrapper.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Oct-25 10:31 ppasr/infer_utils/__init__.py
 -rw-rw-rw-  2.0 fat    11174 b- defN 23-Apr-01 04:48 ppasr/infer_utils/inference_predictor.py
--rw-rw-rw-  2.0 fat     4696 b- defN 22-Oct-28 14:36 ppasr/infer_utils/pun_predictor.py
+-rw-rw-rw-  2.0 fat     4696 b- defN 24-Feb-03 15:03 ppasr/infer_utils/pun_predictor.py
 -rw-rw-rw-  2.0 fat  1807522 b- defN 22-Oct-28 14:36 ppasr/infer_utils/silero_vad.onnx
 -rw-rw-rw-  2.0 fat     8614 b- defN 23-Jan-30 11:56 ppasr/infer_utils/vad_predictor.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jul-12 12:38 ppasr/model_utils/__init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Oct-20 12:39 ppasr/model_utils/conformer/__init__.py
 -rw-rw-rw-  2.0 fat    12421 b- defN 23-Apr-01 02:49 ppasr/model_utils/conformer/attention.py
 -rw-rw-rw-  2.0 fat     5829 b- defN 23-Feb-07 12:26 ppasr/model_utils/conformer/convolution.py
 -rw-rw-rw-  2.0 fat     4738 b- defN 22-Nov-15 11:50 ppasr/model_utils/conformer/embedding.py
@@ -73,13 +73,13 @@
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-30 11:56 ppasr/optimizer/__init__.py
 -rw-rw-rw-  2.0 fat    10523 b- defN 23-Jan-30 11:56 ppasr/optimizer/scheduler.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jul-12 12:38 ppasr/utils/__init__.py
 -rw-rw-rw-  2.0 fat     2844 b- defN 23-Jan-30 11:56 ppasr/utils/logger.py
 -rw-rw-rw-  2.0 fat      893 b- defN 22-Jul-12 12:38 ppasr/utils/metrics.py
 -rw-rw-rw-  2.0 fat    13549 b- defN 23-Jan-30 11:56 ppasr/utils/model_summary.py
 -rw-rw-rw-  2.0 fat     3910 b- defN 23-Feb-22 11:57 ppasr/utils/utils.py
--rw-rw-rw-  2.0 fat    11558 b- defN 23-Sep-07 11:28 ppasr-2.4.6.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    11171 b- defN 23-Sep-07 11:28 ppasr-2.4.6.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Sep-07 11:28 ppasr-2.4.6.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        6 b- defN 23-Sep-07 11:28 ppasr-2.4.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     7609 b- defN 23-Sep-07 11:28 ppasr-2.4.6.dist-info/RECORD
-83 files, 2287366 bytes uncompressed, 1636549 bytes compressed:  28.5%
+-rw-rw-rw-  2.0 fat    11558 b- defN 24-Apr-27 06:48 ppasr-2.4.7.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    11142 b- defN 24-Apr-27 06:48 ppasr-2.4.7.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-27 06:48 ppasr-2.4.7.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        6 b- defN 24-Apr-27 06:48 ppasr-2.4.7.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     7609 b- defN 24-Apr-27 06:48 ppasr-2.4.7.dist-info/RECORD
+83 files, 2288398 bytes uncompressed, 1636700 bytes compressed:  28.5%
```

## zipnote {}

```diff
@@ -228,23 +228,23 @@
 
 Filename: ppasr/utils/model_summary.py
 Comment: 
 
 Filename: ppasr/utils/utils.py
 Comment: 
 
-Filename: ppasr-2.4.6.dist-info/LICENSE
+Filename: ppasr-2.4.7.dist-info/LICENSE
 Comment: 
 
-Filename: ppasr-2.4.6.dist-info/METADATA
+Filename: ppasr-2.4.7.dist-info/METADATA
 Comment: 
 
-Filename: ppasr-2.4.6.dist-info/WHEEL
+Filename: ppasr-2.4.7.dist-info/WHEEL
 Comment: 
 
-Filename: ppasr-2.4.6.dist-info/top_level.txt
+Filename: ppasr-2.4.7.dist-info/top_level.txt
 Comment: 
 
-Filename: ppasr-2.4.6.dist-info/RECORD
+Filename: ppasr-2.4.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ppasr/__init__.py

```diff
@@ -1,3 +1,3 @@
-__version__ = "2.4.6"
+__version__ = "2.4.7"
 # 项目支持的模型
 SUPPORT_MODEL = ['squeezeformer', 'efficient_conformer', 'conformer', 'deepspeech2']
```

## ppasr/trainer.py

```diff
@@ -55,14 +55,20 @@
         self.configs = dict_to_object(configs)
         self.local_rank = 0
         self.use_gpu = use_gpu
         assert self.configs.use_model in SUPPORT_MODEL, f'没有该模型：{self.configs.use_model}'
         self.model = None
         self.test_loader = None
         self.beam_search_decoder = None
+        self.max_step, self.train_step = None, None
+        self.train_loss, self.train_eta_sec = None, None
+        self.eval_best_error_rate = None
+        self.eval_loss, self.eval_error_result = None, None
+        self.test_log_step, self.train_log_step = 0, 0
+        self.stop_train, self.stop_eval = False, False
 
     def __setup_dataloader(self, augment_conf_path=None, is_train=False):
         # 获取训练数据
         if augment_conf_path is not None and os.path.exists(augment_conf_path) and is_train:
             augmentation_config = io.open(augment_conf_path, mode='r', encoding='utf8').read()
         else:
             if augment_conf_path is not None and not os.path.exists(augment_conf_path):
@@ -276,32 +282,33 @@
             try:
                 from ppasr.decoders.beam_search_decoder import BeamSearchDecoder
                 self.beam_search_decoder = BeamSearchDecoder(vocab_list=vocabulary,
                                                              **self.configs.ctc_beam_search_decoder_conf)
             except ModuleNotFoundError:
                 logger.warning('==================================================================')
                 logger.warning('缺少 paddlespeech-ctcdecoders 库，请根据文档安装。')
-                logger.warning('python -m pip install paddlespeech_ctcdecoders -U -i https://ppasr.yeyupiaoling.cn/pypi/simple/')
+                logger.warning(
+                    'python -m pip install paddlespeech_ctcdecoders -U -i https://ppasr.yeyupiaoling.cn/pypi/simple/')
                 logger.warning('【注意】现在已自动切换为ctc_greedy解码器，ctc_greedy解码器准确率相对较低。')
                 logger.warning('==================================================================\n')
                 self.configs.decoder = 'ctc_greedy'
 
         # 执行解码
         outs = [outs[i, :, :] for i, _ in enumerate(range(outs.shape[0]))]
         if self.configs.decoder == 'ctc_greedy':
             result = greedy_decoder_batch(outs, vocabulary)
         else:
             result = self.beam_search_decoder.decode_batch_beam_search_offline(probs_split=outs)
         return result
 
     def __train_epoch(self, epoch_id, save_model_path, writer, nranks):
-        train_times, reader_times, batch_times = [], [], []
+        train_times, reader_times, batch_times, loss_sum = [], [], [], []
         start = time.time()
-        sum_batch = len(self.train_loader) * self.configs.train_conf.max_epoch
         for batch_id, batch in enumerate(self.train_loader()):
+            if self.stop_train: break
             inputs, labels, input_lens, label_lens = batch
             reader_times.append((time.time() - start) * 1000)
             start_step = time.time()
             num_utts = label_lens.shape[0]
             if num_utts == 0:
                 continue
             # 执行模型计算，是否开启自动混合精度
@@ -320,51 +327,52 @@
                     scaled = self.amp_scaler.scale(loss)
                     scaled.backward()
                 else:
                     loss.backward()
 
             # 执行一次梯度计算
             if batch_id % self.configs.train_conf.accum_grad == 0:
-                if self.local_rank == 0 and writer is not None:
-                    writer.add_scalar('Train/Loss', float(loss), self.train_step)
-                    # 记录学习率
-                    writer.add_scalar('Train/lr', self.scheduler.get_lr(), self.train_step)
                 # 是否开启自动混合精度
                 if self.configs.train_conf.enable_amp:
                     # 更新参数（参数梯度先除系数loss_scaling再更新参数）
                     self.amp_scaler.step(self.optimizer)
                     # 基于动态loss_scaling策略更新loss_scaling系数
                     self.amp_scaler.update()
                 else:
                     self.optimizer.step()
                 self.optimizer.clear_grad()
                 self.scheduler.step()
                 self.train_step += 1
+            loss_sum.append(float(loss))
+            train_times.append((time.time() - start) * 1000)
             batch_times.append((time.time() - start_step) * 1000)
+            self.train_step += 1
 
             # 多卡训练只使用一个进程打印
-            train_times.append((time.time() - start) * 1000)
             if batch_id % self.configs.train_conf.log_interval == 0:
                 # 计算每秒训练数据量
                 train_speed = self.configs.dataset_conf.batch_size / (sum(train_times) / len(train_times) / 1000)
                 # 计算剩余时间
-                eta_sec = (sum(train_times) / len(train_times)) * (
-                        sum_batch - (epoch_id - 1) * len(self.train_loader) - batch_id)
-                eta_str = str(timedelta(seconds=int(eta_sec / 1000)))
+                self.train_eta_sec = (sum(train_times) / len(train_times)) * (self.max_step - self.train_step) / 1000
+                eta_str = str(timedelta(seconds=int(self.train_eta_sec)))
+                self.train_loss = sum(loss_sum) / len(loss_sum)
                 logger.info(f'Train epoch: [{epoch_id}/{self.configs.train_conf.max_epoch}], '
                             f'batch: [{batch_id}/{len(self.train_loader)}], '
-                            f'loss: {float(loss):.5f}, '
+                            f'loss: {self.train_loss:.5f}, '
                             f'learning_rate: {self.scheduler.get_lr():>.8f}, '
                             f'reader_cost: {(sum(reader_times) / len(reader_times) / 1000):.4f}, '
                             f'batch_cost: {(sum(batch_times) / len(batch_times) / 1000):.4f}, '
                             f'ips: {train_speed:.4f} speech/sec, '
                             f'eta: {eta_str}')
                 if self.local_rank == 0:
-                    writer.add_scalar('Train/Loss', float(loss), self.train_step)
-                train_times = []
+                    # 记录学习率
+                    writer.add_scalar('Train/lr', self.scheduler.get_lr(), self.train_log_step)
+                    writer.add_scalar('Train/Loss', float(loss), self.train_log_step)
+                self.train_log_step += 1
+                train_times, reader_times, batch_times, loss_sum = [], [], [], []
             # 固定步数也要保存一次模型
             if batch_id % 10000 == 0 and batch_id != 0 and self.local_rank == 0:
                 self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id)
             start = time.time()
 
     def create_data(self,
                     annotation_path='dataset/annotation/',
@@ -483,47 +491,56 @@
         if nranks > 1 and self.use_gpu:
             self.optimizer = fleet.distributed_optimizer(self.optimizer)
             self.model = fleet.distributed_model(self.model)
         logger.info('训练数据：{}'.format(len(self.train_dataset)))
 
         self.__load_pretrained(pretrained_model=pretrained_model)
         # 加载恢复模型
-        last_epoch, best_error_rate = self.__load_checkpoint(save_model_path=save_model_path, resume_model=resume_model)
+        last_epoch, self.eval_best_error_rate = self.__load_checkpoint(save_model_path=save_model_path,
+                                                                       resume_model=resume_model)
 
-        test_step, self.train_step = 0, 0
+        self.train_loss = None
+        self.test_log_step, self.train_log_step = 0, 0
+        self.eval_loss, self.eval_error_result = None, None
         last_epoch += 1
         self.train_batch_sampler.epoch = last_epoch
         if self.local_rank == 0:
             writer.add_scalar('Train/lr', self.scheduler.get_lr(), self.train_step)
+        # 最大步数
+        self.max_step = len(self.train_loader) * self.configs.train_conf.max_epoch
+        self.train_step = max(last_epoch, 0) * len(self.train_loader)
         # 开始训练
         for epoch_id in range(last_epoch, self.configs.train_conf.max_epoch):
+            if self.stop_train: break
             epoch_id += 1
             start_epoch = time.time()
             # 训练一个epoch
             self.__train_epoch(epoch_id=epoch_id, save_model_path=save_model_path, writer=writer, nranks=nranks)
             # 多卡训练只使用一个进程执行评估和保存模型
             logger.info('=' * 70)
-            loss, error_result = self.evaluate(resume_model=None)
-            logger.info('Test epoch: {}, time/epoch: {}, loss: {:.5f}, {}: {:.5f}, best {}: {:.5f}'.format(
-                epoch_id, str(timedelta(seconds=(time.time() - start_epoch))), loss, self.configs.metrics_type,
-                error_result, self.configs.metrics_type, error_result if error_result <= best_error_rate else best_error_rate))
+            if self.stop_eval: continue
+            self.eval_loss, self.eval_error_result = self.evaluate(resume_model=None)
+            logger.info(
+                f'Test epoch: {epoch_id}, time/epoch: {str(timedelta(seconds=(time.time() - start_epoch)))}, '
+                f'loss: {self.eval_loss:.5f}, {self.configs.metrics_type}: {self.eval_error_result:.5f}, '
+                f'best {self.configs.metrics_type}: '
+                f'{self.eval_error_result if self.eval_error_result <= self.eval_best_error_rate else self.eval_best_error_rate:.5f}')
             logger.info('=' * 70)
-            test_step += 1
+            writer.add_scalar(f'Test/{self.configs.metrics_type}', self.eval_error_result, self.test_log_step)
+            writer.add_scalar('Test/Loss', self.eval_loss, self.test_log_step)
+            self.test_log_step += 1
             self.model.train()
-            if self.local_rank == 0:
-                writer.add_scalar('Test/{}'.format(self.configs.metrics_type), error_result, test_step)
-                writer.add_scalar('Test/Loss', loss, test_step)
-                # 保存最优模型
-                if error_result <= best_error_rate:
-                    best_error_rate = error_result
-                    self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id, error_rate=error_result,
-                                           test_loss=loss, best_model=True)
-                # 保存模型
-                self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id, error_rate=error_result,
-                                       test_loss=loss)
+            # 保存最优模型
+            if self.eval_error_result <= self.eval_best_error_rate:
+                self.eval_best_error_rate = self.eval_error_result
+                self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id,
+                                       error_rate=self.eval_error_result, test_loss=self.eval_loss, best_model=True)
+            # 保存模型
+            self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id,
+                                   error_rate=self.eval_error_result, test_loss=self.eval_loss)
 
     def evaluate(self, resume_model='models/conformer_streaming_fbank/best_model/', display_result=False):
         """
         评估模型
         :param resume_model: 所使用的模型
         :param display_result: 是否打印识别结果
         :return: 评估结果
@@ -546,14 +563,15 @@
         else:
             eval_model = self.model
 
         error_results, losses = [], []
         eos = self.test_dataset.vocab_size - 1
         with paddle.no_grad():
             for batch_id, batch in enumerate(tqdm(self.test_loader())):
+                if self.stop_eval: break
                 inputs, labels, input_lens, label_lens = batch
                 loss_dict = self.model(inputs, input_lens, labels, label_lens)
                 losses.append(float(loss_dict['loss']) / self.configs.train_conf.accum_grad)
                 # 获取模型编码器输出
                 outputs = eval_model.get_encoder_out(inputs, input_lens).numpy()
                 out_strings = self.__decoder_result(outs=outputs, vocabulary=self.test_dataset.vocab_list)
                 labels_str = labels_to_string(labels, self.test_dataset.vocab_list, eos=eos)
@@ -566,16 +584,16 @@
                     error_results.append(error_rate)
                     if display_result:
                         logger.info(f'预测结果为：{out_string}')
                         logger.info(f'实际标签为：{label}')
                         logger.info(f'这条数据的{self.configs.metrics_type}：{round(error_rate, 6)}，'
                                     f'当前{self.configs.metrics_type}：{round(sum(error_results) / len(error_results), 6)}')
                         logger.info('-' * 70)
-        loss = float(sum(losses) / len(losses))
-        error_result = float(sum(error_results) / len(error_results))
+        loss = float(sum(losses) / len(losses)) if len(losses) > 0 else -1
+        error_result = float(sum(error_results) / len(error_results)) if len(error_results) > 0 else -1
         self.model.train()
         return loss, error_result
 
     def export(self,
                save_model_path='models/',
                resume_model='models/conformer_streaming_fbank/best_model/',
                save_quant=False):
```

## ppasr/data_utils/audio.py

```diff
@@ -45,21 +45,21 @@
 
     def __ne__(self, other):
         """返回两个对象是否不相等"""
         return not self.__eq__(other)
 
     def __str__(self):
         """返回该音频的信息"""
-        return ("%s: num_samples=%d, sample_rate=%d, duration=%.2fsec, "
-                "rms=%.2fdB" % (type(self), self.num_samples, self.sample_rate, self.duration, self.rms_db))
+        return (f"{type(self)}: num_samples={self.num_samples}, sample_rate={self.sample_rate}, "
+                f"duration={self.duration:.2f}sec, rms={self.rms_db:.2f}dB")
 
     @classmethod
     def from_file(cls, file):
         """从音频文件创建音频段
-        
+
         :param file: 文件路径，或者文件对象
         :type file: str, BufferedReader
         :return: 音频片段实例
         :rtype: AudioSegment
         """
         assert os.path.exists(file), f'文件不存在，请检查路径：{file}'
         try:
@@ -93,17 +93,17 @@
         # 从末尾开始计
         if start < 0.0: start += duration
         if end < 0.0: end += duration
         # 保证数据不越界
         if start < 0.0: start = 0.0
         if end > duration: end = duration
         if end < 0.0:
-            raise ValueError("切片结束位置(%f s)越界" % end)
+            raise ValueError(f"切片结束位置({end} s)越界")
         if start > end:
-            raise ValueError("切片开始位置(%f s)晚于切片结束位置(%f s)" % (start, end))
+            raise ValueError(f"切片开始位置({start} s)晚于切片结束位置({end} s)")
         start_frame = int(start * sample_rate)
         end_frame = int(end * sample_rate)
         sndfile.seek(start_frame)
         data = sndfile.read(frames=end_frame - start_frame, dtype='float32')
         return cls(data, sample_rate)
 
     @classmethod
@@ -155,15 +155,15 @@
     def concatenate(cls, *segments):
         """将任意数量的音频片段连接在一起
 
         :param *segments: 输入音频片段被连接
         :type *segments: tuple of AudioSegment
         :return: Audio segment instance as concatenating results.
         :rtype: AudioSegment
-        :raises ValueError: If the number of segments is zero, or if the 
+        :raises ValueError: If the number of segments is zero, or if the
                             sample_rate of any segments does not match.
         :raises TypeError: If any segment is not AudioSegment instance.
         """
         # Perform basic sanity-checks.
         if len(segments) == 0:
             raise ValueError("没有音频片段被给予连接")
         sample_rate = segments[0]._sample_rate
@@ -187,15 +187,15 @@
         :rtype: AudioSegment
         """
         samples = np.zeros(int(duration * sample_rate))
         return cls(samples, sample_rate)
 
     def to_wav_file(self, filepath, dtype='float32'):
         """保存音频段到磁盘为wav文件
-        
+
         :param filepath: WAV文件路径或文件对象，以保存音频段
         :type filepath: str|file
         :param dtype: Subtype for audio file. Options: 'int16', 'int32',
                       'float32', 'float64'. Default is 'float32'.
         :type dtype: str
         :raises TypeError: If dtype is not supported.
         """
@@ -218,24 +218,24 @@
 
         :param other: 包含样品的片段被添加进去
         :type other: AudioSegments
         :raise TypeError: 如果两个片段的类型不匹配
         :raise ValueError: 不能添加不同类型的段
         """
         if not isinstance(other, type(self)):
-            raise TypeError("不能添加不同类型的段: %s 和 %s" % (type(self), type(other)))
+            raise TypeError(f"不能添加不同类型的段: {type(self)} 和 {type(other)}")
         if self._sample_rate != other._sample_rate:
             raise ValueError("采样率必须匹配才能添加片段")
         if len(self._samples) != len(other._samples):
             raise ValueError("段长度必须匹配才能添加段")
         self._samples += other._samples
 
     def to_bytes(self, dtype='float32'):
         """创建包含音频内容的字节字符串
-        
+
         :param dtype: Data type for export samples. Options: 'int16', 'int32',
                       'float32', 'float64'. Default is 'float32'.
         :type dtype: str
         :return: Byte string containing audio content.
         :rtype: str
         """
         samples = self._convert_samples_from_float32(self._samples, dtype)
@@ -253,19 +253,19 @@
         samples = self._convert_samples_from_float32(self._samples, dtype)
         return samples
 
     def gain_db(self, gain):
         """对音频施加分贝增益。
 
         Note that this is an in-place transformation.
-        
-        :param gain: Gain in decibels to apply to samples. 
+
+        :param gain: Gain in decibels to apply to samples.
         :type gain: float|1darray
         """
-        self._samples *= 10.**(gain / 20.)
+        self._samples *= 10. ** (gain / 20.)
 
     def change_speed(self, speed_rate):
         """通过线性插值改变音频速度
 
         :param speed_rate: Rate of speed change:
                            speed_rate > 1.0, speed up the audio;
                            speed_rate = 1.0, unchanged;
@@ -294,15 +294,14 @@
                             normalization. This is to prevent nans when
                             attempting to normalize a signal consisting of
                             all zeros.
         :type max_gain_db: float
         :raises ValueError: If the required gain to normalize the segment to
                             the target_db value exceeds max_gain_db.
         """
-        if -np.inf == self.rms_db: return
         gain = target_db - self.rms_db
         if gain > max_gain_db:
             raise ValueError(f"无法将段规范化到{target_db}dB，音频增益{gain}增益已经超过max_gain_db ({max_gain_db}dB)")
         self.gain_db(min(max_gain_db, target_db - self.rms_db))
 
     def resample(self, target_sample_rate, filter='kaiser_best'):
         """按目标采样率重新采样音频
@@ -338,15 +337,15 @@
         if sides == "beginning":
             padded = cls.concatenate(silence, self)
         elif sides == "end":
             padded = cls.concatenate(self, silence)
         elif sides == "both":
             padded = cls.concatenate(silence, self, silence)
         else:
-            raise ValueError("Unknown value for the sides %s" % sides)
+            raise ValueError(f"Unknown value for the sides {sides}")
         self._samples = padded._samples
 
     def shift(self, shift_ms):
         """音频偏移。如果shift_ms为正，则随时间提前移位;如果为负，则随时间延迟移位。填补静音以保持持续时间不变。
 
         Note that this is an in-place transformation.
 
@@ -382,21 +381,21 @@
         start_sec = 0.0 if start_sec is None else start_sec
         end_sec = self.duration if end_sec is None else end_sec
         if start_sec < 0.0:
             start_sec = self.duration + start_sec
         if end_sec < 0.0:
             end_sec = self.duration + end_sec
         if start_sec < 0.0:
-            raise ValueError("切片起始位置(%f s)越界" % start_sec)
+            raise ValueError(f"切片起始位置({start_sec} s)越界")
         if end_sec < 0.0:
-            raise ValueError("切片结束位置(%f s)越界" % end_sec)
+            raise ValueError(f"切片结束位置({end_sec} s)越界")
         if start_sec > end_sec:
-            raise ValueError("切片的起始位置(%f s)晚于结束位置(%f s)" % (start_sec, end_sec))
+            raise ValueError(f"切片的起始位置({start_sec} s)晚于结束位置({end_sec} s)")
         if end_sec > self.duration:
-            raise ValueError("切片结束位置(%f s)越界(> %f s)" % (end_sec, self.duration))
+            raise ValueError(f"切片结束位置({end_sec} s)越界(> {self.duration} s)")
         start_sample = int(round(start_sec * self._sample_rate))
         end_sample = int(round(end_sec * self._sample_rate))
         self._samples = self._samples[start_sample:end_sample]
 
     def random_subsegment(self, subsegment_length):
         """随机剪切指定长度的音频片段
 
@@ -417,25 +416,24 @@
         """将这个音频段与给定的脉冲段进行卷积
 
         Note that this is an in-place transformation.
 
         :param impulse_segment: Impulse response segments.
         :type impulse_segment: AudioSegment
         :param allow_resample: Indicates whether resampling is allowed when
-                               the impulse_segment has a different sample 
+                               the impulse_segment has a different sample
                                rate from this signal.
         :type allow_resample: bool
         :raises ValueError: If the sample rate is not match between two
                             audio segments when resample is not allowed.
         """
         if allow_resample and self.sample_rate != impulse_segment.sample_rate:
             impulse_segment.resample(self.sample_rate)
         if self.sample_rate != impulse_segment.sample_rate:
-            raise ValueError("脉冲段采样率(%d Hz)不等于基信号采样率(%d Hz)" %
-                             (impulse_segment.sample_rate, self.sample_rate))
+            raise ValueError(f"脉冲段采样率({impulse_segment.sample_rate} Hz)不等于基信号采样率({self.sample_rate} Hz)")
         samples = signal.fftconvolve(self.samples, impulse_segment.samples, "full")
         self._samples = samples
 
     def convolve_and_normalize(self, impulse_segment, allow_resample=False):
         """对所产生的音频段进行卷积并归一化，使其具有与输入信号相同的平均功率
 
         Note that this is an in-place transformation.
@@ -468,17 +466,17 @@
                             to apply infinite gain to a zero signal.
         :type max_gain_db: float
         :raises ValueError: If the sample rate does not match between the two
                             audio segments, or if the duration of noise segments
                             is shorter than original audio segments.
         """
         if noise.sample_rate != self.sample_rate:
-            raise ValueError("噪声采样率(%d Hz)不等于基信号采样率(%d Hz)" % (noise.sample_rate, self.sample_rate))
+            raise ValueError(f"噪声采样率({noise.sample_rate} Hz)不等于基信号采样率({self.sample_rate} Hz)")
         if noise.duration < self.duration:
-            raise ValueError("噪声信号(%f秒)必须至少与基信号(%f秒)一样长" % (noise.duration, self.duration))
+            raise ValueError(f"噪声信号({noise.duration}秒)必须至少与基信号({self.duration}秒)一样长")
         noise_gain_db = min(self.rms_db - noise.rms_db - snr_dB, max_gain_db)
         noise_new = copy.deepcopy(noise)
         noise_new.random_subsegment(self.duration)
         noise_new.gain_db(noise_gain_db)
         self.superimpose(noise_new)
 
     @property
@@ -522,17 +520,20 @@
         """返回以分贝为单位的音频均方根能量
 
         :return: Root mean square energy in decibels.
         :rtype: float
         """
         # square root => multiply by 10 instead of 20 for dBs
         mean_square = np.mean(self._samples ** 2)
+        if mean_square == 0:
+            mean_square = 1
         return 10 * np.log10(mean_square)
 
-    def _convert_samples_to_float32(self, samples):
+    @staticmethod
+    def _convert_samples_to_float32(samples):
         """Convert sample type to float32.
 
         Audio sample type is usually integer or float-point.
         Integers will be scaled to [-1, 1] in float32.
         """
         float32_samples = samples.astype('float32')
         if samples.dtype in np.sctypes['int']:
@@ -540,15 +541,16 @@
             float32_samples *= (1. / 2 ** (bits - 1))
         elif samples.dtype in np.sctypes['float']:
             pass
         else:
             raise TypeError("Unsupported sample type: %s." % samples.dtype)
         return float32_samples
 
-    def _convert_samples_from_float32(self, samples, dtype):
+    @staticmethod
+    def _convert_samples_from_float32(samples, dtype):
         """Convert sample type from float32 to dtype.
 
         Audio sample type is usually integer or float-point. For integer
         type, float32 will be rescaled from [-1, 1] to the maximum range
         supported by the integer type.
 
         This is for writing a audio file.
```

## ppasr/data_utils/utils.py

```diff
@@ -1,14 +1,15 @@
 import io
 import itertools
 import json
 import os
 import time
 import wave
 
+import gc
 import av
 import numpy as np
 import resampy
 import soundfile
 from tqdm import tqdm
 from zhconv import convert
 
@@ -320,25 +321,28 @@
       A float32 Numpy array.
     """
     resampler = av.audio.resampler.AudioResampler(format="s16", layout="mono", rate=sample_rate)
 
     raw_buffer = io.BytesIO()
     dtype = None
 
-    with av.open(file, metadata_errors="ignore") as container:
+    with av.open(file, mode="r", metadata_errors="ignore") as container:
         frames = container.decode(audio=0)
         frames = _ignore_invalid_frames(frames)
         frames = _group_frames(frames, 500000)
         frames = _resample_frames(frames, resampler)
 
         for frame in frames:
             array = frame.to_ndarray()
             dtype = array.dtype
             raw_buffer.write(array)
 
+    del resampler
+    gc.collect()
+
     audio = np.frombuffer(raw_buffer.getbuffer(), dtype=dtype)
 
     # Convert s16 back to f32.
     return audio.astype(np.float32) / 32768.0
 
 
 def _ignore_invalid_frames(frames):
```

## Comparing `ppasr-2.4.6.dist-info/LICENSE` & `ppasr-2.4.7.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `ppasr-2.4.6.dist-info/METADATA` & `ppasr-2.4.7.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: ppasr
-Version: 2.4.6
+Version: 2.4.7
 Summary: Automatic speech recognition toolkit on PaddlePaddle
 Home-page: https://github.com/yeyupiaoling/PPASR
 Download-URL: https://github.com/yeyupiaoling/PPASR.git
 Author: yeyupiaoling
 License: Apache License 2.0
 Keywords: asr,paddle
 Classifier: Intended Audience :: Developers
@@ -16,31 +16,31 @@
 Classifier: Programming Language :: Python :: 3.6
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Topic :: Utilities
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: numpy (>=1.19.2)
-Requires-Dist: scipy (>=1.6.3)
+Requires-Dist: numpy >=1.19.2
+Requires-Dist: scipy >=1.6.3
 Requires-Dist: tqdm
-Requires-Dist: python-Levenshtein (==0.12.2)
-Requires-Dist: visualdl (>=2.1.1)
-Requires-Dist: soundfile (>=0.12.1)
-Requires-Dist: soundcard (>=0.4.2)
-Requires-Dist: resampy (>=0.2.2)
-Requires-Dist: zhconv (>=1.4.2)
-Requires-Dist: ijson (>=3.1.4)
-Requires-Dist: pyyaml (>=5.4.1)
-Requires-Dist: termcolor (>=1.1.0)
-Requires-Dist: scikit-learn (>=1.0.2)
-Requires-Dist: paddleaudio (>=1.0.1)
-Requires-Dist: typeguard (==2.13.3)
-Requires-Dist: onnxruntime (>=1.11.1)
-Requires-Dist: av (>=10.0.0)
+Requires-Dist: python-Levenshtein ==0.12.2
+Requires-Dist: visualdl >=2.1.1
+Requires-Dist: soundfile >=0.12.1
+Requires-Dist: soundcard >=0.4.2
+Requires-Dist: resampy >=0.2.2
+Requires-Dist: zhconv >=1.4.2
+Requires-Dist: ijson >=3.1.4
+Requires-Dist: pyyaml >=5.4.1
+Requires-Dist: termcolor >=1.1.0
+Requires-Dist: scikit-learn >=1.0.2
+Requires-Dist: paddleaudio >=1.0.1
+Requires-Dist: typeguard ==2.13.3
+Requires-Dist: onnxruntime >=1.11.1
+Requires-Dist: av >=10.0.0
 
 ![python version](https://img.shields.io/badge/python-3.8+-orange.svg)
 ![GitHub forks](https://img.shields.io/github/forks/yeyupiaoling/PPASR)
 ![GitHub Repo stars](https://img.shields.io/github/stars/yeyupiaoling/PPASR)
 ![GitHub](https://img.shields.io/github/license/yeyupiaoling/PPASR)
 ![支持系统](https://img.shields.io/badge/支持系统-Win/Linux/MAC-9cf)
 
@@ -56,24 +56,24 @@
 </div>
 
 
 ## 在线使用
 
 **1. [在AI Studio平台训练预测](https://aistudio.baidu.com/aistudio/projectdetail/3290199)**
 
-**2. [在线使用Dome](http://ppasr.yeyupiaoling.cn:8081)**
+**2. [在线使用Dome](https://www.doiduoyi.com/?app=SPEECHRECOG)**
 
-**3. [在线使用](https://inscode.csdn.net/@yeyupiaoling/ppasr)**
+**3. [inscode](https://inscode.csdn.net/@yeyupiaoling/ppasr)**
 
 <br/>
 
 **本项目使用的环境：**
  - Anaconda 3
  - Python 3.8
- - PaddlePaddle 2.4.1
+ - PaddlePaddle 2.5.1
  - Windows 10 or Ubuntu 18.04
 
 
 ## 项目快速了解
 
  1. 本项目支持流式识别模型`deepspeech2`、`conformer`、`squeezeformer`，`efficient_conformer`，每个模型都支持流式识别和非流式识别，在配置文件中`streaming`参数设置。
  2. 本项目支持两种解码器，分别是集束搜索解码器`ctc_beam_search`和贪心解码器`ctc_greedy`，集束搜索解码器`ctc_beam_search`准确率更高。
```

## Comparing `ppasr-2.4.6.dist-info/RECORD` & `ppasr-2.4.7.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-ppasr/__init__.py,sha256=UNZjmD5rw0RNmh266EXaoZjAiT_zeYGTpdNyqu26w-g,134
+ppasr/__init__.py,sha256=l_HahMoHfGkdZw1mOdRz8Fr-YZbXbpaJO9UNJQ3Ogo8,134
 ppasr/predict.py,sha256=5xUrdL6My1WBf4Kc8sx39KV3RN7KmI8oPks5MWq5aYc,17895
-ppasr/trainer.py,sha256=-lYACGhMLpPPQ54X0EQZiP6Ti1edU1LnabjBDm-iv4s,37501
+ppasr/trainer.py,sha256=0oBSqnddzLCPTI0DcHS4Izs6WCKO2AGBbOMn5ZzXYFw,38580
 ppasr/data_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ppasr/data_utils/audio.py,sha256=bNEe0VW1GkiFi0Iy9K8ScJ6hUYcRALcRSevbFrF6Rek,23922
+ppasr/data_utils/audio.py,sha256=oGW1cbA_SmUVcfBV4ZNTKXwzTtBpFCYAOXLOtyJGxok,23844
 ppasr/data_utils/binary.py,sha256=qNXcZRcLcehLdGg4YwP5FBcLCpFp8Fd9YVSb5numyzs,2377
 ppasr/data_utils/collate_fn.py,sha256=tG5c5EjaCaz3WBQTqTk2gjsBUCZldRp-9mXBP76kfHs,1584
 ppasr/data_utils/normalizer.py,sha256=kPavVCRVk59p_ZHIfE6G20fTCrna5YxN4yOmTzTs67w,5192
 ppasr/data_utils/reader.py,sha256=Zl7po0c5i6ggodRZwgzL3jrjSC8YVx7CigwlFx5d-uU,4244
 ppasr/data_utils/sampler.py,sha256=NKql8qAqNGHrbrFrQ9vLwWssV7cA7P3Pb9Fc0nkUI1g,8477
-ppasr/data_utils/utils.py,sha256=96MzXR9l-uxWuDnykMSN-8XUi-fxfoDcVPAE2hCI1a8,15626
+ppasr/data_utils/utils.py,sha256=ECfUz6UA7RfjhMOc6huKwi1rCbD9LQn-dfYFk8pboCk,15686
 ppasr/data_utils/augmentor/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppasr/data_utils/augmentor/augmentation.py,sha256=Y-ZizFZFAsxHkXMjA8jUKKMWcQlRIOHCmHqKkHR9J7Q,5787
 ppasr/data_utils/augmentor/base.py,sha256=Qw5AsIjpqDkggVkVyfPGTI_IwnfSuIWViRdgo7g2ze0,965
 ppasr/data_utils/augmentor/noise_perturb.py,sha256=EB3Y59EEytiXXCg1ixWUS99hkQJwYGxh-Ykh1E4zaG0,2570
 ppasr/data_utils/augmentor/resample.py,sha256=ugt2468E_SNvy6moQloD9TT92MXyvrX2skWyoDx2FMg,977
 ppasr/data_utils/augmentor/shift_perturb.py,sha256=36R5av4WlK-HIlo1_millJdZP70Aqcg85JKZbmGWTAs,1007
 ppasr/data_utils/augmentor/spec_augment.py,sha256=Eg1i0v4JVZYc8Sg7ezGobGvQZd44c6tq-0UjOus0CK4,5015
@@ -72,12 +72,12 @@
 ppasr/optimizer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppasr/optimizer/scheduler.py,sha256=Ki-0T0JZS2_LCJsTo9Xvb2dHrUuDz0mxh6sH9mb3j2M,10523
 ppasr/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppasr/utils/logger.py,sha256=eHExExIfsMSQD7B0iCh0TSzx257JZIanKJCK7BWp23s,2844
 ppasr/utils/metrics.py,sha256=PGnCSSGnJH61xOzuM0iaNDLGwAllZh0ILPSCPRcGIAg,893
 ppasr/utils/model_summary.py,sha256=75MjURp-FJ8B3c0AdMhg1T-ePWb4IJPvtwk8HertJa8,13549
 ppasr/utils/utils.py,sha256=mfL-DzN8XEn6k_5E7FZ53hGBQKEqFotAgl8-Njrnhtc,3910
-ppasr-2.4.6.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
-ppasr-2.4.6.dist-info/METADATA,sha256=gDcmVB701sMlwGQ7T-6gd1pZUDgdLiBJjJOthJ7w2gI,11171
-ppasr-2.4.6.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-ppasr-2.4.6.dist-info/top_level.txt,sha256=bW0NEVoj4d41szj8YJKhSn7D-K3AX86ipdtJLH_yXD4,6
-ppasr-2.4.6.dist-info/RECORD,,
+ppasr-2.4.7.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
+ppasr-2.4.7.dist-info/METADATA,sha256=CR4v9Gk1EawGbN2E4x7OCCyp-37polWjN84m9qIwi54,11142
+ppasr-2.4.7.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+ppasr-2.4.7.dist-info/top_level.txt,sha256=bW0NEVoj4d41szj8YJKhSn7D-K3AX86ipdtJLH_yXD4,6
+ppasr-2.4.7.dist-info/RECORD,,
```

