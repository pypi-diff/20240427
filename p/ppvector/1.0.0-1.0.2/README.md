# Comparing `tmp/ppvector-1.0.0-py3-none-any.whl.zip` & `tmp/ppvector-1.0.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,35 +1,35 @@
-Zip file size: 60527 bytes, number of entries: 33
--rw-rw-rw-  2.0 fat      134 b- defN 23-Aug-10 01:20 ppvector/__init__.py
--rw-rw-rw-  2.0 fat    17212 b- defN 23-Nov-21 07:49 ppvector/predict.py
--rw-rw-rw-  2.0 fat    32699 b- defN 23-Nov-21 07:49 ppvector/trainer.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-05 15:07 ppvector/data_utils/__init__.py
--rw-rw-rw-  2.0 fat    22259 b- defN 23-Aug-05 15:07 ppvector/data_utils/audio.py
--rw-rw-rw-  2.0 fat      909 b- defN 23-Aug-05 15:07 ppvector/data_utils/collate_fn.py
--rw-rw-rw-  2.0 fat     3771 b- defN 23-Aug-07 05:50 ppvector/data_utils/featurizer.py
--rw-rw-rw-  2.0 fat     5892 b- defN 23-Sep-13 06:27 ppvector/data_utils/reader.py
--rw-rw-rw-  2.0 fat     1581 b- defN 23-Aug-07 06:49 ppvector/data_utils/spec_aug.py
--rw-rw-rw-  2.0 fat     4653 b- defN 23-Aug-05 15:07 ppvector/data_utils/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-05 15:07 ppvector/metric/__init__.py
--rw-rw-rw-  2.0 fat     1208 b- defN 23-Aug-05 15:07 ppvector/metric/metrics.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-05 15:07 ppvector/models/__init__.py
--rw-rw-rw-  2.0 fat    12745 b- defN 23-Aug-10 01:20 ppvector/models/campplus.py
--rw-rw-rw-  2.0 fat    11077 b- defN 23-Aug-10 02:25 ppvector/models/ecapa_tdnn.py
--rw-rw-rw-  2.0 fat    10135 b- defN 23-Aug-10 02:25 ppvector/models/eres2net.py
--rw-rw-rw-  2.0 fat     3661 b- defN 23-Nov-21 07:49 ppvector/models/fc.py
--rw-rw-rw-  2.0 fat     6300 b- defN 23-Nov-21 07:49 ppvector/models/loss.py
--rw-rw-rw-  2.0 fat     5279 b- defN 23-Aug-10 02:25 ppvector/models/pooling.py
--rw-rw-rw-  2.0 fat     6843 b- defN 23-Aug-10 02:25 ppvector/models/res2net.py
--rw-rw-rw-  2.0 fat     5662 b- defN 23-Aug-10 02:25 ppvector/models/resnet_se.py
--rw-rw-rw-  2.0 fat     3459 b- defN 23-Aug-10 02:25 ppvector/models/tdnn.py
--rw-rw-rw-  2.0 fat     5049 b- defN 23-Aug-05 15:07 ppvector/models/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-05 15:07 ppvector/utils/__init__.py
--rw-rw-rw-  2.0 fat     2839 b- defN 23-Aug-05 15:07 ppvector/utils/logger.py
--rw-rw-rw-  2.0 fat     1067 b- defN 23-Aug-05 15:07 ppvector/utils/record.py
--rw-rw-rw-  2.0 fat     3399 b- defN 23-Aug-05 15:07 ppvector/utils/scheduler.py
--rw-rw-rw-  2.0 fat     2790 b- defN 23-Aug-05 15:07 ppvector/utils/utils.py
--rw-rw-rw-  2.0 fat    11558 b- defN 23-Nov-21 07:50 ppvector-1.0.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    27457 b- defN 23-Nov-21 07:50 ppvector-1.0.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Nov-21 07:50 ppvector-1.0.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        9 b- defN 23-Nov-21 07:50 ppvector-1.0.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2709 b- defN 23-Nov-21 07:50 ppvector-1.0.0.dist-info/RECORD
-33 files, 212448 bytes uncompressed, 56211 bytes compressed:  73.5%
+Zip file size: 63389 bytes, number of entries: 33
+-rw-rw-rw-  2.0 fat      134 b- defN 24-Apr-27 06:49 ppvector/__init__.py
+-rw-rw-rw-  2.0 fat    17739 b- defN 24-Jan-10 12:55 ppvector/predict.py
+-rw-rw-rw-  2.0 fat    34807 b- defN 24-Apr-21 12:12 ppvector/trainer.py
+-rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-04 14:44 ppvector/data_utils/__init__.py
+-rw-rw-rw-  2.0 fat    22221 b- defN 24-Apr-27 05:31 ppvector/data_utils/audio.py
+-rw-rw-rw-  2.0 fat      909 b- defN 23-Feb-15 13:35 ppvector/data_utils/collate_fn.py
+-rw-rw-rw-  2.0 fat     3771 b- defN 23-Aug-29 13:07 ppvector/data_utils/featurizer.py
+-rw-rw-rw-  2.0 fat     5892 b- defN 23-Sep-13 12:01 ppvector/data_utils/reader.py
+-rw-rw-rw-  2.0 fat     1581 b- defN 23-Aug-29 13:07 ppvector/data_utils/spec_aug.py
+-rw-rw-rw-  2.0 fat     4713 b- defN 24-Jan-06 11:55 ppvector/data_utils/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Mar-16 05:03 ppvector/metric/__init__.py
+-rw-rw-rw-  2.0 fat     1208 b- defN 23-Aug-29 13:07 ppvector/metric/metrics.py
+-rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-05 11:08 ppvector/models/__init__.py
+-rw-rw-rw-  2.0 fat    12745 b- defN 23-Aug-29 13:07 ppvector/models/campplus.py
+-rw-rw-rw-  2.0 fat    11077 b- defN 23-Aug-29 13:07 ppvector/models/ecapa_tdnn.py
+-rw-rw-rw-  2.0 fat    10135 b- defN 23-Aug-29 13:07 ppvector/models/eres2net.py
+-rw-rw-rw-  2.0 fat     3731 b- defN 24-Jan-10 12:55 ppvector/models/fc.py
+-rw-rw-rw-  2.0 fat     9521 b- defN 24-Jan-10 12:55 ppvector/models/loss.py
+-rw-rw-rw-  2.0 fat     5279 b- defN 23-Aug-29 13:07 ppvector/models/pooling.py
+-rw-rw-rw-  2.0 fat     6843 b- defN 23-Aug-29 13:07 ppvector/models/res2net.py
+-rw-rw-rw-  2.0 fat     5662 b- defN 23-Aug-29 13:07 ppvector/models/resnet_se.py
+-rw-rw-rw-  2.0 fat     3459 b- defN 23-Aug-29 13:07 ppvector/models/tdnn.py
+-rw-rw-rw-  2.0 fat     5049 b- defN 23-Aug-05 13:45 ppvector/models/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-04 14:44 ppvector/utils/__init__.py
+-rw-rw-rw-  2.0 fat     2839 b- defN 22-Nov-04 14:44 ppvector/utils/logger.py
+-rw-rw-rw-  2.0 fat     1385 b- defN 24-Jan-15 12:44 ppvector/utils/record.py
+-rw-rw-rw-  2.0 fat     3399 b- defN 24-Jan-17 14:13 ppvector/utils/scheduler.py
+-rw-rw-rw-  2.0 fat     2790 b- defN 23-Mar-16 11:21 ppvector/utils/utils.py
+-rw-rw-rw-  2.0 fat    11558 b- defN 24-Apr-27 06:49 ppvector-1.0.2.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    33186 b- defN 24-Apr-27 06:49 ppvector-1.0.2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-27 06:49 ppvector-1.0.2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        9 b- defN 24-Apr-27 06:49 ppvector-1.0.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2709 b- defN 24-Apr-27 06:49 ppvector-1.0.2.dist-info/RECORD
+33 files, 224443 bytes uncompressed, 59073 bytes compressed:  73.7%
```

## zipnote {}

```diff
@@ -78,23 +78,23 @@
 
 Filename: ppvector/utils/scheduler.py
 Comment: 
 
 Filename: ppvector/utils/utils.py
 Comment: 
 
-Filename: ppvector-1.0.0.dist-info/LICENSE
+Filename: ppvector-1.0.2.dist-info/LICENSE
 Comment: 
 
-Filename: ppvector-1.0.0.dist-info/METADATA
+Filename: ppvector-1.0.2.dist-info/METADATA
 Comment: 
 
-Filename: ppvector-1.0.0.dist-info/WHEEL
+Filename: ppvector-1.0.2.dist-info/WHEEL
 Comment: 
 
-Filename: ppvector-1.0.0.dist-info/top_level.txt
+Filename: ppvector-1.0.2.dist-info/top_level.txt
 Comment: 
 
-Filename: ppvector-1.0.0.dist-info/RECORD
+Filename: ppvector-1.0.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ppvector/__init__.py

```diff
@@ -1,3 +1,3 @@
-__version__ = "1.0.0"
+__version__ = "1.0.2"
 # 项目支持的模型
 SUPPORT_MODEL = ['ERes2Net', 'CAMPPlus', 'EcapaTdnn', 'Res2Net', 'ResNetSE', 'TDNN']
```

## ppvector/predict.py

```diff
@@ -266,29 +266,35 @@
         # 执行预测
         if self.configs.use_model == 'EcapaTdnn':
             features = self.predictor([audio_feature, input_lens_ratio]).numpy()
         else:
             features = self.predictor(audio_feature).numpy()
         return features
 
-    # 声纹对比
     def contrast(self, audio_data1, audio_data2):
+        """声纹对比
+
+        param audio_data1: 需要对比的音频1，支持文件路径，文件对象，字节，numpy。如果是字节的话，必须是完整的字节文件
+        param audio_data2: 需要对比的音频2，支持文件路径，文件对象，字节，numpy。如果是字节的话，必须是完整的字节文件
+
+        return: 两个音频的相似度
+        """
         feature1 = self.predict(audio_data1)
         feature2 = self.predict(audio_data2)
         # 对角余弦值
         dist = np.dot(feature1, feature2) / (np.linalg.norm(feature1) * np.linalg.norm(feature2))
         return dist
 
     def register(self,
-                 user_name,
                  audio_data,
+                 user_name: str,
                  sample_rate=16000):
         """声纹注册
-        :param user_name: 注册用户的名字
         :param audio_data: 需要识别的数据，支持文件路径，文件对象，字节，numpy。如果是字节的话，必须是完整的字节文件
+        :param user_name: 注册用户的名字
         :param sample_rate: 如果传入的事numpy数据，需要指定采样率
         :return: 识别的文本结果和解码的得分数
         """
         # 加载音频文件
         if isinstance(audio_data, str):
             audio_segment = AudioSegment.from_file(audio_data)
         elif isinstance(audio_data, BufferedReader):
@@ -326,14 +332,21 @@
         """
         if threshold:
             self.threshold = threshold
         feature = self.predict(audio_data, sample_rate=sample_rate)
         name, score = self.__retrieval(np_feature=[feature])[0]
         return name, score
 
+    def get_users(self):
+        """获取所有用户
+
+        return: 所有用户
+        """
+        return self.users_name
+
     def remove_user(self, user_name):
         """删除用户
 
         :param user_name: 用户名
         :return:
         """
         if user_name in self.users_name:
```

## ppvector/trainer.py

```diff
@@ -24,15 +24,15 @@
 from ppvector.data_utils.reader import CustomDataset
 from ppvector.data_utils.spec_aug import SpecAug
 from ppvector.metric.metrics import compute_fnr_fpr, compute_eer, compute_dcf
 from ppvector.models.campplus import CAMPPlus
 from ppvector.models.ecapa_tdnn import EcapaTdnn
 from ppvector.models.eres2net import ERes2Net
 from ppvector.models.fc import SpeakerIdentification
-from ppvector.models.loss import AAMLoss, AMLoss, ARMLoss, CELoss, SubCenterLoss
+from ppvector.models.loss import AAMLoss, AMLoss, ARMLoss, CELoss, SubCenterLoss, SphereFace2
 from ppvector.models.res2net import Res2Net
 from ppvector.models.resnet_se import ResNetSE
 from ppvector.models.tdnn import TDNN
 from ppvector.utils.logger import setup_logger
 from ppvector.utils.scheduler import cosine_decay_with_warmup, MarginScheduler
 from ppvector.utils.utils import dict_to_object, print_arguments
 
@@ -69,14 +69,20 @@
         # 获取特征器
         self.audio_featurizer = AudioFeaturizer(feature_method=self.configs.preprocess_conf.feature_method,
                                                 method_args=self.configs.preprocess_conf.get('method_args', {}))
         self.spec_aug = SpecAug(**self.configs.dataset_conf.get('spec_aug_args', {}))
         if platform.system().lower() == 'windows':
             self.configs.dataset_conf.dataLoader.num_workers = 0
             logger.warning('Windows系统不支持多线程读取数据，已自动关闭！')
+        self.max_step, self.train_step = None, None
+        self.train_loss, self.train_acc = None, None
+        self.train_eta_sec = None
+        self.eval_eer, self.eval_min_dcf, self.eval_threshold = None, None, None
+        self.test_log_step, self.train_log_step = 0, 0
+        self.stop_train, self.stop_eval = False, False
 
     def __setup_dataloader(self, is_train=False):
         if is_train:
             self.train_dataset = CustomDataset(data_list_path=self.configs.dataset_conf.train_list,
                                                do_vad=self.configs.dataset_conf.do_vad,
                                                max_duration=self.configs.dataset_conf.max_duration,
                                                min_duration=self.configs.dataset_conf.min_duration,
@@ -159,14 +165,16 @@
             self.model = nn.Sequential(self.backbone, classifier)
 
             # 获取损失函数
             loss_args = self.configs.loss_conf.get('args', {})
             loss_args = loss_args if loss_args is not None else {}
             if use_loss == 'AAMLoss':
                 self.loss = AAMLoss(**loss_args)
+            elif use_loss == 'SphereFace2':
+                self.loss = SphereFace2(**loss_args)
             elif use_loss == 'SubCenterLoss':
                 self.loss = SubCenterLoss(**loss_args)
             elif use_loss == 'AMLoss':
                 self.loss = AMLoss(**loss_args)
             elif use_loss == 'ARMLoss':
                 self.loss = ARMLoss(**loss_args)
             elif use_loss == 'CELoss':
@@ -257,19 +265,24 @@
             if self.amp_scaler is not None and os.path.exists(os.path.join(resume_model, 'scaler.pdparams')):
                 self.amp_scaler.load_state_dict(paddle.load(os.path.join(resume_model, 'scaler.pdparams')))
             with open(os.path.join(resume_model, 'model.state'), 'r', encoding='utf-8') as f:
                 json_data = json.load(f)
                 last_epoch = json_data['last_epoch'] - 1
                 if 'eer' in json_data.keys():
                     best_eer = json_data['eer']
+            if last_epoch >= 0:
+                # 恢复学习率和margin
+                self.scheduler.step((last_epoch + 1) * len(self.train_loader))
+                if self.margin_scheduler:
+                    self.margin_scheduler.step((last_epoch + 1) * len(self.train_loader))
             logger.info('成功恢复模型参数和优化方法参数：{}'.format(resume_model))
         return last_epoch, best_eer
 
     # 保存模型
-    def __save_checkpoint(self, save_model_path, epoch_id, best_eer=None, best_model=False):
+    def __save_checkpoint(self, save_model_path, epoch_id, eer=None, min_dcf=None, threshold=None, best_model=False):
         if best_model:
             model_path = os.path.join(save_model_path,
                                       f'{self.configs.use_model}_{self.configs.preprocess_conf.feature_method}',
                                       'best_model')
         else:
             model_path = os.path.join(save_model_path,
                                       f'{self.configs.use_model}_{self.configs.preprocess_conf.feature_method}',
@@ -282,16 +295,20 @@
             if self.amp_scaler is not None:
                 paddle.save(self.amp_scaler.state_dict(), os.path.join(model_path, 'scaler.pdparams'))
         except Exception as e:
             logger.error(f'保存模型时出现错误，错误信息：{e}')
             return
         with open(os.path.join(model_path, 'model.state'), 'w', encoding='utf-8') as f:
             data = {"last_epoch": epoch_id, "version": __version__}
-            if best_eer is not None:
-                data['eer'] = best_eer
+            if eer is not None:
+                data['threshold'] = threshold
+                data['eer'] = eer
+                data['min_dcf'] = min_dcf
+            if self.margin_scheduler:
+                data['margin'] = self.margin_scheduler.get_margin()
             f.write(json.dumps(data, ensure_ascii=False))
         if not best_model:
             last_model_path = os.path.join(save_model_path,
                                            f'{self.configs.use_model}_{self.configs.preprocess_conf.feature_method}',
                                            'last_model')
             shutil.rmtree(last_model_path, ignore_errors=True)
             shutil.copytree(model_path, last_model_path)
@@ -303,16 +320,16 @@
                 shutil.rmtree(old_model_path)
         logger.info('已保存模型：{}'.format(model_path))
 
     def __train_epoch(self, epoch_id, save_model_path, local_rank, writer):
         train_times, accuracies, loss_sum = [], [], []
         start = time.time()
         use_loss = self.configs.loss_conf.get('use_loss', 'AAMLoss')
-        sum_batch = len(self.train_loader) * self.configs.train_conf.max_epoch
         for batch_id, (audio, label, input_lens_ratio) in enumerate(self.train_loader()):
+            if self.stop_train: break
             features, _ = self.audio_featurizer(audio, input_lens_ratio)
             if self.configs.dataset_conf.use_spec_aug:
                 features = self.spec_aug(features)
             # 执行模型计算，是否开启自动混合精度
             with paddle.amp.auto_cast(enable=self.configs.train_conf.enable_amp, level='O1'):
                 if self.configs.use_model == 'EcapaTdnn':
                     output = self.model([features, input_lens_ratio])
@@ -342,35 +359,39 @@
                 cosine = paddle.reshape(output, (-1, output.shape[1] // loss_args.K, loss_args.K))
                 output = paddle.max(cosine, 2)
             label = paddle.reshape(label, shape=(-1, 1))
             acc = accuracy(input=paddle.nn.functional.softmax(output), label=label)
             accuracies.append(float(acc))
             loss_sum.append(float(los))
             train_times.append((time.time() - start) * 1000)
+            self.train_step += 1
 
             # 多卡训练只使用一个进程打印
             if batch_id % self.configs.train_conf.log_interval == 0 and local_rank == 0:
                 # 计算每秒训练数据量
                 train_speed = self.configs.dataset_conf.dataLoader.batch_size / (
                             sum(train_times) / len(train_times) / 1000)
                 # 计算剩余时间
-                eta_sec = (sum(train_times) / len(train_times)) * (
-                        sum_batch - (epoch_id - 1) * len(self.train_loader) - batch_id)
-                eta_str = str(timedelta(seconds=int(eta_sec / 1000)))
+                self.train_eta_sec = (sum(train_times) / len(train_times)) * (self.max_step - self.train_step) / 1000
+                eta_str = str(timedelta(seconds=int(self.train_eta_sec)))
+                self.train_loss = sum(loss_sum) / len(loss_sum)
+                self.train_acc = sum(accuracies) / len(accuracies)
+                margin_str = f'margin: {self.margin_scheduler.get_margin()}' if self.margin_scheduler else ''
                 logger.info(f'Train epoch: [{epoch_id}/{self.configs.train_conf.max_epoch}], '
                             f'batch: [{batch_id}/{len(self.train_loader)}], '
-                            f'loss: {sum(loss_sum) / len(loss_sum):.5f}, '
-                            f'accuracy: {sum(accuracies) / len(accuracies):.5f}, '
-                            f'learning rate: {self.scheduler.get_lr():>.8f}, '
+                            f'loss: {self.train_loss:.5f}, accuracy: {self.train_acc:.5f}, '
+                            f'learning rate: {self.scheduler.get_lr():.8f}, {margin_str} '
                             f'speed: {train_speed:.2f} data/sec, eta: {eta_str}')
-                writer.add_scalar('Train/Loss', sum(loss_sum) / len(loss_sum), self.train_step)
-                writer.add_scalar('Train/Accuracy', (sum(accuracies) / len(accuracies)), self.train_step)
+                writer.add_scalar('Train/Loss', self.train_loss, self.train_log_step)
+                writer.add_scalar('Train/Accuracy', self.train_acc, self.train_log_step)
                 # 记录学习率
-                writer.add_scalar('Train/lr', self.scheduler.get_lr(), self.train_step)
-                self.train_step += 1
+                writer.add_scalar('Train/lr', self.scheduler.get_lr(), self.train_log_step)
+                if self.margin_scheduler:
+                    writer.add_scalar('Train/margin', self.margin_scheduler.get_margin(), self.train_log_step)
+                self.train_log_step += 1
                 train_times, accuracies, loss_sum = [], [], []
             # 固定步数也要保存一次模型
             if batch_id % 10000 == 0 and batch_id != 0 and local_rank == 0:
                 self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id)
             start = time.time()
             self.scheduler.step()
             if self.margin_scheduler:
@@ -413,47 +434,54 @@
             self.model = fleet.distributed_model(self.model)
         logger.info('训练数据：{}'.format(len(self.train_dataset)))
 
         self.__load_pretrained(pretrained_model=pretrained_model)
         # 加载恢复模型
         last_epoch, best_eer = self.__load_checkpoint(save_model_path=save_model_path, resume_model=resume_model)
 
-        eer = None
-        test_step, self.train_step = 0, 0
+        self.train_loss, self.train_acc = None, None
+        self.test_log_step, self.train_log_step = 0, 0
+        self.eval_eer, self.eval_min_dcf, self.eval_threshold = None, None, None
         last_epoch += 1
         if local_rank == 0:
             writer.add_scalar('Train/lr', self.scheduler.get_lr(), last_epoch)
+        # 最大步数
+        self.max_step = len(self.train_loader) * self.configs.train_conf.max_epoch
+        self.train_step = max(last_epoch, 0) * len(self.train_loader)
         # 开始训练
         for epoch_id in range(last_epoch, self.configs.train_conf.max_epoch):
+            if self.stop_train: break
             epoch_id += 1
             start_epoch = time.time()
             # 训练一个epoch
             self.__train_epoch(epoch_id=epoch_id, save_model_path=save_model_path, local_rank=local_rank,
                                writer=writer)
             # 多卡训练只使用一个进程执行评估和保存模型
             if local_rank == 0 and do_eval:
+                if self.stop_eval: continue
                 logger.info('=' * 70)
-                eer, min_dcf, threshold = self.evaluate()
+                self.eval_eer, self.eval_min_dcf, self.eval_threshold = self.evaluate()
                 logger.info('Test epoch: {}, time/epoch: {}, threshold: {:.2f}, EER: {:.5f}, '
                             'MinDCF: {:.5f}'.format(epoch_id, str(timedelta(
-                    seconds=(time.time() - start_epoch))), threshold, eer, min_dcf))
+                    seconds=(time.time() - start_epoch))), self.eval_threshold, self.eval_eer, self.eval_min_dcf))
                 logger.info('=' * 70)
-                writer.add_scalar('Test/threshold', threshold, test_step)
-                writer.add_scalar('Test/min_dcf', min_dcf, test_step)
-                writer.add_scalar('Test/eer', eer, test_step)
-                test_step += 1
+                writer.add_scalar('Test/threshold', self.eval_threshold, self.test_log_step)
+                writer.add_scalar('Test/min_dcf', self.eval_min_dcf, self.test_log_step)
+                writer.add_scalar('Test/eer', self.eval_eer, self.test_log_step)
+                self.test_log_step += 1
                 self.model.train()
                 # # 保存最优模型
-                if eer <= best_eer:
-                    best_eer = eer
-                    self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id, best_eer=eer,
-                                           best_model=True)
+                if self.eval_eer <= best_eer:
+                    best_eer = self.eval_eer
+                    self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id, eer=self.eval_eer,
+                                           min_dcf=self.eval_min_dcf, threshold=self.eval_threshold, best_model=True)
             if local_rank == 0:
                 # 保存模型
-                self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id, best_eer=eer)
+                self.__save_checkpoint(save_model_path=save_model_path, epoch_id=epoch_id, eer=self.eval_eer,
+                                       min_dcf=self.eval_min_dcf, threshold=self.eval_threshold)
 
     def evaluate(self, resume_model=None, save_image_path=None):
         """
         评估模型
         :param resume_model: 所使用的模型
         :param save_image_path: 保存图片的路径
         :return: 评估结果
@@ -475,27 +503,29 @@
         else:
             eval_model = self.model if len(self.model) == 1 else self.model[0]
 
         # 获取注册的声纹特征和标签
         enroll_features, enroll_labels = None, None
         with paddle.no_grad():
             for batch_id, (audio, label, input_lens_ratio) in enumerate(tqdm(self.enroll_loader, desc="注册音频声纹特征")):
+                if self.stop_eval: break
                 audio_features, _ = self.audio_featurizer(audio, input_lens_ratio)
                 if self.configs.use_model == 'EcapaTdnn':
                     feature = eval_model([audio_features, input_lens_ratio]).numpy()
                 else:
                     feature = eval_model(audio_features).numpy()
                 label = label.numpy()
                 # 存放特征
                 enroll_features = np.concatenate((enroll_features, feature)) if enroll_features is not None else feature
                 enroll_labels = np.concatenate((enroll_labels, label)) if enroll_labels is not None else label
         # 获取检验的声纹特征和标签
         trials_features, trials_labels = None, None
         with paddle.no_grad():
             for batch_id, (audio, label, input_lens_ratio) in enumerate(tqdm(self.trials_loader, desc="验证音频声纹特征")):
+                if self.stop_eval: break
                 audio_features, _ = self.audio_featurizer(audio, input_lens_ratio)
                 if self.configs.use_model == 'EcapaTdnn':
                     feature = eval_model([audio_features, input_lens_ratio]).numpy()
                 else:
                     feature = eval_model(audio_features).numpy()
                 label = label.numpy()
                 # 存放特征
@@ -503,20 +533,22 @@
                 trials_labels = np.concatenate((trials_labels, label)) if trials_labels is not None else label
         self.model.train()
         enroll_labels = enroll_labels.astype(np.int32)
         trials_labels = trials_labels.astype(np.int32)
         print('开始对比音频特征...')
         all_score, all_labels = [], []
         for i in tqdm(range(len(trials_features)), desc='特征对比'):
+            if self.stop_eval: break
             trials_feature = np.expand_dims(trials_features[i], 0).repeat(len(enroll_features), axis=0)
             score = cosine_similarity(trials_feature, enroll_features).tolist()[0]
             trials_label = np.expand_dims(trials_labels[i], 0).repeat(len(enroll_features), axis=0)
             y_true = np.array(enroll_labels == trials_label).astype(np.int32).tolist()
             all_score.extend(score)
             all_labels.extend(y_true)
+        if self.stop_eval: return -1, -1, -1,
         # 计算EER
         all_score = np.array(all_score)
         all_labels = np.array(all_labels)
         fnr, fpr, thresholds = compute_fnr_fpr(all_score, all_labels)
         eer, threshold = compute_eer(fnr, fpr, all_score)
         min_dcf = compute_dcf(fnr, fpr)
 
@@ -532,15 +564,15 @@
             plt.grid(True)  # 显示网格线
             # 保存图像
             os.makedirs(save_image_path, exist_ok=True)
             plt.savefig(os.path.join(save_image_path, 'result.png'))
             logger.info(f"结果图以保存在：{os.path.join(save_image_path, 'result.png')}")
         return eer, min_dcf, threshold
 
-    def export(self, save_model_path='models/', resume_model='models/EcapaTdnn_Fbank/best_model/'):
+    def export(self, save_model_path='models/', resume_model='models/CAMPPlus_Fbank/best_model/'):
         """
         导出预测模型
         :param save_model_path: 模型保存的路径
         :param resume_model: 准备转换的模型路径
         :return:
         """
         # 获取模型
```

## ppvector/data_utils/audio.py

```diff
@@ -44,16 +44,16 @@
 
     def __ne__(self, other):
         """返回两个对象是否不相等"""
         return not self.__eq__(other)
 
     def __str__(self):
         """返回该音频的信息"""
-        return ("%s: num_samples=%d, sample_rate=%d, duration=%.2fsec, "
-                "rms=%.2fdB" % (type(self), self.num_samples, self.sample_rate, self.duration, self.rms_db))
+        return (f"{type(self)}: num_samples={self.num_samples}, sample_rate={self.sample_rate}, "
+                f"duration={self.duration:.2f}sec, rms={self.rms_db:.2f}dB")
 
     @classmethod
     def from_file(cls, file):
         """从音频文件创建音频段
 
         :param file: 文件路径，或者文件对象
         :type file: str, BufferedReader
@@ -91,17 +91,17 @@
         # 从末尾开始计
         if start < 0.0: start += duration
         if end < 0.0: end += duration
         # 保证数据不越界
         if start < 0.0: start = 0.0
         if end > duration: end = duration
         if end < 0.0:
-            raise ValueError("切片结束位置(%f s)越界" % end)
+            raise ValueError(f"切片结束位置({end} s)越界")
         if start > end:
-            raise ValueError("切片开始位置(%f s)晚于切片结束位置(%f s)" % (start, end))
+            raise ValueError(f"切片开始位置({start} s)晚于切片结束位置({end} s)")
         start_frame = int(start * sample_rate)
         end_frame = int(end * sample_rate)
         sndfile.seek(start_frame)
         data = sndfile.read(frames=end_frame - start_frame, dtype='float32')
         return cls(data, sample_rate)
 
     @classmethod
@@ -216,15 +216,15 @@
 
         :param other: 包含样品的片段被添加进去
         :type other: AudioSegments
         :raise TypeError: 如果两个片段的类型不匹配
         :raise ValueError: 不能添加不同类型的段
         """
         if not isinstance(other, type(self)):
-            raise TypeError("不能添加不同类型的段: %s 和 %s" % (type(self), type(other)))
+            raise TypeError(f"不能添加不同类型的段: {type(self)} 和 {type(other)}")
         if self._sample_rate != other._sample_rate:
             raise ValueError("采样率必须匹配才能添加片段")
         if len(self._samples) != len(other._samples):
             raise ValueError("段长度必须匹配才能添加段")
         self._samples += other._samples
 
     def to_bytes(self, dtype='float32'):
@@ -252,15 +252,15 @@
         return samples
 
     def gain_db(self, gain):
         """对音频施加分贝增益。
 
         Note that this is an in-place transformation.
 
-        :param gain: Gain in decibels to apply to samples. 
+        :param gain: Gain in decibels to apply to samples.
         :type gain: float|1darray
         """
         self._samples *= 10. ** (gain / 20.)
 
     def change_speed(self, speed_rate):
         """通过线性插值改变音频速度
 
@@ -292,19 +292,17 @@
                             normalization. This is to prevent nans when
                             attempting to normalize a signal consisting of
                             all zeros.
         :type max_gain_db: float
         :raises ValueError: If the required gain to normalize the segment to
                             the target_db value exceeds max_gain_db.
         """
-        if -np.inf == self.rms_db: return
         gain = target_db - self.rms_db
         if gain > max_gain_db:
-            raise ValueError(
-                "无法将段规范化到 %f dB，因为可能的增益已经超过max_gain_db (%f dB)" % (target_db, max_gain_db))
+            raise ValueError(f"无法将段规范化到{target_db}dB，音频增益{gain}增益已经超过max_gain_db ({max_gain_db}dB)")
         self.gain_db(min(max_gain_db, target_db - self.rms_db))
 
     def resample(self, target_sample_rate, filter='kaiser_best'):
         """按目标采样率重新采样音频
 
         Note that this is an in-place transformation.
 
@@ -337,15 +335,15 @@
         if sides == "beginning":
             padded = cls.concatenate(silence, self)
         elif sides == "end":
             padded = cls.concatenate(self, silence)
         elif sides == "both":
             padded = cls.concatenate(silence, self, silence)
         else:
-            raise ValueError("Unknown value for the sides %s" % sides)
+            raise ValueError(f"Unknown value for the sides {sides}")
         self._samples = padded._samples
 
     def shift(self, shift_ms):
         """音频偏移。如果shift_ms为正，则随时间提前移位;如果为负，则随时间延迟移位。填补静音以保持持续时间不变。
 
         Note that this is an in-place transformation.
 
@@ -381,21 +379,21 @@
         start_sec = 0.0 if start_sec is None else start_sec
         end_sec = self.duration if end_sec is None else end_sec
         if start_sec < 0.0:
             start_sec = self.duration + start_sec
         if end_sec < 0.0:
             end_sec = self.duration + end_sec
         if start_sec < 0.0:
-            raise ValueError("切片起始位置(%f s)越界" % start_sec)
+            raise ValueError(f"切片起始位置({start_sec} s)越界")
         if end_sec < 0.0:
-            raise ValueError("切片结束位置(%f s)越界" % end_sec)
+            raise ValueError(f"切片结束位置({end_sec} s)越界")
         if start_sec > end_sec:
-            raise ValueError("切片的起始位置(%f s)晚于结束位置(%f s)" % (start_sec, end_sec))
+            raise ValueError(f"切片的起始位置({start_sec} s)晚于结束位置({end_sec} s)")
         if end_sec > self.duration:
-            raise ValueError("切片结束位置(%f s)越界(> %f s)" % (end_sec, self.duration))
+            raise ValueError(f"切片结束位置({end_sec} s)越界(> {self.duration} s)")
         start_sample = int(round(start_sec * self._sample_rate))
         end_sample = int(round(end_sec * self._sample_rate))
         self._samples = self._samples[start_sample:end_sample]
 
     def random_subsegment(self, subsegment_length):
         """随机剪切指定长度的音频片段
 
@@ -429,17 +427,17 @@
                             to apply infinite gain to a zero signal.
         :type max_gain_db: float
         :raises ValueError: If the sample rate does not match between the two
                             audio segments, or if the duration of noise segments
                             is shorter than original audio segments.
         """
         if noise.sample_rate != self.sample_rate:
-            raise ValueError("噪声采样率(%d Hz)不等于基信号采样率(%d Hz)" % (noise.sample_rate, self.sample_rate))
+            raise ValueError(f"噪声采样率({noise.sample_rate} Hz)不等于基信号采样率({self.sample_rate} Hz)")
         if noise.duration < self.duration:
-            raise ValueError("噪声信号(%f秒)必须至少与基信号(%f秒)一样长" % (noise.duration, self.duration))
+            raise ValueError(f"噪声信号({noise.duration}秒)必须至少与基信号({self.duration}秒)一样长")
         noise_gain_db = min(self.rms_db - noise.rms_db - snr_dB, max_gain_db)
         noise_new = copy.deepcopy(noise)
         noise_new.random_subsegment(self.duration)
         noise_new.gain_db(noise_gain_db)
         self.superimpose(noise_new)
 
     def vad(self, top_db=20, overlap=200):
@@ -494,33 +492,37 @@
         """返回以分贝为单位的音频均方根能量
 
         :return: Root mean square energy in decibels.
         :rtype: float
         """
         # square root => multiply by 10 instead of 20 for dBs
         mean_square = np.mean(self._samples ** 2)
+        if mean_square == 0:
+            mean_square = 1
         return 10 * np.log10(mean_square)
 
-    def _convert_samples_to_float32(self, samples):
+    @staticmethod
+    def _convert_samples_to_float32(samples):
         """Convert sample type to float32.
 
         Audio sample type is usually integer or float-point.
         Integers will be scaled to [-1, 1] in float32.
         """
         float32_samples = samples.astype('float32')
         if samples.dtype in np.sctypes['int']:
             bits = np.iinfo(samples.dtype).bits
             float32_samples *= (1. / 2 ** (bits - 1))
         elif samples.dtype in np.sctypes['float']:
             pass
         else:
-            raise TypeError("Unsupported sample type: %s." % samples.dtype)
+            raise TypeError(f"Unsupported sample type: {samples.dtype}.")
         return float32_samples
 
-    def _convert_samples_from_float32(self, samples, dtype):
+    @staticmethod
+    def _convert_samples_from_float32(samples, dtype):
         """Convert sample type from float32 to dtype.
 
         Audio sample type is usually integer or float-point. For integer
         type, float32 will be rescaled from [-1, 1] to the maximum range
         supported by the integer type.
 
         This is for writing a audio file.
@@ -536,9 +538,9 @@
             output_samples[output_samples < min_val] = min_val
         elif samples.dtype in np.sctypes['float']:
             min_val = np.finfo(dtype).min
             max_val = np.finfo(dtype).max
             output_samples[output_samples > max_val] = max_val
             output_samples[output_samples < min_val] = min_val
         else:
-            raise TypeError("Unsupported sample type: %s." % samples.dtype)
+            raise TypeError(f"Unsupported sample type: {samples.dtype}.")
         return output_samples.astype(dtype)
```

## ppvector/data_utils/utils.py

```diff
@@ -1,10 +1,11 @@
 import io
 import itertools
 
+import gc
 import av
 import librosa
 import numpy as np
 import paddle
 
 
 def vad(wav, top_db=20, overlap=200):
@@ -77,25 +78,28 @@
       A float32 Numpy array.
     """
     resampler = av.audio.resampler.AudioResampler(format="s16", layout="mono", rate=sample_rate)
 
     raw_buffer = io.BytesIO()
     dtype = None
 
-    with av.open(file, metadata_errors="ignore") as container:
+    with av.open(file, mode="r", metadata_errors="ignore") as container:
         frames = container.decode(audio=0)
         frames = _ignore_invalid_frames(frames)
         frames = _group_frames(frames, 500000)
         frames = _resample_frames(frames, resampler)
 
         for frame in frames:
             array = frame.to_ndarray()
             dtype = array.dtype
             raw_buffer.write(array)
 
+    del resampler
+    gc.collect()
+
     audio = np.frombuffer(raw_buffer.getbuffer(), dtype=dtype)
 
     # Convert s16 back to f32.
     return audio.astype(np.float32) / 32768.0
 
 
 def _ignore_invalid_frames(frames):
```

## ppvector/models/fc.py

```diff
@@ -25,28 +25,28 @@
         self.blocks = nn.LayerList()
 
         for index in range(num_blocks):
             self.blocks.append(DenseLayer(input_dim, inter_dim, config_str='batchnorm'))
             input_dim = inter_dim
 
         if self.loss_type == 'AAMLoss' or self.loss_type == 'SubCenterLoss' or \
-                self.loss_type == 'AMLoss' or self.loss_type == 'ARMLoss':
+                self.loss_type == 'AMLoss' or self.loss_type == 'ARMLoss' or self.loss_type == 'SphereFace2':
             self.weight = paddle.create_parameter(shape=[input_dim, num_speakers * K],
                                                   dtype='float32',
                                                   attr=paddle.ParamAttr(initializer=nn.initializer.XavierUniform()), )
         else:
             self.output = nn.Linear(input_dim, num_speakers)
 
     def forward(self, x):
         # x: [B, dim]
         for layer in self.blocks:
             x = layer(x)
 
         # normalized
-        if self.loss_type == 'AAMLoss' or self.loss_type == 'SubCenterLoss':
+        if self.loss_type == 'AAMLoss' or self.loss_type == 'SubCenterLoss' or self.loss_type == 'SphereFace2':
             logits = F.linear(F.normalize(x), F.normalize(self.weight, axis=0))
         elif self.loss_type == 'AMLoss' or self.loss_type == 'ARMLoss':
             x_norm = paddle.norm(x, p=2, axis=1, keepdim=True).clip(min=1e-12)
             x_norm = paddle.divide(x, x_norm)
             w_norm = paddle.norm(self.weight, p=2, axis=0, keepdim=True).clip(min=1e-12)
             w_norm = paddle.divide(self.weight, w_norm)
             logits = paddle.mm(x_norm, w_norm)
```

## ppvector/models/loss.py

```diff
@@ -22,19 +22,19 @@
         self.criterion = nn.CrossEntropyLoss()
 
         self.update(margin)
 
     def forward(self, cosine, label):
         """
         Args:
-            cosine (torch.Tensor): cosine distance between the two tensors, shape [batch, num_classes].
-            label (torch.Tensor): label of speaker id, shape [batch, ].
+            cosine (paddle.Tensor): cosine distance between the two tensors, shape [batch, num_classes].
+            label (paddle.Tensor): label of speaker id, shape [batch, ].
 
         Returns:
-            torch.Tensor: loss value.
+            paddle.Tensor: loss value.
         """
         sine = paddle.sqrt(1.0 - paddle.pow(cosine, 2))
         phi = cosine * self.cos_m - sine * self.sin_m
         if self.easy_margin:
             phi = paddle.where(cosine > 0, phi, cosine)
         else:
             phi = paddle.where(cosine > self.th, phi, cosine - self.mmm)
@@ -52,14 +52,83 @@
         self.sin_m = math.sin(margin)
         self.th = math.cos(math.pi - margin)
         self.mm = math.sin(math.pi - margin) * margin
         self.m = self.margin
         self.mmm = 1.0 + math.cos(math.pi - margin)
 
 
+class SphereFace2(nn.Layer):
+    def __init__(self, margin=0.2, scale=32.0, lanbuda=0.7, t=3, margin_type='C'):
+        """Implement of sphereface2 for speaker verification:
+            Reference:
+                [1] Exploring Binary Classification Loss for Speaker Verification
+                https://ieeexplore.ieee.org/abstract/document/10094954
+                [2] Sphereface2: Binary classification is all you need for deep face recognition
+                https://arxiv.org/pdf/2108.01513
+            Args:
+                scale: norm of input feature
+                margin: margin
+                lanbuda: weight of positive and negative pairs
+                t: parameter for adjust score distribution
+                margin_type: A:cos(theta+margin) or C:cos(theta)-margin
+            Recommend margin:
+                training: 0.2 for C and 0.15 for A
+                LMF: 0.3 for C and 0.25 for A
+        """
+        super(SphereFace2, self).__init__()
+        self.scale = scale
+        self.bias = paddle.create_parameter([1, 1], dtype=paddle.float32, is_bias=True)
+        self.t = t
+        self.lanbuda = lanbuda
+        self.margin_type = margin_type
+
+        self.update(margin)
+
+    def fun_g(self, z, t: int):
+        gz = 2 * paddle.pow((z + 1) / 2, t) - 1
+        return gz
+
+    def forward(self, cosine, label):
+        """
+        Args:
+            cosine (paddle.Tensor): cosine distance between the two tensors, shape [batch, num_classes].
+            label (paddle.Tensor): label of speaker id, shape [batch, ].
+
+        Returns:
+            paddle.Tensor: loss value.
+        """
+        if self.margin_type == 'A':  # arcface type
+            sin = paddle.sqrt(1.0 - paddle.pow(cosine, 2))
+            cos_m_theta_p = self.scale * self.fun_g(
+                paddle.where(cosine > self.th, cosine * self.cos_m - sin * self.sin_m, cosine - self.mmm), self.t) + \
+                            self.bias[0][0]
+            cos_m_theta_n = self.scale * self.fun_g(cosine * self.cos_m + sin * self.sin_m, self.t) + self.bias[0][0]
+            cos_p_theta = self.lanbuda * paddle.log(1 + paddle.exp(-1.0 * cos_m_theta_p))
+            cos_n_theta = (1 - self.lanbuda) * paddle.log(1 + paddle.exp(cos_m_theta_n))
+        else:
+            # cosface type
+            cos_m_theta_p = self.scale * (self.fun_g(cosine, self.t) - self.margin) + self.bias[0][0]
+            cos_m_theta_n = self.scale * (self.fun_g(cosine, self.t) + self.margin) + self.bias[0][0]
+            cos_p_theta = self.lanbuda * paddle.log(1 + paddle.exp(-1.0 * cos_m_theta_p))
+            cos_n_theta = (1 - self.lanbuda) * paddle.log(1 + paddle.exp(cos_m_theta_n))
+
+        target_mask = F.one_hot(label, cosine.shape[1])
+        nontarget_mask = 1 - target_mask
+        loss = (target_mask * cos_p_theta + nontarget_mask * cos_n_theta).sum(1).mean()
+        return loss
+
+    def update(self, margin=0.2):
+        self.margin = margin
+        self.cos_m = math.cos(margin)
+        self.sin_m = math.sin(margin)
+        self.th = math.cos(math.pi - margin)
+        self.mm = math.sin(math.pi - margin)
+        self.mmm = 1.0 + math.cos(math.pi - margin)
+
+
 class AMLoss(nn.Layer):
     def __init__(self, margin=0.2, scale=30):
         super(AMLoss, self).__init__()
         self.m = margin
         self.s = scale
         self.criterion = paddle.nn.CrossEntropyLoss(reduction="sum")
```

## ppvector/utils/record.py

```diff
@@ -1,8 +1,9 @@
 import os
+import time
 
 import soundcard
 import soundfile
 
 
 class RecordAudio:
     def __init__(self, channels=1, sample_rate=16000):
@@ -18,14 +19,18 @@
 
         :param record_seconds: 录音时间，默认3秒
         :param save_path: 录音保存的路径，后缀名为wav
         :return: 音频的numpy数据
         """
         print("开始录音......")
         num_frames = int(record_seconds * self.sample_rate)
+        start_time = time.time()
         data = self.default_mic.record(samplerate=self.sample_rate, numframes=num_frames, channels=self.channels)
+        if int(time.time() - start_time) < record_seconds:
+            raise Exception('录音错误，请检查录音设备，或者卸载soundfile，使用命令重新安装：'
+                            'pip install git+https://github.com/bastibe/SoundCard.git')
         audio_data = data.squeeze()
         print("录音已结束!")
         if save_path is not None:
             os.makedirs(os.path.dirname(save_path), exist_ok=True)
             soundfile.write(save_path, data=data, samplerate=self.sample_rate)
         return audio_data
```

## Comparing `ppvector-1.0.0.dist-info/LICENSE` & `ppvector-1.0.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `ppvector-1.0.0.dist-info/METADATA` & `ppvector-1.0.2.dist-info/METADATA`

 * *Files 18% similar despite different names*

```diff
@@ -1,373 +1,461 @@
-Metadata-Version: 2.1
-Name: ppvector
-Version: 1.0.0
-Summary: Voice Print Recognition toolkit on PaddlePaddle
-Home-page: https://github.com/yeyupiaoling/VoiceprintRecognition_PaddlePaddle
-Download-URL: https://github.com/yeyupiaoling/VoiceprintRecognition_PaddlePaddle.git
-Author: yeyupiaoling
-License: Apache License 2.0
-Keywords: Voice,paddle
-Platform: UNKNOWN
-Classifier: Intended Audience :: Developers
-Classifier: License :: OSI Approved :: Apache Software License
-Classifier: Operating System :: OS Independent
-Classifier: Natural Language :: Chinese (Simplified)
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.5
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Topic :: Utilities
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: numba (>=0.52.0)
-Requires-Dist: librosa (>=0.9.1)
-Requires-Dist: numpy (>=1.19.2)
-Requires-Dist: tqdm (>=4.59.0)
-Requires-Dist: visualdl (>=2.1.1)
-Requires-Dist: resampy (>=0.2.2)
-Requires-Dist: soundfile (>=0.12.1)
-Requires-Dist: soundcard (>=0.4.2)
-Requires-Dist: pyyaml (>=5.4.1)
-Requires-Dist: termcolor (>=1.1.0)
-Requires-Dist: typeguard (==2.13.3)
-Requires-Dist: paddleaudio (>=1.0.1)
-Requires-Dist: scikit-learn (>=1.0.2)
-Requires-Dist: pydub (>=0.25.1)
-Requires-Dist: av (>=10.0.0)
-
-# 基于PaddlePaddle实现的声纹识别系统
-
-![python version](https://img.shields.io/badge/python-3.8+-orange.svg)
-![GitHub forks](https://img.shields.io/github/forks/yeyupiaoling/VoiceprintRecognition-PaddlePaddle)
-![GitHub Repo stars](https://img.shields.io/github/stars/yeyupiaoling/VoiceprintRecognition-PaddlePaddle)
-![GitHub](https://img.shields.io/github/license/yeyupiaoling/VoiceprintRecognition-PaddlePaddle)
-![支持系统](https://img.shields.io/badge/支持系统-Win/Linux/MAC-9cf)
-
-本分支为1.0版本，如果要使用之前的0.3版本请在[0.x分支](https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle/tree/release/0.x)使用。本项目使用了EcapaTdnn、ResNetSE、ERes2Net、CAM++等多种先进的声纹识别模型，不排除以后会支持更多模型，同时本项目也支持了MelSpectrogram、Spectrogram、MFCC、Fbank等多种数据预处理方法，使用了ArcFace Loss，ArcFace loss：Additive Angular Margin Loss（加性角度间隔损失函数），对应项目中的AAMLoss，对特征向量和权重归一化，对θ加上角度间隔m，角度间隔比余弦间隔在对角度的影响更加直接，除此之外，还支持AMLoss、ARMLoss、CELoss等多种损失函数。
-
-**欢迎大家扫码入知识星球或者QQ群讨论，知识星球里面提供项目的模型文件和博主其他相关项目的模型文件，也包括其他一些资源。**
-
-<div align="center">
-  <img src="https://yeyupiaoling.cn/zsxq.png" alt="知识星球" width="400">
-  <img src="https://yeyupiaoling.cn/qq.png" alt="QQ群" width="400">
-</div>
-
-
-使用环境：
-
- - Anaconda 3
- - Python 3.8
- - PaddlePaddle 2.4.1
- - Windows 10 or Ubuntu 18.04
-
-# 项目特性
-
-1. 支持模型：EcapaTdnn、TDNN、Res2Net、ResNetSE、ERes2Net、CAM++
-2. 支持池化层：AttentiveStatsPool(ASP)、SelfAttentivePooling(SAP)、TemporalStatisticsPooling(TSP)、TemporalAveragePooling(TAP)、TemporalStatsPool(TSTP)
-3. 支持损失函数：AAMLoss、AMLoss、ARMLoss、CELoss
-4. 支持预处理方法：MelSpectrogram、Spectrogram、MFCC、Fbank
-
-**模型论文：**
-
-- EcapaTdnn：[ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification](https://arxiv.org/abs/2005.07143v3)
-- TDNN：[Prediction of speech intelligibility with DNN-based performance measures](https://arxiv.org/abs/2203.09148)
-- Res2Net：[Res2Net: A New Multi-scale Backbone Architecture](https://arxiv.org/abs/1904.01169)
-- ResNetSE：[Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)
-- CAMPPlus：[CAM++: A Fast and Efficient Network for Speaker Verification Using Context-Aware Masking](https://arxiv.org/abs/2303.00332v3)
-- ERes2Net：[An Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification](https://arxiv.org/abs/2305.12838v1)
-
-# 模型下载
-
-|    模型     | Params(M) | 预处理方法 |                数据集                 | train speakers | threshold |   EER   | MinDCF  |                                      模型下载                                       |
-|:---------:|:---------:|:-----:|:----------------------------------:|:--------------:|:---------:|:-------:|:-------:|:-------------------------------------------------------------------------------:|
-|   CAM++   |    7.5    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.25    | 0.09485 | 0.56214 | 加入知识星球获取/[CSDN下载](https://download.csdn.net/download/qq_33200967/88265940)(不建议) |
-| ERes2Net  |    8.2    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.22    | 0.09637 | 0.52627 |                                    加入知识星球获取                                     |
-| ResNetSE  |   10.7    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.19    | 0.10222 | 0.57981 |                                    加入知识星球获取                                     |
-| EcapaTdnn |    6.7    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.25    | 0.10465 | 0.58521 |                                    加入知识星球获取                                     |
-|   TDNN    |    3.2    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.23    | 0.11804 | 0.61070 |                                    加入知识星球获取                                     |
-|  Res2Net  |    7.2    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.18    | 0.14126 | 0.68511 |                                    加入知识星球获取                                     |
-|   CAM++   |    7.5    | Fbank |               更大数据集                |      2W+       |   0.34    | 0.07884 | 0.52738 |                                    加入知识星球获取                                     |
-| ERes2Net  |    8.2    | Fbank |               其他数据集                |      20W       |   0.36    | 0.02939 | 0.18355 |                                    加入知识星球获取                                     |
-|   CAM++   |    7.5    | Flank |               其他数据集                |      20W       |   0.29    | 0.04768 | 0.31429 |                                    加入知识星球获取                                     |
-
-说明：
-1. 评估的测试集为[CN-Celeb的测试集](https://aistudio.baidu.com/aistudio/datasetdetail/233361)，包含196个说话人。
-
-## 安装环境
-
- - 首先安装的是PaddlePaddle的GPU版本，如果已经安装过了，请跳过。
-```shell
-conda install paddlepaddle-gpu==2.4.1 cudatoolkit=10.2 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/
-```
-
- - 安装ppvector库。
- 
-使用pip安装，命令如下：
-```shell
-python -m pip install ppvector -U -i https://pypi.tuna.tsinghua.edu.cn/simple
-```
-
-**建议源码安装**，源码安装能保证使用最新代码。
-```shell
-git clone https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle.git
-cd VoiceprintRecognition-PaddlePaddle/
-pip install .
-```
-
-# 修改预处理方法
-
-配置文件中默认使用的是Fbank预处理方法，如果要使用其他预处理方法，可以修改配置文件中的安装下面方式修改，具体的值可以根据自己情况修改。如果不清楚如何设置参数，可以直接删除该部分，直接使用默认值。
-
-```yaml
-# 数据预处理参数
-preprocess_conf:
-  # 音频预处理方法，支持：LogMelSpectrogram、MelSpectrogram、Spectrogram、MFCC、Fbank
-  feature_method: 'Fbank'
-  # 设置API参数，更参数查看对应API，不清楚的可以直接删除该部分，直接使用默认值
-  method_args:
-    sr: 16000
-    n_mels: 80
-```
-
-# 训练模型
-使用`train.py`训练模型，本项目支持多个音频预处理方式，通过`configs/ecapa_tdnn.yml`配置文件的参数`preprocess_conf.feature_method`可以指定，`MelSpectrogram`为梅尔频谱，`Spectrogram`为语谱图，`MFCC`梅尔频谱倒谱系数。通过参数`augment_conf_path`可以指定数据增强方式。训练过程中，会使用VisualDL保存训练日志，通过启动VisualDL可以随时查看训练结果，启动命令`visualdl --logdir=log --host 0.0.0.0`
-```shell
-# 单卡训练
-CUDA_VISIBLE_DEVICES=0 python train.py
-# 多卡训练
-python -m paddle.distributed.launch --gpus '0,1' train.py
-```
-
-训练输出日志：
-```
-[2023-08-05 09:52:06.497988 INFO   ] utils:print_arguments:13 - ----------- 额外配置参数 -----------
-[2023-08-05 09:52:06.498094 INFO   ] utils:print_arguments:15 - configs: configs/ecapa_tdnn.yml
-[2023-08-05 09:52:06.498149 INFO   ] utils:print_arguments:15 - do_eval: True
-[2023-08-05 09:52:06.498191 INFO   ] utils:print_arguments:15 - local_rank: 0
-[2023-08-05 09:52:06.498230 INFO   ] utils:print_arguments:15 - pretrained_model: None
-[2023-08-05 09:52:06.498269 INFO   ] utils:print_arguments:15 - resume_model: None
-[2023-08-05 09:52:06.498306 INFO   ] utils:print_arguments:15 - save_model_path: models/
-[2023-08-05 09:52:06.498342 INFO   ] utils:print_arguments:15 - use_gpu: True
-[2023-08-05 09:52:06.498378 INFO   ] utils:print_arguments:16 - ------------------------------------------------
-[2023-08-05 09:52:06.513761 INFO   ] utils:print_arguments:18 - ----------- 配置文件参数 -----------
-[2023-08-05 09:52:06.513906 INFO   ] utils:print_arguments:21 - dataset_conf:
-[2023-08-05 09:52:06.513957 INFO   ] utils:print_arguments:24 -         dataLoader:
-[2023-08-05 09:52:06.513995 INFO   ] utils:print_arguments:26 -                 batch_size: 64
-[2023-08-05 09:52:06.514031 INFO   ] utils:print_arguments:26 -                 num_workers: 4
-[2023-08-05 09:52:06.514066 INFO   ] utils:print_arguments:28 -         do_vad: False
-[2023-08-05 09:52:06.514101 INFO   ] utils:print_arguments:28 -         enroll_list: dataset/enroll_list.txt
-[2023-08-05 09:52:06.514135 INFO   ] utils:print_arguments:24 -         eval_conf:
-[2023-08-05 09:52:06.514169 INFO   ] utils:print_arguments:26 -                 batch_size: 1
-[2023-08-05 09:52:06.514203 INFO   ] utils:print_arguments:26 -                 max_duration: 20
-[2023-08-05 09:52:06.514237 INFO   ] utils:print_arguments:28 -         max_duration: 3
-[2023-08-05 09:52:06.514274 INFO   ] utils:print_arguments:28 -         min_duration: 0.5
-[2023-08-05 09:52:06.514308 INFO   ] utils:print_arguments:28 -         noise_aug_prob: 0.2
-[2023-08-05 09:52:06.514342 INFO   ] utils:print_arguments:28 -         noise_dir: dataset/noise
-[2023-08-05 09:52:06.514374 INFO   ] utils:print_arguments:28 -         num_speakers: 3242
-[2023-08-05 09:52:06.514408 INFO   ] utils:print_arguments:28 -         sample_rate: 16000
-[2023-08-05 09:52:06.514441 INFO   ] utils:print_arguments:28 -         speed_perturb: True
-[2023-08-05 09:52:06.514475 INFO   ] utils:print_arguments:28 -         target_dB: -20
-[2023-08-05 09:52:06.514508 INFO   ] utils:print_arguments:28 -         train_list: dataset/train_list.txt
-[2023-08-05 09:52:06.514542 INFO   ] utils:print_arguments:28 -         trials_list: dataset/trials_list.txt
-[2023-08-05 09:52:06.514575 INFO   ] utils:print_arguments:28 -         use_dB_normalization: True
-[2023-08-05 09:52:06.514609 INFO   ] utils:print_arguments:21 - loss_conf:
-[2023-08-05 09:52:06.514643 INFO   ] utils:print_arguments:24 -         args:
-[2023-08-05 09:52:06.514678 INFO   ] utils:print_arguments:26 -                 easy_margin: False
-[2023-08-05 09:52:06.514713 INFO   ] utils:print_arguments:26 -                 margin: 0.2
-[2023-08-05 09:52:06.514746 INFO   ] utils:print_arguments:26 -                 scale: 32
-[2023-08-05 09:52:06.514779 INFO   ] utils:print_arguments:24 -         margin_scheduler_args:
-[2023-08-05 09:52:06.514814 INFO   ] utils:print_arguments:26 -                 final_margin: 0.3
-[2023-08-05 09:52:06.514848 INFO   ] utils:print_arguments:28 -         use_loss: AAMLoss
-[2023-08-05 09:52:06.514882 INFO   ] utils:print_arguments:28 -         use_margin_scheduler: True
-[2023-08-05 09:52:06.514915 INFO   ] utils:print_arguments:21 - model_conf:
-[2023-08-05 09:52:06.514950 INFO   ] utils:print_arguments:24 -         backbone:
-[2023-08-05 09:52:06.514984 INFO   ] utils:print_arguments:26 -                 embd_dim: 192
-[2023-08-05 09:52:06.515017 INFO   ] utils:print_arguments:26 -                 pooling_type: ASP
-[2023-08-05 09:52:06.515050 INFO   ] utils:print_arguments:24 -         classifier:
-[2023-08-05 09:52:06.515084 INFO   ] utils:print_arguments:26 -                 num_blocks: 0
-[2023-08-05 09:52:06.515118 INFO   ] utils:print_arguments:21 - optimizer_conf:
-[2023-08-05 09:52:06.515154 INFO   ] utils:print_arguments:28 -         learning_rate: 0.001
-[2023-08-05 09:52:06.515188 INFO   ] utils:print_arguments:28 -         optimizer: Adam
-[2023-08-05 09:52:06.515221 INFO   ] utils:print_arguments:28 -         scheduler: CosineAnnealingLR
-[2023-08-05 09:52:06.515254 INFO   ] utils:print_arguments:28 -         scheduler_args: None
-[2023-08-05 09:52:06.515289 INFO   ] utils:print_arguments:28 -         weight_decay: 1e-06
-[2023-08-05 09:52:06.515323 INFO   ] utils:print_arguments:21 - preprocess_conf:
-[2023-08-05 09:52:06.515357 INFO   ] utils:print_arguments:28 -         feature_method: MelSpectrogram
-[2023-08-05 09:52:06.515390 INFO   ] utils:print_arguments:24 -         method_args:
-[2023-08-05 09:52:06.515426 INFO   ] utils:print_arguments:26 -                 f_max: 14000.0
-[2023-08-05 09:52:06.515460 INFO   ] utils:print_arguments:26 -                 f_min: 50.0
-[2023-08-05 09:52:06.515493 INFO   ] utils:print_arguments:26 -                 hop_length: 320
-[2023-08-05 09:52:06.515527 INFO   ] utils:print_arguments:26 -                 n_fft: 1024
-[2023-08-05 09:52:06.515560 INFO   ] utils:print_arguments:26 -                 n_mels: 64
-[2023-08-05 09:52:06.515593 INFO   ] utils:print_arguments:26 -                 sample_rate: 16000
-[2023-08-05 09:52:06.515626 INFO   ] utils:print_arguments:26 -                 win_length: 1024
-[2023-08-05 09:52:06.515660 INFO   ] utils:print_arguments:21 - train_conf:
-[2023-08-05 09:52:06.515694 INFO   ] utils:print_arguments:28 -         log_interval: 100
-[2023-08-05 09:52:06.515728 INFO   ] utils:print_arguments:28 -         max_epoch: 30
-[2023-08-05 09:52:06.515761 INFO   ] utils:print_arguments:30 - use_model: EcapaTdnn
-[2023-08-05 09:52:06.515794 INFO   ] utils:print_arguments:31 - ------------------------------------------------
-----------------------------------------------------------------------------------------
-        Layer (type)             Input Shape          Output Shape         Param #    
-========================================================================================
-          Conv1D-2              [[1, 64, 102]]        [1, 512, 98]         164,352    
-          Conv1d-1              [[1, 64, 98]]         [1, 512, 98]            0       
-           ReLU-1               [[1, 512, 98]]        [1, 512, 98]            0       
-       BatchNorm1D-2            [[1, 512, 98]]        [1, 512, 98]          2,048     
-       BatchNorm1d-1            [[1, 512, 98]]        [1, 512, 98]            0       
-        TDNNBlock-1             [[1, 64, 98]]         [1, 512, 98]            0       
-          Conv1D-4              [[1, 512, 98]]        [1, 512, 98]         262,656    
-          Conv1d-3              [[1, 512, 98]]        [1, 512, 98]            0       
-           ReLU-2               [[1, 512, 98]]        [1, 512, 98]            0       
-       BatchNorm1D-4            [[1, 512, 98]]        [1, 512, 98]          2,048     
-       BatchNorm1d-3            [[1, 512, 98]]        [1, 512, 98]            0       
-        TDNNBlock-2             [[1, 512, 98]]        [1, 512, 98]            0       
-··········································
-         SEBlock-3           [[1, 512, 98], None]     [1, 512, 98]            0       
-      SERes2NetBlock-3          [[1, 512, 98]]        [1, 512, 98]            0       
-         Conv1D-70             [[1, 1536, 98]]       [1, 1536, 98]        2,360,832   
-         Conv1d-69             [[1, 1536, 98]]       [1, 1536, 98]            0       
-          ReLU-32              [[1, 1536, 98]]       [1, 1536, 98]            0       
-       BatchNorm1D-58          [[1, 1536, 98]]       [1, 1536, 98]          6,144     
-       BatchNorm1d-57          [[1, 1536, 98]]       [1, 1536, 98]            0       
-        TDNNBlock-29           [[1, 1536, 98]]       [1, 1536, 98]            0       
-         Conv1D-72             [[1, 4608, 98]]        [1, 128, 98]         589,952    
-         Conv1d-71             [[1, 4608, 98]]        [1, 128, 98]            0       
-          ReLU-33               [[1, 128, 98]]        [1, 128, 98]            0       
-       BatchNorm1D-60           [[1, 128, 98]]        [1, 128, 98]           512      
-       BatchNorm1d-59           [[1, 128, 98]]        [1, 128, 98]            0       
-        TDNNBlock-30           [[1, 4608, 98]]        [1, 128, 98]            0       
-           Tanh-1               [[1, 128, 98]]        [1, 128, 98]            0       
-         Conv1D-74              [[1, 128, 98]]       [1, 1536, 98]         198,144    
-         Conv1d-73              [[1, 128, 98]]       [1, 1536, 98]            0       
-AttentiveStatisticsPooling-1   [[1, 1536, 98]]        [1, 3072, 1]            0       
-       BatchNorm1D-62           [[1, 3072, 1]]        [1, 3072, 1]         12,288     
-       BatchNorm1d-61           [[1, 3072, 1]]        [1, 3072, 1]            0       
-         Conv1D-76              [[1, 3072, 1]]        [1, 192, 1]          590,016    
-         Conv1d-75              [[1, 3072, 1]]        [1, 192, 1]             0       
-        EcapaTdnn-1             [[1, 98, 64]]           [1, 192]              0       
-  SpeakerIdentification-1         [[1, 192]]           [1, 9726]          1,867,392   
-========================================================================================
-Total params: 8,039,808
-Trainable params: 8,020,480
-Non-trainable params: 19,328
-----------------------------------------------------------------------------------------
-Input size (MB): 0.02
-Forward/backward pass size (MB): 35.60
-Params size (MB): 30.67
-Estimated Total Size (MB): 66.30
-----------------------------------------------------------------------------------------
-[2023-08-05 09:52:08.084231 INFO   ] trainer:train:388 - 训练数据：874175
-[2023-08-05 09:52:09.186542 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [0/13659], loss: 11.95824, accuracy: 0.00000, learning rate: 0.00100000, speed: 58.09 data/sec, eta: 5 days, 5:24:08
-[2023-08-05 09:52:22.477905 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [100/13659], loss: 10.35675, accuracy: 0.00278, learning rate: 0.00100000, speed: 481.65 data/sec, eta: 15:07:15
-[2023-08-05 09:52:35.948581 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [200/13659], loss: 10.22089, accuracy: 0.00505, learning rate: 0.00100000, speed: 475.27 data/sec, eta: 15:19:12
-[2023-08-05 09:52:49.249098 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [300/13659], loss: 10.00268, accuracy: 0.00706, learning rate: 0.00100000, speed: 481.45 data/sec, eta: 15:07:11
-[2023-08-05 09:53:03.716015 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [400/13659], loss: 9.76052, accuracy: 0.00830, learning rate: 0.00100000, speed: 442.74 data/sec, eta: 16:26:16
-[2023-08-05 09:53:18.258807 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [500/13659], loss: 9.50189, accuracy: 0.01060, learning rate: 0.00100000, speed: 440.46 data/sec, eta: 16:31:08
-[2023-08-05 09:53:31.618354 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [600/13659], loss: 9.26083, accuracy: 0.01256, learning rate: 0.00100000, speed: 479.50 data/sec, eta: 15:10:12
-[2023-08-05 09:53:45.439642 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [700/13659], loss: 9.03548, accuracy: 0.01449, learning rate: 0.00099999, speed: 463.63 data/sec, eta: 15:41:08
-```
-
-VisualDL页面：
-![VisualDL页面](./docs/images/log.jpg)
-
-
-# 评估模型
-训练结束之后会保存预测模型，我们用预测模型来预测测试集中的音频特征，然后使用音频特征进行两两对比，计算EER和MinDCF。
-```shell
-python eval.py
-```
-
-输出类似如下：
-```
-······
-------------------------------------------------
-W0425 08:27:32.057426 17654 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
-W0425 08:27:32.065165 17654 device_context.cc:465] device: 0, cuDNN Version: 7.6.
-[2023-03-16 20:20:47.195908 INFO   ] trainer:evaluate:341 - 成功加载模型：models/EcapaTdnn_Fbank/best_model/model.pth
-100%|███████████████████████████| 84/84 [00:28<00:00,  2.95it/s]
-开始两两对比音频特征...
-100%|███████████████████████████| 5332/5332 [00:05<00:00, 1027.83it/s]
-评估消耗时间：65s，threshold：0.26，EER: 0.14739, MinDCF: 0.41999
-```
-
-# 声纹对比
-下面开始实现声纹对比，创建`infer_contrast.py`程序，首先介绍几个重要的函数，`predict()`函数是可以获取声纹特征，`predict_batch()`函数是可以获取一批的声纹特征，`contrast()`函数可以对比两条音频的相似度，`register()`函数注册一条音频到声纹库里面，`recognition()`函输入一条音频并且从声纹库里面对比识别，`remove_user()`函数移除你好。声纹库里面的注册人。我们输入两个语音，通过预测函数获取他们的特征数据，使用这个特征数据可以求他们的对角余弦值，得到的结果可以作为他们相识度。对于这个相识度的阈值`threshold`，读者可以根据自己项目的准确度要求进行修改。
-```shell
-python infer_contrast.py --audio_path1=audio/a_1.wav --audio_path2=audio/b_2.wav
-```
-
-输出类似如下：
-```
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:13 - ----------- 额外配置参数 -----------
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - audio_path1: dataset/a_1.wav
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - audio_path2: dataset/b_2.wav
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - configs: configs/ecapa_tdnn.yml
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - model_path: models/EcapaTdnn_Fbank/best_model/
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - threshold: 0.6
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - use_gpu: True
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:16 - ------------------------------------------------
-······································································
-W0425 08:29:10.006249 21121 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
-W0425 08:29:10.008555 21121 device_context.cc:465] device: 0, cuDNN Version: 7.6.
-成功加载模型参数和优化方法参数：models/ecapa_tdnn/model.pdparams
-audio/a_1.wav 和 audio/b_2.wav 不是同一个人，相似度为：-0.09565544128417969
-```
-
-# 声纹识别
-
-在新闻识别里面主要使用到`register()`函数和`recognition()`函数，首先使用`register()`函数函数来注册音频到声纹库里面，也可以直接把文件添加到`audio_db`文件夹里面，使用的时候通过`recognition()`函数来发起识别，输入一条音频，就可以从声纹库里面识别到所需要的说话人。
-
-有了上面的声纹识别的函数，读者可以根据自己项目的需求完成声纹识别的方式，例如笔者下面提供的是通过录音来完成声纹识别。首先必须要加载语音库中的语音，语音库文件夹为`audio_db`，然后用户回车后录音3秒钟，然后程序会自动录音，并使用录音到的音频进行声纹识别，去匹配语音库中的语音，获取用户的信息。通过这样方式，读者也可以修改成通过服务请求的方式完成声纹识别，例如提供一个API供APP调用，用户在APP上通过声纹登录时，把录音到的语音发送到后端完成声纹识别，再把结果返回给APP，前提是用户已经使用语音注册，并成功把语音数据存放在`audio_db`文件夹中。
-```shell
-python infer_recognition.py
-```
-
-输出类似如下：
-```
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:13 - ----------- 额外配置参数 -----------
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - audio_db_path: audio_db/
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - configs: configs/ecapa_tdnn.yml
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - model_path: models/EcapaTdnn_Fbank/best_model/
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - record_seconds: 3
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - threshold: 0.6
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - use_gpu: True
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:16 - ------------------------------------------------
-······································································
-W0425 08:30:13.257884 23889 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
-W0425 08:30:13.260191 23889 device_context.cc:465] device: 0, cuDNN Version: 7.6.
-成功加载模型参数和优化方法参数：models/ecapa_tdnn/model.pdparams
-Loaded 沙瑞金 audio.
-Loaded 李达康 audio.
-请选择功能，0为注册音频到声纹库，1为执行声纹识别：0
-按下回车键开机录音，录音3秒中：
-开始录音......
-录音已结束!
-请输入该音频用户的名称：夜雨飘零
-请选择功能，0为注册音频到声纹库，1为执行声纹识别：1
-按下回车键开机录音，录音3秒中：
-开始录音......
-录音已结束!
-识别说话的为：夜雨飘零，相似度为：0.920434
-```
-
-# 其他版本
- - Tensorflow：[VoiceprintRecognition-Tensorflow](https://github.com/yeyupiaoling/VoiceprintRecognition-Tensorflow)
- - Pytorch：[VoiceprintRecognition-Pytorch](https://github.com/yeyupiaoling/VoiceprintRecognition-Pytorch)
- - Keras：[VoiceprintRecognition-Keras](https://github.com/yeyupiaoling/VoiceprintRecognition-Keras)
-
-## 打赏作者
-
-<br/>
-<div align="center">
-<p>打赏一块钱支持一下作者</p>
-<img src="https://yeyupiaoling.cn/reward.png" alt="打赏作者" width="400">
-</div>
-
-# 参考资料
-1. https://github.com/PaddlePaddle/PaddleSpeech
-2. https://github.com/yeyupiaoling/PaddlePaddle-MobileFaceNets
-3. https://github.com/yeyupiaoling/PPASR
-
-
+Metadata-Version: 2.1
+Name: ppvector
+Version: 1.0.2
+Summary: Voice Print Recognition toolkit on PaddlePaddle
+Home-page: https://github.com/yeyupiaoling/VoiceprintRecognition_PaddlePaddle
+Download-URL: https://github.com/yeyupiaoling/VoiceprintRecognition_PaddlePaddle.git
+Author: yeyupiaoling
+License: Apache License 2.0
+Keywords: Voice,paddle
+Classifier: Intended Audience :: Developers
+Classifier: License :: OSI Approved :: Apache Software License
+Classifier: Operating System :: OS Independent
+Classifier: Natural Language :: Chinese (Simplified)
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.5
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Topic :: Utilities
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: numba >=0.52.0
+Requires-Dist: librosa >=0.9.1
+Requires-Dist: numpy >=1.19.2
+Requires-Dist: tqdm >=4.59.0
+Requires-Dist: visualdl >=2.1.1
+Requires-Dist: resampy >=0.2.2
+Requires-Dist: soundfile >=0.12.1
+Requires-Dist: soundcard >=0.4.2
+Requires-Dist: pyyaml >=5.4.1
+Requires-Dist: termcolor >=1.1.0
+Requires-Dist: typeguard ==2.13.3
+Requires-Dist: paddleaudio >=1.0.1
+Requires-Dist: scikit-learn >=1.0.2
+Requires-Dist: pydub >=0.25.1
+Requires-Dist: av >=10.0.0
+
+# 基于PaddlePaddle实现的声纹识别系统
+
+![python version](https://img.shields.io/badge/python-3.8+-orange.svg)
+![GitHub forks](https://img.shields.io/github/forks/yeyupiaoling/VoiceprintRecognition-PaddlePaddle)
+![GitHub Repo stars](https://img.shields.io/github/stars/yeyupiaoling/VoiceprintRecognition-PaddlePaddle)
+![GitHub](https://img.shields.io/github/license/yeyupiaoling/VoiceprintRecognition-PaddlePaddle)
+![支持系统](https://img.shields.io/badge/支持系统-Win/Linux/MAC-9cf)
+
+本分支为1.0版本，如果要使用之前的0.3版本请在[0.x分支](https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle/tree/release/0.x)使用。本项目使用了EcapaTdnn、ResNetSE、ERes2Net、CAM++等多种先进的声纹识别模型，不排除以后会支持更多模型，同时本项目也支持了MelSpectrogram、Spectrogram、MFCC、Fbank等多种数据预处理方法，使用了ArcFace Loss，ArcFace loss：Additive Angular Margin Loss（加性角度间隔损失函数），对应项目中的AAMLoss，对特征向量和权重归一化，对θ加上角度间隔m，角度间隔比余弦间隔在对角度的影响更加直接，除此之外，还支持AMLoss、ARMLoss、CELoss等多种损失函数。
+
+**欢迎大家扫码入知识星球或者QQ群讨论，知识星球里面提供项目的模型文件和博主其他相关项目的模型文件，也包括其他一些资源。**
+
+<div align="center">
+  <img src="https://yeyupiaoling.cn/zsxq.png" alt="知识星球" width="400">
+  <img src="https://yeyupiaoling.cn/qq.png" alt="QQ群" width="400">
+</div>
+
+
+使用环境：
+
+ - Anaconda 3
+ - Python 3.11
+ - PaddlePaddle 2.5.1
+ - Windows 10 or Ubuntu 18.04
+
+# 项目特性
+
+1. 支持模型：EcapaTdnn、TDNN、Res2Net、ResNetSE、ERes2Net、CAM++
+2. 支持池化层：AttentiveStatsPool(ASP)、SelfAttentivePooling(SAP)、TemporalStatisticsPooling(TSP)、TemporalAveragePooling(TAP)、TemporalStatsPool(TSTP)
+3. 支持损失函数：AAMLoss、SphereFace2、AMLoss、ARMLoss、CELoss
+4. 支持预处理方法：MelSpectrogram、Spectrogram、MFCC、Fbank
+
+**模型论文：**
+
+- EcapaTdnn：[ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification](https://arxiv.org/abs/2005.07143v3)
+- TDNN：[Prediction of speech intelligibility with DNN-based performance measures](https://arxiv.org/abs/2203.09148)
+- Res2Net：[Res2Net: A New Multi-scale Backbone Architecture](https://arxiv.org/abs/1904.01169)
+- ResNetSE：[Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)
+- CAMPPlus：[CAM++: A Fast and Efficient Network for Speaker Verification Using Context-Aware Masking](https://arxiv.org/abs/2303.00332v3)
+- ERes2Net：[An Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification](https://arxiv.org/abs/2305.12838v1)
+
+# 模型下载
+
+### 训练CN-Celeb数据，共有2796个说话人。
+
+|    模型     | Params(M) | 预处理方法 |                数据集                 | train speakers | threshold |   EER   | MinDCF  |   模型下载   |
+|:---------:|:---------:|:-----:|:----------------------------------:|:--------------:|:---------:|:-------:|:-------:|:--------:|
+|   CAM++   |    6.8    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.25    | 0.09485 | 0.56214 | 加入知识星球获取 |
+| ERes2Net  |    6.6    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.22    | 0.09637 | 0.52627 | 加入知识星球获取 |
+| ResNetSE  |    7.8    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.19    | 0.10222 | 0.57981 | 加入知识星球获取 |
+| EcapaTdnn |    6.1    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.25    | 0.10465 | 0.58521 | 加入知识星球获取 |
+|   TDNN    |    2.6    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.23    | 0.11804 | 0.61070 | 加入知识星球获取 |
+|  Res2Net  |    5.0    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.18    | 0.14126 | 0.68511 | 加入知识星球获取 |
+|   CAM++   |    6.8    | Fbank |               更大数据集                |      2W+       |   0.34    | 0.07884 | 0.52738 | 加入知识星球获取 |
+|   CAM++   |    6.8    | Flank |               其他数据集                |      20W       |   0.29    | 0.04768 | 0.31429 | 加入知识星球获取 |
+| ERes2Net  |   55.1    | Fbank |               其他数据集                |      20W       |   0.36    | 0.02939 | 0.18355 | 加入知识星球获取 |
+
+说明：
+1. 评估的测试集为[CN-Celeb的测试集](https://aistudio.baidu.com/aistudio/datasetdetail/233361)，包含196个说话人。
+2. 使用语速增强分类大小翻三倍`speed_perturb_3_class: True`。
+3. 参数数量不包含了分类器的参数数量。
+
+
+### 训练VoxCeleb1&2数据，共有7205个说话人。
+
+|    模型     | Params(M) | 预处理方法 |     数据集     | train speakers | threshold |   EER   | MinDCF  |   模型下载   |
+|:---------:|:---------:|:-----:|:-----------:|:--------------:|:---------:|:-------:|:-------:|:--------:|
+|   CAM++   |    6.8    | Fbank | VoxCeleb1&2 |      7205      |   0.27    | 0.03350 | 0.25726 | 加入知识星球获取 |
+| ERes2Net  |    6.6    | Fbank | VoxCeleb1&2 |      7205      |   0.21    | 0.03997 | 0.30614 | 加入知识星球获取 |
+| ResNetSE  |    7.8    | Fbank | VoxCeleb1&2 |      7205      |   0.21    | 0.03758 | 0.27625 | 加入知识星球获取 |
+| EcapaTdnn |    6.1    | Fbank | VoxCeleb1&2 |      7205      |   0.27    | 0.02852 | 0.19432 | 加入知识星球获取 |
+|   TDNN    |    2.6    | Fbank | VoxCeleb1&2 |      7205      |   0.25    | 0.03541 | 0.28130 | 加入知识星球获取 |
+|  Res2Net  |    5.0    | Fbank | VoxCeleb1&2 |      7205      |   0.21    | 0.04749 | 0.44950 | 加入知识星球获取 |
+|   CAM++   |    6.8    | Fbank |    更大数据集    |      2W+       |   0.28    | 0.03192 | 0.22032 | 加入知识星球获取 |
+|   CAM++   |    6.8    | Fbank |    其他数据集    |      20W+      |   0.49    | 0.10331 | 0.71206 | 加入知识星球获取 |
+| ERes2Net  |   55.1    | Fbank |    其他数据集    |      20W+      |   0.53    | 0.08903 | 0.62131 | 加入知识星球获取 |
+
+说明：
+
+1. 评估的测试集为[VoxCeleb1&2的测试集](https://aistudio.baidu.com/aistudio/datasetdetail/255977)，包含158个说话人。
+2. 使用语速增强分类大小翻三倍`speed_perturb_3_class: True`。
+3. 参数数量不包含了分类器的参数数量。
+
+
+## 安装环境
+
+ - 首先安装的是PaddlePaddle的GPU版本，如果已经安装过了，请跳过。
+```shell
+conda install paddlepaddle-gpu==2.4.1 cudatoolkit=10.2 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/
+```
+
+ - 安装ppvector库。
+ 
+使用pip安装，命令如下：
+```shell
+python -m pip install ppvector -U -i https://pypi.tuna.tsinghua.edu.cn/simple
+```
+
+**建议源码安装**，源码安装能保证使用最新代码。
+```shell
+git clone https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle.git
+cd VoiceprintRecognition-PaddlePaddle/
+pip install .
+```
+
+# 创建数据
+本教程笔者使用的是[CN-Celeb](https://openslr.elda.org/resources/82)，这个数据集一共有约3000个人的语音数据，有65W+条语音数据，下载之后要解压数据集到`dataset`目录，另外如果要评估，还需要下载[CN-Celeb的测试集](https://aistudio.baidu.com/aistudio/datasetdetail/233361)。如果读者有其他更好的数据集，可以混合在一起使用，但最好是要用python的工具模块aukit处理音频，降噪和去除静音。
+
+首先是创建一个数据列表，数据列表的格式为`<语音文件路径\t语音分类标签>`，创建这个列表主要是方便之后的读取，也是方便读取使用其他的语音数据集，语音分类标签是指说话人的唯一ID，不同的语音数据集，可以通过编写对应的生成数据列表的函数，把这些数据集都写在同一个数据列表中。
+
+执行`create_data.py`程序完成数据准备。
+```shell
+python create_data.py
+```
+
+执行上面的程序之后，会生成以下的数据格式，如果要自定义数据，参考如下数据列表，前面是音频的相对路径，后面的是该音频对应的说话人的标签，就跟分类一样。**自定义数据集的注意**，测试数据列表的ID可以不用跟训练的ID一样，也就是说测试的数据的说话人可以不用出现在训练集，只要保证测试数据列表中同一个人相同的ID即可。
+```
+dataset/CN-Celeb2_flac/data/id11999/recitation-03-019.flac      2795
+dataset/CN-Celeb2_flac/data/id11999/recitation-10-023.flac      2795
+dataset/CN-Celeb2_flac/data/id11999/recitation-06-025.flac      2795
+dataset/CN-Celeb2_flac/data/id11999/recitation-04-014.flac      2795
+dataset/CN-Celeb2_flac/data/id11999/recitation-06-030.flac      2795
+dataset/CN-Celeb2_flac/data/id11999/recitation-10-032.flac      2795
+dataset/CN-Celeb2_flac/data/id11999/recitation-06-028.flac      2795
+dataset/CN-Celeb2_flac/data/id11999/recitation-10-031.flac      2795
+dataset/CN-Celeb2_flac/data/id11999/recitation-05-003.flac      2795
+dataset/CN-Celeb2_flac/data/id11999/recitation-04-017.flac      2795
+dataset/CN-Celeb2_flac/data/id11999/recitation-10-016.flac      2795
+dataset/CN-Celeb2_flac/data/id11999/recitation-09-001.flac      2795
+dataset/CN-Celeb2_flac/data/id11999/recitation-05-010.flac      2795
+```
+
+# 修改预处理方法
+
+配置文件中默认使用的是Fbank预处理方法，如果要使用其他预处理方法，可以修改配置文件中的安装下面方式修改，具体的值可以根据自己情况修改。如果不清楚如何设置参数，可以直接删除该部分，直接使用默认值。
+
+```yaml
+# 数据预处理参数
+preprocess_conf:
+  # 音频预处理方法，支持：LogMelSpectrogram、MelSpectrogram、Spectrogram、MFCC、Fbank
+  feature_method: 'Fbank'
+  # 设置API参数，更参数查看对应API，不清楚的可以直接删除该部分，直接使用默认值
+  method_args:
+    sr: 16000
+    n_mels: 80
+```
+
+# 训练模型
+使用`train.py`训练模型，本项目支持多个音频预处理方式，通过`configs/ecapa_tdnn.yml`配置文件的参数`preprocess_conf.feature_method`可以指定，`MelSpectrogram`为梅尔频谱，`Spectrogram`为语谱图，`MFCC`梅尔频谱倒谱系数。通过参数`augment_conf_path`可以指定数据增强方式。训练过程中，会使用VisualDL保存训练日志，通过启动VisualDL可以随时查看训练结果，启动命令`visualdl --logdir=log --host 0.0.0.0`
+```shell
+# 单卡训练
+CUDA_VISIBLE_DEVICES=0 python train.py
+# 多卡训练
+python -m paddle.distributed.launch --gpus '0,1' train.py
+```
+
+训练输出日志：
+```
+[2023-08-05 09:52:06.497988 INFO   ] utils:print_arguments:13 - ----------- 额外配置参数 -----------
+[2023-08-05 09:52:06.498094 INFO   ] utils:print_arguments:15 - configs: configs/ecapa_tdnn.yml
+[2023-08-05 09:52:06.498149 INFO   ] utils:print_arguments:15 - do_eval: True
+[2023-08-05 09:52:06.498191 INFO   ] utils:print_arguments:15 - local_rank: 0
+[2023-08-05 09:52:06.498230 INFO   ] utils:print_arguments:15 - pretrained_model: None
+[2023-08-05 09:52:06.498269 INFO   ] utils:print_arguments:15 - resume_model: None
+[2023-08-05 09:52:06.498306 INFO   ] utils:print_arguments:15 - save_model_path: models/
+[2023-08-05 09:52:06.498342 INFO   ] utils:print_arguments:15 - use_gpu: True
+[2023-08-05 09:52:06.498378 INFO   ] utils:print_arguments:16 - ------------------------------------------------
+[2023-08-05 09:52:06.513761 INFO   ] utils:print_arguments:18 - ----------- 配置文件参数 -----------
+[2023-08-05 09:52:06.513906 INFO   ] utils:print_arguments:21 - dataset_conf:
+[2023-08-05 09:52:06.513957 INFO   ] utils:print_arguments:24 -         dataLoader:
+[2023-08-05 09:52:06.513995 INFO   ] utils:print_arguments:26 -                 batch_size: 64
+[2023-08-05 09:52:06.514031 INFO   ] utils:print_arguments:26 -                 num_workers: 4
+[2023-08-05 09:52:06.514066 INFO   ] utils:print_arguments:28 -         do_vad: False
+[2023-08-05 09:52:06.514101 INFO   ] utils:print_arguments:28 -         enroll_list: dataset/enroll_list.txt
+[2023-08-05 09:52:06.514135 INFO   ] utils:print_arguments:24 -         eval_conf:
+[2023-08-05 09:52:06.514169 INFO   ] utils:print_arguments:26 -                 batch_size: 1
+[2023-08-05 09:52:06.514203 INFO   ] utils:print_arguments:26 -                 max_duration: 20
+[2023-08-05 09:52:06.514237 INFO   ] utils:print_arguments:28 -         max_duration: 3
+[2023-08-05 09:52:06.514274 INFO   ] utils:print_arguments:28 -         min_duration: 0.5
+[2023-08-05 09:52:06.514308 INFO   ] utils:print_arguments:28 -         noise_aug_prob: 0.2
+[2023-08-05 09:52:06.514342 INFO   ] utils:print_arguments:28 -         noise_dir: dataset/noise
+[2023-08-05 09:52:06.514374 INFO   ] utils:print_arguments:28 -         num_speakers: 3242
+[2023-08-05 09:52:06.514408 INFO   ] utils:print_arguments:28 -         sample_rate: 16000
+[2023-08-05 09:52:06.514441 INFO   ] utils:print_arguments:28 -         speed_perturb: True
+[2023-08-05 09:52:06.514475 INFO   ] utils:print_arguments:28 -         target_dB: -20
+[2023-08-05 09:52:06.514508 INFO   ] utils:print_arguments:28 -         train_list: dataset/train_list.txt
+[2023-08-05 09:52:06.514542 INFO   ] utils:print_arguments:28 -         trials_list: dataset/trials_list.txt
+[2023-08-05 09:52:06.514575 INFO   ] utils:print_arguments:28 -         use_dB_normalization: True
+[2023-08-05 09:52:06.514609 INFO   ] utils:print_arguments:21 - loss_conf:
+[2023-08-05 09:52:06.514643 INFO   ] utils:print_arguments:24 -         args:
+[2023-08-05 09:52:06.514678 INFO   ] utils:print_arguments:26 -                 easy_margin: False
+[2023-08-05 09:52:06.514713 INFO   ] utils:print_arguments:26 -                 margin: 0.2
+[2023-08-05 09:52:06.514746 INFO   ] utils:print_arguments:26 -                 scale: 32
+[2023-08-05 09:52:06.514779 INFO   ] utils:print_arguments:24 -         margin_scheduler_args:
+[2023-08-05 09:52:06.514814 INFO   ] utils:print_arguments:26 -                 final_margin: 0.3
+[2023-08-05 09:52:06.514848 INFO   ] utils:print_arguments:28 -         use_loss: AAMLoss
+[2023-08-05 09:52:06.514882 INFO   ] utils:print_arguments:28 -         use_margin_scheduler: True
+[2023-08-05 09:52:06.514915 INFO   ] utils:print_arguments:21 - model_conf:
+[2023-08-05 09:52:06.514950 INFO   ] utils:print_arguments:24 -         backbone:
+[2023-08-05 09:52:06.514984 INFO   ] utils:print_arguments:26 -                 embd_dim: 192
+[2023-08-05 09:52:06.515017 INFO   ] utils:print_arguments:26 -                 pooling_type: ASP
+[2023-08-05 09:52:06.515050 INFO   ] utils:print_arguments:24 -         classifier:
+[2023-08-05 09:52:06.515084 INFO   ] utils:print_arguments:26 -                 num_blocks: 0
+[2023-08-05 09:52:06.515118 INFO   ] utils:print_arguments:21 - optimizer_conf:
+[2023-08-05 09:52:06.515154 INFO   ] utils:print_arguments:28 -         learning_rate: 0.001
+[2023-08-05 09:52:06.515188 INFO   ] utils:print_arguments:28 -         optimizer: Adam
+[2023-08-05 09:52:06.515221 INFO   ] utils:print_arguments:28 -         scheduler: CosineAnnealingLR
+[2023-08-05 09:52:06.515254 INFO   ] utils:print_arguments:28 -         scheduler_args: None
+[2023-08-05 09:52:06.515289 INFO   ] utils:print_arguments:28 -         weight_decay: 1e-06
+[2023-08-05 09:52:06.515323 INFO   ] utils:print_arguments:21 - preprocess_conf:
+[2023-08-05 09:52:06.515357 INFO   ] utils:print_arguments:28 -         feature_method: MelSpectrogram
+[2023-08-05 09:52:06.515390 INFO   ] utils:print_arguments:24 -         method_args:
+[2023-08-05 09:52:06.515426 INFO   ] utils:print_arguments:26 -                 f_max: 14000.0
+[2023-08-05 09:52:06.515460 INFO   ] utils:print_arguments:26 -                 f_min: 50.0
+[2023-08-05 09:52:06.515493 INFO   ] utils:print_arguments:26 -                 hop_length: 320
+[2023-08-05 09:52:06.515527 INFO   ] utils:print_arguments:26 -                 n_fft: 1024
+[2023-08-05 09:52:06.515560 INFO   ] utils:print_arguments:26 -                 n_mels: 64
+[2023-08-05 09:52:06.515593 INFO   ] utils:print_arguments:26 -                 sample_rate: 16000
+[2023-08-05 09:52:06.515626 INFO   ] utils:print_arguments:26 -                 win_length: 1024
+[2023-08-05 09:52:06.515660 INFO   ] utils:print_arguments:21 - train_conf:
+[2023-08-05 09:52:06.515694 INFO   ] utils:print_arguments:28 -         log_interval: 100
+[2023-08-05 09:52:06.515728 INFO   ] utils:print_arguments:28 -         max_epoch: 30
+[2023-08-05 09:52:06.515761 INFO   ] utils:print_arguments:30 - use_model: EcapaTdnn
+[2023-08-05 09:52:06.515794 INFO   ] utils:print_arguments:31 - ------------------------------------------------
+----------------------------------------------------------------------------------------
+        Layer (type)             Input Shape          Output Shape         Param #    
+========================================================================================
+          Conv1D-2              [[1, 64, 102]]        [1, 512, 98]         164,352    
+          Conv1d-1              [[1, 64, 98]]         [1, 512, 98]            0       
+           ReLU-1               [[1, 512, 98]]        [1, 512, 98]            0       
+       BatchNorm1D-2            [[1, 512, 98]]        [1, 512, 98]          2,048     
+       BatchNorm1d-1            [[1, 512, 98]]        [1, 512, 98]            0       
+        TDNNBlock-1             [[1, 64, 98]]         [1, 512, 98]            0       
+          Conv1D-4              [[1, 512, 98]]        [1, 512, 98]         262,656    
+          Conv1d-3              [[1, 512, 98]]        [1, 512, 98]            0       
+           ReLU-2               [[1, 512, 98]]        [1, 512, 98]            0       
+       BatchNorm1D-4            [[1, 512, 98]]        [1, 512, 98]          2,048     
+       BatchNorm1d-3            [[1, 512, 98]]        [1, 512, 98]            0       
+        TDNNBlock-2             [[1, 512, 98]]        [1, 512, 98]            0       
+··········································
+         SEBlock-3           [[1, 512, 98], None]     [1, 512, 98]            0       
+      SERes2NetBlock-3          [[1, 512, 98]]        [1, 512, 98]            0       
+         Conv1D-70             [[1, 1536, 98]]       [1, 1536, 98]        2,360,832   
+         Conv1d-69             [[1, 1536, 98]]       [1, 1536, 98]            0       
+          ReLU-32              [[1, 1536, 98]]       [1, 1536, 98]            0       
+       BatchNorm1D-58          [[1, 1536, 98]]       [1, 1536, 98]          6,144     
+       BatchNorm1d-57          [[1, 1536, 98]]       [1, 1536, 98]            0       
+        TDNNBlock-29           [[1, 1536, 98]]       [1, 1536, 98]            0       
+         Conv1D-72             [[1, 4608, 98]]        [1, 128, 98]         589,952    
+         Conv1d-71             [[1, 4608, 98]]        [1, 128, 98]            0       
+          ReLU-33               [[1, 128, 98]]        [1, 128, 98]            0       
+       BatchNorm1D-60           [[1, 128, 98]]        [1, 128, 98]           512      
+       BatchNorm1d-59           [[1, 128, 98]]        [1, 128, 98]            0       
+        TDNNBlock-30           [[1, 4608, 98]]        [1, 128, 98]            0       
+           Tanh-1               [[1, 128, 98]]        [1, 128, 98]            0       
+         Conv1D-74              [[1, 128, 98]]       [1, 1536, 98]         198,144    
+         Conv1d-73              [[1, 128, 98]]       [1, 1536, 98]            0       
+AttentiveStatisticsPooling-1   [[1, 1536, 98]]        [1, 3072, 1]            0       
+       BatchNorm1D-62           [[1, 3072, 1]]        [1, 3072, 1]         12,288     
+       BatchNorm1d-61           [[1, 3072, 1]]        [1, 3072, 1]            0       
+         Conv1D-76              [[1, 3072, 1]]        [1, 192, 1]          590,016    
+         Conv1d-75              [[1, 3072, 1]]        [1, 192, 1]             0       
+        EcapaTdnn-1             [[1, 98, 64]]           [1, 192]              0       
+  SpeakerIdentification-1         [[1, 192]]           [1, 9726]          1,867,392   
+========================================================================================
+Total params: 8,039,808
+Trainable params: 8,020,480
+Non-trainable params: 19,328
+----------------------------------------------------------------------------------------
+Input size (MB): 0.02
+Forward/backward pass size (MB): 35.60
+Params size (MB): 30.67
+Estimated Total Size (MB): 66.30
+----------------------------------------------------------------------------------------
+[2023-08-05 09:52:08.084231 INFO   ] trainer:train:388 - 训练数据：874175
+[2023-08-05 09:52:09.186542 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [0/13659], loss: 11.95824, accuracy: 0.00000, learning rate: 0.00100000, speed: 58.09 data/sec, eta: 5 days, 5:24:08
+[2023-08-05 09:52:22.477905 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [100/13659], loss: 10.35675, accuracy: 0.00278, learning rate: 0.00100000, speed: 481.65 data/sec, eta: 15:07:15
+[2023-08-05 09:52:35.948581 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [200/13659], loss: 10.22089, accuracy: 0.00505, learning rate: 0.00100000, speed: 475.27 data/sec, eta: 15:19:12
+[2023-08-05 09:52:49.249098 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [300/13659], loss: 10.00268, accuracy: 0.00706, learning rate: 0.00100000, speed: 481.45 data/sec, eta: 15:07:11
+[2023-08-05 09:53:03.716015 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [400/13659], loss: 9.76052, accuracy: 0.00830, learning rate: 0.00100000, speed: 442.74 data/sec, eta: 16:26:16
+[2023-08-05 09:53:18.258807 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [500/13659], loss: 9.50189, accuracy: 0.01060, learning rate: 0.00100000, speed: 440.46 data/sec, eta: 16:31:08
+[2023-08-05 09:53:31.618354 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [600/13659], loss: 9.26083, accuracy: 0.01256, learning rate: 0.00100000, speed: 479.50 data/sec, eta: 15:10:12
+[2023-08-05 09:53:45.439642 INFO   ] trainer:__train_epoch:334 - Train epoch: [1/30], batch: [700/13659], loss: 9.03548, accuracy: 0.01449, learning rate: 0.00099999, speed: 463.63 data/sec, eta: 15:41:08
+```
+
+启动VisualDL：`visualdl --logdir=log --host 0.0.0.0`，VisualDL页面如下：
+
+<img src="./docs/images/log.jpg" alt="VisualDL页面" width="600">
+
+
+# 评估模型
+训练结束之后会保存预测模型，我们用预测模型来预测测试集中的音频特征，然后使用音频特征进行两两对比，计算EER和MinDCF。
+```shell
+python eval.py
+```
+
+输出类似如下：
+```
+······
+------------------------------------------------
+W0425 08:27:32.057426 17654 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
+W0425 08:27:32.065165 17654 device_context.cc:465] device: 0, cuDNN Version: 7.6.
+[2023-03-16 20:20:47.195908 INFO   ] trainer:evaluate:341 - 成功加载模型：models/EcapaTdnn_Fbank/best_model/model.pth
+100%|███████████████████████████| 84/84 [00:28<00:00,  2.95it/s]
+开始两两对比音频特征...
+100%|███████████████████████████| 5332/5332 [00:05<00:00, 1027.83it/s]
+评估消耗时间：65s，threshold：0.26，EER: 0.14739, MinDCF: 0.41999
+```
+
+# 推理接口
+
+下面给出了几个常用的接口，更多接口请参考`mvector/predict.py`，也可以往下看`声纹对比`和`声纹识别`的例子。
+
+```python
+from ppvector.predict import PPVectorPredictor
+
+predictor = PPVectorPredictor(configs='configs/cam++.yml',
+                              model_path='models/CAMPPlus_Fbank/best_model/')
+# 获取音频特征
+embedding = predictor.predict(audio_data='dataset/a_1.wav')
+# 获取两个音频的相似度
+similarity = predictor.contrast(audio_data1='dataset/a_1.wav', audio_data2='dataset/a_2.wav')
+
+# 注册用户音频
+predictor.register(user_name='夜雨飘零', audio_data='dataset/test.wav')
+# 识别用户音频
+name, score = predictor.recognition(audio_data='dataset/test1.wav')
+# 获取所有用户
+users_name = predictor.get_users()
+# 删除用户音频
+predictor.remove_user(user_name='夜雨飘零')
+```
+
+# 声纹对比
+下面开始实现声纹对比，创建`infer_contrast.py`程序，首先介绍几个重要的函数，`predict()`函数是可以获取声纹特征，`predict_batch()`函数是可以获取一批的声纹特征，`contrast()`函数可以对比两条音频的相似度，`register()`函数注册一条音频到声纹库里面，`recognition()`函输入一条音频并且从声纹库里面对比识别，`remove_user()`函数移除你好。声纹库里面的注册人。我们输入两个语音，通过预测函数获取他们的特征数据，使用这个特征数据可以求他们的对角余弦值，得到的结果可以作为他们相识度。对于这个相识度的阈值`threshold`，读者可以根据自己项目的准确度要求进行修改。
+```shell
+python infer_contrast.py --audio_path1=audio/a_1.wav --audio_path2=audio/b_2.wav
+```
+
+输出类似如下：
+```
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:13 - ----------- 额外配置参数 -----------
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - audio_path1: dataset/a_1.wav
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - audio_path2: dataset/b_2.wav
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - configs: configs/ecapa_tdnn.yml
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - model_path: models/EcapaTdnn_Fbank/best_model/
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - threshold: 0.6
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - use_gpu: True
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:16 - ------------------------------------------------
+······································································
+W0425 08:29:10.006249 21121 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
+W0425 08:29:10.008555 21121 device_context.cc:465] device: 0, cuDNN Version: 7.6.
+成功加载模型参数和优化方法参数：models/ecapa_tdnn/model.pdparams
+audio/a_1.wav 和 audio/b_2.wav 不是同一个人，相似度为：-0.09565544128417969
+```
+
+同时还提供了有GUI界面的声纹对比程序，执行`infer_contrast_gui.py`启动程序，界面如下，分别选择两个音频，点击开始判断，就可以判断它们是否是同一个人。
+
+<img src="./docs/images/contrast.jpg" alt="声纹对比界面">
+
+
+# 声纹识别
+
+在新闻识别里面主要使用到`register()`函数和`recognition()`函数，首先使用`register()`函数函数来注册音频到声纹库里面，也可以直接把文件添加到`audio_db`文件夹里面，使用的时候通过`recognition()`函数来发起识别，输入一条音频，就可以从声纹库里面识别到所需要的说话人。
+
+有了上面的声纹识别的函数，读者可以根据自己项目的需求完成声纹识别的方式，例如笔者下面提供的是通过录音来完成声纹识别。首先必须要加载语音库中的语音，语音库文件夹为`audio_db`，然后用户回车后录音3秒钟，然后程序会自动录音，并使用录音到的音频进行声纹识别，去匹配语音库中的语音，获取用户的信息。通过这样方式，读者也可以修改成通过服务请求的方式完成声纹识别，例如提供一个API供APP调用，用户在APP上通过声纹登录时，把录音到的语音发送到后端完成声纹识别，再把结果返回给APP，前提是用户已经使用语音注册，并成功把语音数据存放在`audio_db`文件夹中。
+```shell
+python infer_recognition.py
+```
+
+输出类似如下：
+```
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:13 - ----------- 额外配置参数 -----------
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - audio_db_path: audio_db/
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - configs: configs/ecapa_tdnn.yml
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - model_path: models/EcapaTdnn_Fbank/best_model/
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - record_seconds: 3
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - threshold: 0.6
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - use_gpu: True
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:16 - ------------------------------------------------
+······································································
+W0425 08:30:13.257884 23889 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
+W0425 08:30:13.260191 23889 device_context.cc:465] device: 0, cuDNN Version: 7.6.
+成功加载模型参数和优化方法参数：models/ecapa_tdnn/model.pdparams
+Loaded 沙瑞金 audio.
+Loaded 李达康 audio.
+请选择功能，0为注册音频到声纹库，1为执行声纹识别：0
+按下回车键开机录音，录音3秒中：
+开始录音......
+录音已结束!
+请输入该音频用户的名称：夜雨飘零
+请选择功能，0为注册音频到声纹库，1为执行声纹识别：1
+按下回车键开机录音，录音3秒中：
+开始录音......
+录音已结束!
+识别说话的为：夜雨飘零，相似度为：0.920434
+```
+
+
+同时还提供了有GUI界面的声纹识别程序，执行`infer_recognition_gui.py`启动，点击`注册音频到声纹库`按钮，理解开始说话，录制3秒钟，然后输入注册人的名称，之后可以`执行声纹识别`按钮，然后立即说话，录制3秒钟后，等待识别结果。`删除用户`按钮可以删除用户。`实时识别`按钮可以实时识别，可以一直录音，一直识别。
+
+<img src="./docs/images/recognition.jpg" alt="声纹识别界面">
+
+
+# 其他版本
+ - Tensorflow：[VoiceprintRecognition-Tensorflow](https://github.com/yeyupiaoling/VoiceprintRecognition-Tensorflow)
+ - Pytorch：[VoiceprintRecognition-Pytorch](https://github.com/yeyupiaoling/VoiceprintRecognition-Pytorch)
+ - Keras：[VoiceprintRecognition-Keras](https://github.com/yeyupiaoling/VoiceprintRecognition-Keras)
+
+## 打赏作者
+
+<br/>
+<div align="center">
+<p>打赏一块钱支持一下作者</p>
+<img src="https://yeyupiaoling.cn/reward.png" alt="打赏作者" width="400">
+</div>
+
+# 参考资料
+1. https://github.com/PaddlePaddle/PaddleSpeech
+2. https://github.com/yeyupiaoling/PaddlePaddle-MobileFaceNets
+3. https://github.com/yeyupiaoling/PPASR
+4. https://github.com/alibaba-damo-academy/3D-Speaker
+5. https://github.com/wenet-e2e/wespeaker
```

## Comparing `ppvector-1.0.0.dist-info/RECORD` & `ppvector-1.0.2.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-ppvector/__init__.py,sha256=7NDcJpjWxksGC1IbC5D97bswVK64kAZzO2jrCEpxHh0,134
-ppvector/predict.py,sha256=GATOTqbfOxJ99RbStPH6V8yi0U0gu6mR4WutUZ6Qxi4,17212
-ppvector/trainer.py,sha256=IC_zlyRbjkY2wx_tYD5AZ9Ln08sLmsi7LitxOPKD74c,32699
+ppvector/__init__.py,sha256=nuwAt02ANJKWpNLCcQvX8UfLYxrRTI3OhFIZ_x5pb5g,134
+ppvector/predict.py,sha256=ZVnd7B5uC0gh0-2oGcr1EtkNcRmGYdxagVV0tAtSzuQ,17739
+ppvector/trainer.py,sha256=R_plMxX9HDT9C0xj1Sm1fbzBqXC5LQLP_O75sGo5_XM,34807
 ppvector/data_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ppvector/data_utils/audio.py,sha256=cVCj87WN-9B393NzlRKdnnrem3UBvmWQQGRYriSqhS4,22259
+ppvector/data_utils/audio.py,sha256=If9HGHdfZeNjQgERNl8TTY8MFbkkWF0NKXq_RA5uTjs,22221
 ppvector/data_utils/collate_fn.py,sha256=tb5yu7VH69x1nGKxJUBoZ5UNsVgYxZ_zbth2YN52zPw,909
 ppvector/data_utils/featurizer.py,sha256=9j7nEQbe1f5udaxZwFIG6D_erTW3gMKuIZB2cd3-hMI,3771
 ppvector/data_utils/reader.py,sha256=3OJvJI_psix9duILZ7t0IzG3xK4SOXhIg0giRiH99v0,5892
 ppvector/data_utils/spec_aug.py,sha256=Qw32xZTw1f8Fk3Kw9aQt_5IKiH8k1giXbCX3XUzU1xw,1581
-ppvector/data_utils/utils.py,sha256=J-4RwA_DTZReBRU9l2oSAVXxsdqLitEKqZNYp-vRsyw,4653
+ppvector/data_utils/utils.py,sha256=wripWTTPcEOgskVli8eTyHROs8wok8CItFd8uYpIBcU,4713
 ppvector/metric/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppvector/metric/metrics.py,sha256=BuQcf0E_6JUwbdFYKvzpb3rCAssizJjk_JvrQmLlfkA,1208
 ppvector/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppvector/models/campplus.py,sha256=Xfq1O8zdf9sDjMsoQ-1E2rpScoMlaJjfErBxCyBytgM,12745
 ppvector/models/ecapa_tdnn.py,sha256=x6d4Hd4e7KKlUDZ66egQw4Zpu4V39krPgvGW1QL-Za0,11077
 ppvector/models/eres2net.py,sha256=5y39Ol5FOWZvKW803CGa5ZFux7GfkzfFzDsS3KwsAfc,10135
-ppvector/models/fc.py,sha256=EqL0cHw9Voj0qJ8zG_lFpWTkTEC7LBQdXM24uYNSbLU,3661
-ppvector/models/loss.py,sha256=TobMLuiL2oaWli6W5N5XqIqnRHUl7lhAPHRgdO5spn0,6300
+ppvector/models/fc.py,sha256=dtMca714xkcaP_399V-1pzfBGUeR1cXpkQ3bhJj5jx4,3731
+ppvector/models/loss.py,sha256=6WSKIpRiUFktzkClZdvTae2GYxZKzS2LmtINgWX6unE,9521
 ppvector/models/pooling.py,sha256=WVJILqRl9YyghxsTUJR9fTwRA3XCr69Kns_08FPvCGA,5279
 ppvector/models/res2net.py,sha256=f2-cv1VQi6KkF8NETt2-BsLvVIlvjM2whUiBaCCeZtQ,6843
 ppvector/models/resnet_se.py,sha256=dmet6cJZvE3DGmfnlpT95exy-9cP2eeWiK-6AmtCH_Q,5662
 ppvector/models/tdnn.py,sha256=t_thrn6_UxV9xn4Hn6I3-l-V9_YxP8vH5qm7uvRtNiI,3459
 ppvector/models/utils.py,sha256=WFaGb66sSLalS-2yr9VIOnICrLjR8rGmiggGaWnZV_Y,5049
 ppvector/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppvector/utils/logger.py,sha256=-ssorx8FlHA_wrd2Eq6f4HkOqaOG2YseBGvYAo8NXN8,2839
-ppvector/utils/record.py,sha256=2i4kz5kPDa9KkbAK_Q34sVIXOkD9TroPROIe5QdzqWg,1067
+ppvector/utils/record.py,sha256=S2sGoLPJrdRsrG7_ojNt4kwL05VNrOxnmnMOwNOZ9-0,1385
 ppvector/utils/scheduler.py,sha256=IZ1kU6S86hfnuZLfkepflStogDyYvRPEDMP4P_mwGvs,3399
 ppvector/utils/utils.py,sha256=zzBjiCYwNwxYGMfZyo_wubYp5MTmN3YRuFMKXZ65S7c,2790
-ppvector-1.0.0.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
-ppvector-1.0.0.dist-info/METADATA,sha256=XTNUto3li7W8dViwbByj29HpUu-gJSd53b2mx7eiC1k,27457
-ppvector-1.0.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-ppvector-1.0.0.dist-info/top_level.txt,sha256=NywKjkr9phu2LhphLKQRNvdJUG8iJGaZQbe_HC0PhcQ,9
-ppvector-1.0.0.dist-info/RECORD,,
+ppvector-1.0.2.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
+ppvector-1.0.2.dist-info/METADATA,sha256=gmN0gr0-WDhrPOT-w97MBJzlwx4EY5hoIUqzPWGSkFQ,33186
+ppvector-1.0.2.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+ppvector-1.0.2.dist-info/top_level.txt,sha256=NywKjkr9phu2LhphLKQRNvdJUG8iJGaZQbe_HC0PhcQ,9
+ppvector-1.0.2.dist-info/RECORD,,
```

